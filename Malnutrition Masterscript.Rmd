---
title: "Malnutrition Masterscript"
author: "Jim Perry"
date: "5/30/2022"
output: pdf_document
---
# PREPROCESSING: Preparing data from original survey for use in statistical/ML analysis.
#####
```{r Import Data}
#Reading and cleaning the original spreadsheet from the survey.
library(readxl)
library(dplyr)

SURVEY = read_excel("SURVEY.xlsx")
SURVEY = SURVEY[-1,] #First row is a misread full of NAs - drop it
SURVEY = SURVEY[,-c(35, 54, 115, 134)] #Empty columns

#Now the fun part - manually rename any weird variable names
names(SURVEY)[13] = "Class" #Which class in the grade is the child in?
names(SURVEY)[17] = "FamilySize"
names(SURVEY)[18] = "NumOldBros"
names(SURVEY)[19] = "Bros<12?"
names(SURVEY)[20] = "Vaccine"
names(SURVEY)[21] = "BCGScar"
names(SURVEY)[22] = "FeverLast2Weeks"
names(SURVEY)[23] = "FeverQuantity"
names(SURVEY)[24] = "DiarrheaLast2Weeks"
names(SURVEY)[25] = "DiarrheaQuantity"
names(SURVEY)[26] = "CoughLast2Weeks"
names(SURVEY)[27] = "CoughQuantity"
names(SURVEY)[28] = "NailsTrimmed"
names(SURVEY)[29] = "NailsDirty"
names(SURVEY)[30] = "NailTrimFrequency"
names(SURVEY)[31] = "SchoolLat"
names(SURVEY)[32] = "SchoolLatDoors"
names(SURVEY)[33] = "SchoolLatFlies"
names(SURVEY)[34] = "SchoolLatVisibleStool"
names(SURVEY)[35] = "HeardOfAL"
names(SURVEY)[36] = "HeardOfTT"
names(SURVEY)[37] = "HeardOfHW"
names(SURVEY)[38] = "HeardOfHIV"
names(SURVEY)[39] = "HeardOfWorms"
names(SURVEY)[40] = "HeardOfMalaria"
names(SURVEY)[41] = "HeardOfTB"
names(SURVEY)[42] = "HeardOfSCh"
names(SURVEY)[43] = "ToldByFam"
names(SURVEY)[44] = "ToldByHP"
names(SURVEY)[45] = "ToldByTeacher"
names(SURVEY)[46] = "ToldByMedia"
names(SURVEY)[47] = "KnowWormsSpread"
names(SURVEY)[48] = "HowKnowWormsSpread"
names(SURVEY)[49] = "KnowWormsBad"
names(SURVEY)[50] = "HowKnowWormsBad"
names(SURVEY)[51] = "KnowAvoidWorms"
names(SURVEY)[52] = "HowKnowAvoidWorms"
names(SURVEY)[53] = "WhereLive"
names(SURVEY)[54] = "Address"
names(SURVEY)[55] = "Occupation"
names(SURVEY)[56] = "MomEduc"
names(SURVEY)[57] = "HouseFloorMats"
names(SURVEY)[58] = "KitchenSeparate"
names(SURVEY)[59] = "SepKitchenMats"
names(SURVEY)[60] = "SepKitchenRoof"
names(SURVEY)[61] = "SepKitchenWall"
names(SURVEY)[62] = "SepKitchenNeither" #Should be able to get removed in 1HE
names(SURVEY)[63] = "CookWood"
names(SURVEY)[64] = "CookGas"
names(SURVEY)[65] = "CookCoal"
names(SURVEY)[66] = "CookKerosine"
names(SURVEY)[67] = "CookElectric"
names(SURVEY)[68] = "Electricity"
names(SURVEY)[69] = "Radio"
names(SURVEY)[70] = "TV"
names(SURVEY)[71] = "Phone"
names(SURVEY)[72] = "WhyPhone"
names(SURVEY)[73] = "Cattle"
names(SURVEY)[74] = "Sheep/Goat"
names(SURVEY)[75] = "Chicken"
names(SURVEY)[76] = "HousePet"
names(SURVEY)[77] = "NoAnimal"
names(SURVEY)[78] = "HouseHasWater"
names(SURVEY)[79] = "WhereGetWater"
names(SURVEY)[80] = "WaterTreated"
names(SURVEY)[81] = "HowTreat"
names(SURVEY)[82] = "FamLat"
names(SURVEY)[83] = "LatInside"
names(SURVEY)[84] = "LatDistanceHouse"
names(SURVEY)[85] = "LatDistanceKitchen"
names(SURVEY)[86] = "LatConnectedTo"
names(SURVEY)[87] = "RiverBathFrequency"
names(SURVEY)[88] = "RiverLaundryFrequency"
names(SURVEY)[89] = "DefecateField"
names(SURVEY)[90] = "UseSchoolLat"
names(SURVEY)[91] = "UseTP"
names(SURVEY)[92] = "WashHandsLat"
names(SURVEY)[93] = "WashHandsLatHow"
names(SURVEY)[94] = "WashHandsSoapLatFrequency"
names(SURVEY)[95] = "WashHandsEat"
names(SURVEY)[96] = "WashHandsEatHow"
names(SURVEY)[97] = "WashHandsSoapEatFrequency"
names(SURVEY)[98] = "EatSoil"
names(SURVEY)[99] = "FavFruit"
names(SURVEY)[100] = "WashFruit"
names(SURVEY)[101] = "EatRawVeg"
names(SURVEY)[102] = "WashRawVegFrequency"
names(SURVEY)[103] = "WalkBarefoot"
names(SURVEY)[104] = "HomeShoeOrSandal"
names(SURVEY)[105] = "ForWhatBarefoot"
names(SURVEY)[106] = "DewormPill"
names(SURVEY)[107] = "WhenDewormPill"
names(SURVEY)[108] = "Antibiotics"
names(SURVEY)[109] = "MostFreqFood"
names(SURVEY)[110] = "TakesMeds"
names(SURVEY)[111] = "Name/TypeOfMeds"
names(SURVEY)[112] = "AntiMalaria3Months"
names(SURVEY)[113] = "Wheezing"
names(SURVEY)[114] = "Wheezing2Yrs"
names(SURVEY)[115] = "Wheezing1Yr"
names(SURVEY)[116] = "Wheezing1YrQuantity"
names(SURVEY)[117] = "Asthma"
names(SURVEY)[118] = "Asthma2Yrs"
names(SURVEY)[119] = "Asthma1Yr"
names(SURVEY)[120] = "DocConfirmedAsthma"
names(SURVEY)[121] = "Rash"
names(SURVEY)[122] = "RashElbow"
names(SURVEY)[123] = "RashKnees"
names(SURVEY)[124] = "RashAnkles"
names(SURVEY)[125] = "RashButt"
names(SURVEY)[126] = "RashNeck"
names(SURVEY)[127] = "RashEyesEars"
names(SURVEY)[128] = "HayFever"
names(SURVEY)[129] = "HayFever2Yrs"
names(SURVEY)[130] = "HayFever1Yr"

SURVEY = SURVEY[,-c(1, 7, 8, 9)] #Don't need these columns; I forgot to remove them earlier, and doing so now would mean I'd have to re-index the >100 variable renames I just did above. No way.
```



```{r Handling NAs & Datatype Conversions, warning = FALSE}
reducedSurvey = SURVEY[,-c(44, 46, 48, 51, 64, 95, 101, 105, 107)] #Potential candidates for removal from dataset; we'll work with this, as I think these changes will end up in the final iteration.
################################################################################
#1HE "Where do you get your water from?"
tapWater = c()
WaterFromNeighbor = c()
WaterFromRiver = c()
WaterFromWell = c()
WaterFromTank = c()

for (i in 1:1036){
  if (is.na(reducedSurvey[[i, 70]])){
    tapWater[[i]] = 0
    WaterFromNeighbor[[i]] = 0
    WaterFromRiver[[i]] = 0
    WaterFromWell[[i]] = 0
    WaterFromTank[[i]] = 0
    next
  }
  if (tolower(reducedSurvey[[i, 70]]) == "tap water"){
    tapWater[[i]] = 1
    WaterFromNeighbor[[i]] = 0
    WaterFromRiver[[i]] = 0
    WaterFromWell[[i]] = 0
    WaterFromTank[[i]] = 0
  }else if (reducedSurvey[[i, 70]] == 0){
    tapWater[[i]] = 0
    WaterFromNeighbor[[i]] = 1
    WaterFromRiver[[i]] = 0
    WaterFromWell[[i]] = 0
    WaterFromTank[[i]] = 0
  }else if (reducedSurvey[[i, 70]] == 1){
    tapWater[[i]] = 0
    WaterFromNeighbor[[i]] = 0
    WaterFromRiver[[i]] = 1
    WaterFromWell[[i]] = 0
    WaterFromTank[[i]] = 0
  }else if (reducedSurvey[[i, 70]] == 2){
    tapWater[[i]] = 0
    WaterFromNeighbor[[i]] = 0
    WaterFromRiver[[i]] = 0
    WaterFromWell[[i]] = 1
    WaterFromTank[[i]] = 0
  }else if (reducedSurvey[[i, 70]] == 4){
    tapWater[[i]] = 0
    WaterFromNeighbor[[i]] = 0
    WaterFromRiver[[i]] = 0
    WaterFromWell[[i]] = 0
    WaterFromTank[[i]] = 1
  }
}
tapWater = as.numeric(as.character(tapWater))
reducedSurvey[[70]] = tapWater
names(reducedSurvey)[[70]] = "WaterFromTap"

WaterFromNeighbor = as.numeric(as.character(WaterFromNeighbor))
reducedSurvey[[117]] = WaterFromNeighbor
names(reducedSurvey)[[117]] = "WaterFromNeighbor"

WaterFromRiver = as.numeric(as.character(WaterFromRiver))
reducedSurvey[[118]] = WaterFromRiver
names(reducedSurvey)[[118]] = "WaterFromRiver"

WaterFromWell = as.numeric(as.character(WaterFromWell))
reducedSurvey[[119]] = WaterFromWell
names(reducedSurvey)[[119]] = "WaterFromWell"

WaterFromTank = as.numeric(as.character(WaterFromTank))
reducedSurvey[[120]] = WaterFromTank
names(reducedSurvey)[[120]] = "WaterFromTank"

################################################################################
#1HE Rural/Suburban/Urban split
rural = c()
urban = c()
for (i in 1:1036){
  if (is.na(reducedSurvey[[i, 46]])){
      next
  }
  neighborhood = reducedSurvey[[i, 46]]
  if(neighborhood == 0){
      urban[[i]] = 1
      rural[[i]] = 0
  }else if(neighborhood == 1){
    rural[[i]] = 1
    urban[[i]] = 0
    }
}
urban = as.numeric(as.character(urban))
reducedSurvey[[46]] = urban
names(reducedSurvey)[[46]] = "Urban"
rural = as.numeric(as.character(rural)) #Can't convert directly from double to numeric
reducedSurvey[[47]] = rural# = replace(reducedSurvey, reducedSurvey$Address, rural)
names(reducedSurvey)[[47]] = "Rural"
################################################################################
#Remove units from measurements so they can be converted to numerics
for (i in 1:1036){
  distance = reducedSurvey[[i, 75]]
  reducedSurvey[[i, 75]] = substr(distance, 1, length(distance))
  distance2 = reducedSurvey[[i, 76]]
  reducedSurvey[[i, 76]] = substr(distance2, 1, length(distance2))
  whenDeworm = reducedSurvey[[i, 96]]
  reducedSurvey[[i, 96]] = substr(whenDeworm, 1, length(whenDeworm))

}
reducedSurvey[[75]] = as.numeric(reducedSurvey[[75]]) #Can typecast directly to numeric
reducedSurvey[[76]] = as.numeric(reducedSurvey[[76]])
reducedSurvey[[96]] = as.numeric(reducedSurvey[[96]])
################################################################################
#These factors have number measurements but are of datatype "character"; simple conversion to datatype "numeric"
changeInds = c(19, 21, 23, 31:42, 50:59, 64:68, 72, 76, 77, 82:117)
for (factor in changeInds){
  reducedSurvey[[factor]] = as.numeric(reducedSurvey[[factor]])
}
################################################################################
#Drop "Class"? Doesn't seem specific enough to be useful.
reducedSurvey = reducedSurvey[, -9]
################################################################################
#Adjusting data values
#Convert "99"s in numeric factors to "Na"s for later imputation under kNN algorithm
for (sample in 1:1036){
  for (factor in 11:116) #Columns where 99s indicate "I don't know" as a possible answer
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if (reducedSurvey[[sample, factor]] == 99){
      reducedSurvey[[sample, factor]] = NA
    }
}

#A sample's height was misinput as 92.00 meters.
for (sample in 1:1036){
  if (reducedSurvey[[sample, 10]] == 92.00){ 
    reducedSurvey[[sample, 10]] = NA
    break
  }
}

#Impute conditional responses where "NA" implies 0
for (sample in 1:1036){
  for (factor in c(18, 20, 22, 31:41, 53, 55:58, 64:67, 71, 95, 109:113)){
    if (is.na(reducedSurvey[[sample, factor]])){
      reducedSurvey[[sample, factor]] = 0
    }
  }
}

#These columns were input as a different binary; as "NA/0" instead of "0/1"
for (sample in 1:1036){
  for (factor in c(30, 54, 63)){ 
    if (is.na(reducedSurvey[[sample, factor]])){
      reducedSurvey[[sample, factor]] = 0
    }else if (reducedSurvey[[sample, factor]] == 0){
      reducedSurvey[[sample, factor]] = 1
    }else{
      reducedSurvey[[sample, factor]] = 0
    }
  }
}

#Convert different numeric binaries 0/2 binaries into 0/1
for (sample in 1:1036){
  for (factor in c(49, 65)){
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if (reducedSurvey[[sample, factor]] == 2){
      reducedSurvey[[sample, factor]] = 1
    }else if (reducedSurvey[[sample, factor]] != 0){
      reducedSurvey[[sample, factor]] = NA
    }
  }
}
#CookElectric's binary is 0/4; converting into 0/1
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 58]])){
      next
    }else if (reducedSurvey[[sample, 58]] == 4){
      reducedSurvey[[sample, 58]] = 1
    }else if (reducedSurvey[[sample, 58]] != 0){
      reducedSurvey[[sample, 58]] = NA
    }
  }
}
#0/3 here
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 66]])){
      reducedSurvey[[sample, 66]] = 0
    }else if (reducedSurvey[[sample, 66]] == 3){
      reducedSurvey[[sample, 66]] = 1
    }else if (reducedSurvey[[sample, 6]] != 0){
      reducedSurvey[[sample, 66]] = NA
    }
  }
}
#I don't know why this code in the above loop doesn't work. I don't know why it does work when placed into its own loop. Code is mysterious; let's hope this desire for loop autonomy doesn't spread
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 66]])){
      reducedSurvey[[sample, 66]] = 0
    }
  }
}

#copySet = reducedSurvey
#reducedSurvey = copySet

#Setting weird values in binary vectors (anything that's not 0/1) to NA for imputation
for (sample in 1:1036){
  for (factor in c(15, 16, 21, 23, 24, 49, 51, 58, 65, 68, 72:73)){
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if ((reducedSurvey[[sample, factor]] != 0) & (reducedSurvey[[sample, factor]] != 1)){
      reducedSurvey[[sample, factor]] = NA
    }
  }
}



reducedSurvey$HouseFloorMats = tolower(reducedSurvey$HouseFloorMats)

#Getting rid of some more weird values
for (sample in 1:1036){
  for (factor in c(85:86))
  if (is.na(reducedSurvey[[sample, factor]])){
    next
  }
  else if (reducedSurvey[[sample, factor]] == 21 || reducedSurvey[[sample, factor]] == 10){
    reducedSurvey[[sample, factor]] = NA
  }
}

for (sample in 1:1036){
  if (is.na(reducedSurvey[[sample, 10]])){
    next
  }
  else if (reducedSurvey[[sample, 10]] == 1.79){
    reducedSurvey[[sample, 10]] = 1.29
  }
}

```



```{r kNN, One Hot Encoding}
library(VIM)
library(fastDummies)
################################################################################
#DEAL WITH CATEGORICALS
#impute ordinal categoricals
imputed = kNN(reducedSurvey, variable = colnames(reducedSurvey), k = 5, impNA = TRUE)
imputed = subset(imputed, select = AL:WaterFromTank)

#1HE nominal categoricals
#I already encoded some of these by hand in the last code block, but the remainder can be handled by this imported function
oneHotEncoded = dummy_cols(.data = imputed,
                     ignore_na = TRUE,
                     remove_first_dummy =  TRUE,
                     split = ",")
oneHotEncoded = oneHotEncoded[,-c(48, 123)] #One category got broken up and the other was created to account for a sample answer that got misinput as both
oneHotEncoded[[843, 121]] = 1

```



```{r Final Layer of Data Handling}
#Handle variables we decided to remove or calculate differently after I wrote 300 lines of code. I'm not mad - I'm not just rewriting everything above to account for new index values.
processed = oneHotEncoded[, -c(1:5, 11, 13:14, 18, 20, 22, 25:26, 30:44, 49, 52:56, 61, 66, 70, 73:75, 83, 86, 92, 94, 99:104, 107:112, 114, 119:121, 127)]

#Encode mother's education
for (i in 1:1036){
  education = processed[[i, 19]]
  processed$momFinishedPrimary[[i]] = 0
  processed$momFinishedSecondary[[i]] = 0
  processed$momFinishedTertiary[[i]] = 0
  if (education == 1){
    processed$momFinishedPrimary[[i]] = 1
  }else if (education == 2){
    processed$momFinishedSecondary[[i]] = 1
  }else if (education == 3){
    processed$momFinishedTertiary[[i]] = 1
  }
}

processed$momFinishedPrimary = as.numeric(as.character(processed$momFinishedPrimary))
processed$momFinishedSecondary = as.numeric(as.character(processed$momFinishedSecondary))
processed$momFinishedTertiary = as.numeric(as.character(processed$momFinishedTertiary))
processed = processed[, -19]

#Convert house floor materials into a binary for whether that floor is dust or not
for (i in 1:1036){
  if (sum(processed[i,61:65]) > 0){
    processed$houseDustFloor[[i]] = 0
  }else{
    processed$houseDustFloor[[i]] = 1
  }
}
processed$houseDustFloor = as.numeric(as.character(processed$houseDustFloor))
processed = processed[,-c(61:65)]

#Converting 1/2 binaries to 0/1
for (sample in 1:1036){
  for (factor in 53:56){
    if (processed[sample, factor] == 2){
      processed[sample, factor] = 0
    }
  }
}

#Every sample has the same answer for this variable - it tells us nothing
processed = processed[,-c(21)]
processed = processed[,-3]

#1HE washing hands after lat into "never", "with water", and "with soap" groups
for (i in 1:1036){
  if (processed[[i,38]] == 2){
    processed$washHandsLatWater[[i]] = 0
    processed$washHandsLatWaterSoap[[i]] = 0
  }else{
    if (processed[[i, 39]] == 0){
      processed$washHandsLatWater[[i]] = 1
      processed$washHandsLatWaterSoap[[i]] = 0
    }else if (processed[[i, 39]] == 1){
      processed$washHandsLatWater[[i]] = 0
      processed$washHandsLatWaterSoap[[i]] = 1
    }
  }
}
processed$washHandsLatWater = as.numeric(as.character(processed$washHandsLatWater))
processed$washHandsLatWaterSoap = as.numeric(as.character(processed$washHandsLatWaterSoap))
processed = processed[, -c(38:39)]

#1HE washing hands before eat into "never", "with water", "with soap" groups
for (i in 1:1036){
  if (processed[[i,38]] == 2){
    processed$washHandsWaterB4Eat[[i]] = 0
    processed$washHandsSoapB4Eat[[i]] = 0
  }else{
    if (processed[[i, 39]] == 0){
      processed$washHandsWaterB4Eat[[i]] = 1
      processed$washHandsSoapB4Eat[[i]] = 0
    }else if (processed[[i, 39]] == 1){
      processed$washHandsWaterB4Eat[[i]] = 0
      processed$washHandsSoapB4Eat[[i]] = 1
    }
  }
}
processed$washHandsWaterB4Eat = as.numeric(as.character(processed$washHandsWaterB4Eat))
processed$washHandsSoapB4Eat = as.numeric(as.character(processed$washHandsSoapB4Eat))
processed = processed[, -c(38:39)]

#Change ternary variables into binaries based on biological significance
for (sample in 1:1036){
  for (factor in c(33:37)){
    if (processed[[sample, factor]] == 2){
      processed[[sample, factor]] = 0
    }else{
      processed[[sample, factor]] = 1
    }
  }
}
names(processed)[[33]] = "RiverBathing"
names(processed)[[34]] = "RiverLaundry"

for (sample in 1:1036){
 if (processed[[sample, 38]] == 1){
   processed[[sample, 38]] = 0
 }else{
   processed[[sample, 38]] = 1
 }
}

for (sample in 1:1036){
  if (processed[[sample, 39]] != 0){
    processed[[sample, 39]] = 0
  }else if (processed[[sample, 39]] == 0){
    processed[[sample, 39]] = 1
  }
}
names(processed)[[39]] = "NeverWashFruit"

#More ternary variables, but split differently according to biological significance
for (sample in 1:1036){
  for (factor in c(40:42)){
    if (processed[[sample, factor]] != 0){
      processed[[sample, factor]] = 1
    }else{
      processed[[sample, factor]] = 0
    }
  }
}

#This loop below does two things: it re-formats data so 0 is false and 1 is true, OR turns already-correct but useless data into correctly-encoded data (i.e. "NoPhone" and "NoWaterInHouse")
for (sample in 1:1036){
  for (factor in c(11,13,23,31,32,36,37)){
    if (processed[[sample, factor]] == 1){
      processed[[sample, factor]] = 0
    }else{
      processed[[sample, factor]] = 1
    }
  }
}
names(processed)[[11]] = "NailsNotTrimmed"
names(processed)[[13]] = "NoSchoolLatDoors"
names(processed)[[23]] = "NoPhone"
names(processed)[[28]] = "PotableWaterFromHouse"
names(processed)[[30]] = "DontTreatWater"
names(processed)[[31]] = "NoFamLat"
names(processed)[[32]] = "LatOutside"
names(processed)[[36]] = "DontUseSchoolLat"
names(processed)[[37]] = "DontUseTP"

for (i in 1:1036){
  if (processed[[i,8]] == 1 | processed[[i,9]] == 1 | processed[[i,10]] == 1){
    processed$SickLast2Weeks[i] = 1
  }else{
    processed$SickLast2Weeks[i] = 0
  }
}
processed = processed[,-c(8:10)]

for (i in 1:1036){
  if (processed[[i,8]] == 1 | processed[[i,9]] == 1) {
    processed$NailsMaintained[i] = 0
  }else{
    processed$NailsMaintained[i] = 1
  }
}
processed = processed[,-c(8:10)]

for (i in 1:1036){
  if (processed[[i,8]] == 1 | processed[[i,9]] == 1) {
    processed$SchoolLatClean[i] = 0
  }else{
    processed$SchoolLatClean[i] = 1
  }
}
processed = processed[,-c(8:9, 11)]
processed = processed[,-c(10)]

for (i in 1:1036){
  if (processed[[i,11]] == 1 | processed[[i,12]] == 1) {
    processed$"Radio/TV"[i] = 1
  }else{
    processed$"Radio/TV"[i] = 0
  }
}
processed = processed[,-c(11:12)]
processed = processed[,-11]

processed = processed[,-c(16, 18)]

for (i in 1:1036){
  if (processed[[i,18]] == 1 | processed[[i,19]] == 1) {
    processed$RiverCleaning[i] = 1
  }else{
    processed$RiverCleaning[i] = 0
  }
}
processed = processed[,-c(18,19)]

for (i in 1:1036){
  if (processed[[i,28]] == 1 | processed[[i,29]] == 1) {
    processed$OtherMeds[i] = 1
  }else{
    processed$OtherMeds[i] = 0
  }
}
processed = processed[,-c(28,29)]
processed = processed[,-28]
processed = processed[,-c(30:34)]

for (i in 1:1036){
  if (processed[[i,31]] == 1 | processed[[i,32]] == 1) {
    processed$"momFinished2+"[i] = 1
  }else{
    processed$"momFinished2+"[i] = 0
  }
}
processed = processed[,-c(31, 32)]
```


```{r Dependent Variable Calculations}
#Derive WAZ/HAZ/BAZ for specific analysis
#WAZ: Weight-relative-to-age Z-score
#HAZ: Height-relative-to-age Z-score (WHO says unreliable for ages 11+, so ignore older kids)
#BAZ: BMI-relative-to-age Z-score

library(anthroplus)

depVars = anthroplus_zscores(sex = processed[["Sex"]]+1,
                             age_in_months = processed[["Age"]]*12,
                             weight_in_kg = processed[["weight"]],
                             height_in_cm = processed[["Height"]]*100
                             )

for (i in 1:1036){
  ZHeight = depVars$zhfa[[i]]
  processed$HAZ[[i]] = ZHeight
  processed$Stunting[[i]] = ifelse(ZHeight < -2, 1, 0 )
  
  ZBMI = depVars$zbfa[[i]]
  processed$BAZ[[i]] = ZBMI
  processed$Thinness[[i]] = ifelse (ZBMI < -2, 1, 0)
  
  if (processed[[i,"Age"]] < 11){ #scores not calculated for children 11 and up
    ZWeight = depVars$zwfa[[i]]
    processed$WAZ[[i]] = ZWeight
    processed$Wasting[[i]] = ifelse (ZWeight < -2, 1, 0)
  }else{
    processed$WAZ[[i]] = NA
    processed$Wasting[[i]] = NA
  }
}

processed$HAZ = as.numeric(as.character(processed$HAZ))
processed$Stunting = as.numeric(as.character(processed$Stunting))
processed$BAZ = as.numeric(as.character(processed$BAZ))
processed$Thinness = as.numeric(as.character(processed$Thinness))

#Some samples were too old/young for the WHO's package to calculate Stunting scores; we will remove them
for (i in 1:1036){
  if (is.na(processed$Stunting[i])){
    processed = processed[-i,]
  }
}
#as.numeric(as.character(badAges))
#processed = processed[[-badAges,]]
```



```{r Saving Preprocessing}
stunting = processed[,-c(3, 4, 43, 45:48)]
thinness = processed[,-c(3, 4, 43:45, 47:48)]
wasting = processed[,-c(3, 4, 43:47)]
wasting = subset(wasting, Age < 11)
wasting$Wasting = as.numeric(as.character(wasting$Wasting))
################################################################################
stuntingUrban = subset(stunting, Urban == 1)
stuntingNotUrban = subset(stunting, Urban == 0)
stuntingMale = subset(stunting, Sex == 0)
stuntingFemale = subset(stunting, Sex == 1)

thinnessUrban = subset(thinness, Urban == 1)
thinnessNotUrban = subset(thinness, Urban == 0)
thinnessMale = subset(thinness, Sex == 0)
thinnessFemale = subset(thinness, Sex == 1)

wastingUrban = subset(wasting, Urban == 1)
wastingNotUrban = subset(wasting, Urban == 0)
wastingMale = subset(wasting, Sex == 0)
wastingFemale = subset(wasting, Sex == 1)
################################################################################
# write.csv(stunting, "stunting.csv", row.names = FALSE)
# write.csv(thinness, "thinness.csv", row.names = FALSE)
# write.csv(wasting, "wasting.csv", row.names = FALSE)
```

# TRAIN/TEST SPLITS: Prepares train/test splits for use in feature selection and
# classifier model training. Each dependent variable (Stunting and Thinness)
# will be cloned; one clone will remain as-is (unbalanced), and the other will
# be balanced via SMOTE for (hopefully) improved classifier results.
#####

```{r Read Data}
stunting = read.csv("stunting.csv")
thinness = read.csv("thinness.csv")
wasting = read.csv("wasting.csv")
```

```{r Stunting Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
testFolds = createFolds(stunting$Stunting, k = 10) #Create k splits
testSetsStunting = list(stunting[testFolds$Fold01, ], stunting[testFolds$Fold02, ],
                        stunting[testFolds$Fold03, ], stunting[testFolds$Fold04, ],
                        stunting[testFolds$Fold05, ], stunting[testFolds$Fold06, ],
                        stunting[testFolds$Fold07, ], stunting[testFolds$Fold08, ],
                        stunting[testFolds$Fold09, ], stunting[testFolds$Fold10, ]
                        )
sansTestStunting = list(stunting[-testFolds$Fold01, ], stunting[-testFolds$Fold02, ],
                        stunting[-testFolds$Fold03, ], stunting[-testFolds$Fold04, ],
                        stunting[-testFolds$Fold05, ], stunting[-testFolds$Fold06, ],
                        stunting[-testFolds$Fold07, ], stunting[-testFolds$Fold08, ],
                        stunting[-testFolds$Fold09, ], stunting[-testFolds$Fold10, ]
                        )
UBStunting = list()
for (i in 1:10){
  curSet = sansTestStunting[[i]]
  validationFolds = createFolds(sansTestStunting[[i]]$Stunting, k = 10)
  UBStunting[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStunting = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStunting[[i]] = list()
  for (j in 1:10){
    SMOTEStunting[[i]][[j]] = SMOTE(X = as.data.frame(UBStunting[[i]][[j]]),
                              target = UBStunting[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEStunting[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(stuntingMale$Stunting, k = 10) #Create k splits
testSetsStuntingMale = list(stuntingMale[testFolds$Fold01, ], stuntingMale[testFolds$Fold02, ],
                        stuntingMale[testFolds$Fold03, ], stuntingMale[testFolds$Fold04, ],
                        stuntingMale[testFolds$Fold05, ], stuntingMale[testFolds$Fold06, ],
                        stuntingMale[testFolds$Fold07, ], stuntingMale[testFolds$Fold08, ],
                        stuntingMale[testFolds$Fold09, ], stuntingMale[testFolds$Fold10, ]
                        )
sansTestStuntingMale = list(stuntingMale[-testFolds$Fold01, ], stuntingMale[-testFolds$Fold02, ],
                        stuntingMale[-testFolds$Fold03, ], stuntingMale[-testFolds$Fold04, ],
                        stuntingMale[-testFolds$Fold05, ], stuntingMale[-testFolds$Fold06, ],
                        stuntingMale[-testFolds$Fold07, ], stuntingMale[-testFolds$Fold08, ],
                        stuntingMale[-testFolds$Fold09, ], stuntingMale[-testFolds$Fold10, ]
                        )
UBStuntingMale = list()
for (i in 1:10){
  curSet = sansTestStuntingMale[[i]]
  validationFolds = createFolds(sansTestStuntingMale[[i]]$Stunting, k = 10)
  UBStuntingMale[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStuntingMale = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStuntingMale[[i]] = list()
  for (j in 1:10){
    SMOTEStuntingMale[[i]][[j]] = SMOTE(X = as.data.frame(UBStuntingMale[[i]][[j]]),
                              target = UBStuntingMale[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEStuntingMale[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(stuntingFemale$Stunting, k = 10) #Create k splits
testSetsStuntingFemale = list(stuntingFemale[testFolds$Fold01, ], stuntingFemale[testFolds$Fold02, ],
                        stuntingFemale[testFolds$Fold03, ], stuntingFemale[testFolds$Fold04, ],
                        stuntingFemale[testFolds$Fold05, ], stuntingFemale[testFolds$Fold06, ],
                        stuntingFemale[testFolds$Fold07, ], stuntingFemale[testFolds$Fold08, ],
                        stuntingFemale[testFolds$Fold09, ], stuntingFemale[testFolds$Fold10, ]
                        )
sansTestStuntingFemale = list(stuntingFemale[-testFolds$Fold01, ], stuntingFemale[-testFolds$Fold02, ],
                        stuntingFemale[-testFolds$Fold03, ], stuntingFemale[-testFolds$Fold04, ],
                        stuntingFemale[-testFolds$Fold05, ], stuntingFemale[-testFolds$Fold06, ],
                        stuntingFemale[-testFolds$Fold07, ], stuntingFemale[-testFolds$Fold08, ],
                        stuntingFemale[-testFolds$Fold09, ], stuntingFemale[-testFolds$Fold10, ]
                        )
UBStuntingFemale = list()
for (i in 1:10){
  curSet = sansTestStuntingFemale[[i]]
  validationFolds = createFolds(sansTestStuntingFemale[[i]]$Stunting, k = 10)
  UBStuntingFemale[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStuntingFemale = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStuntingFemale[[i]] = list()
  for (j in 1:10){
    SMOTEStuntingFemale[[i]][[j]] = SMOTE(X = as.data.frame(UBStuntingFemale[[i]][[j]]),
                              target = UBStuntingFemale[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEStuntingFemale[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(stuntingUrban$Stunting, k = 10) #Create k splits
testSetsStuntingUrban = list(stuntingUrban[testFolds$Fold01, ], stuntingUrban[testFolds$Fold02, ],
                        stuntingUrban[testFolds$Fold03, ], stuntingUrban[testFolds$Fold04, ],
                        stuntingUrban[testFolds$Fold05, ], stuntingUrban[testFolds$Fold06, ],
                        stuntingUrban[testFolds$Fold07, ], stuntingUrban[testFolds$Fold08, ],
                        stuntingUrban[testFolds$Fold09, ], stuntingUrban[testFolds$Fold10, ]
                        )
sansTestStuntingUrban = list(stuntingUrban[-testFolds$Fold01, ], stuntingUrban[-testFolds$Fold02, ],
                        stuntingUrban[-testFolds$Fold03, ], stuntingUrban[-testFolds$Fold04, ],
                        stuntingUrban[-testFolds$Fold05, ], stuntingUrban[-testFolds$Fold06, ],
                        stuntingUrban[-testFolds$Fold07, ], stuntingUrban[-testFolds$Fold08, ],
                        stuntingUrban[-testFolds$Fold09, ], stuntingUrban[-testFolds$Fold10, ]
                        )
UBStuntingUrban = list()
for (i in 1:10){
  curSet = sansTestStuntingUrban[[i]]
  validationFolds = createFolds(sansTestStuntingUrban[[i]]$Stunting, k = 10)
  UBStuntingUrban[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStuntingUrban = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStuntingUrban[[i]] = list()
  for (j in 1:10){
    SMOTEStuntingUrban[[i]][[j]] = SMOTE(X = as.data.frame(UBStuntingUrban[[i]][[j]]),
                              target = UBStuntingUrban[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEStuntingUrban[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(stuntingNotUrban$Stunting, k = 10) #Create k splits
testSetsStuntingNotUrban = list(stuntingNotUrban[testFolds$Fold01, ], stuntingNotUrban[testFolds$Fold02, ],
                        stuntingNotUrban[testFolds$Fold03, ], stuntingNotUrban[testFolds$Fold04, ],
                        stuntingNotUrban[testFolds$Fold05, ], stuntingNotUrban[testFolds$Fold06, ],
                        stuntingNotUrban[testFolds$Fold07, ], stuntingNotUrban[testFolds$Fold08, ],
                        stuntingNotUrban[testFolds$Fold09, ], stuntingNotUrban[testFolds$Fold10, ]
                        )
sansTestStuntingNotUrban = list(stuntingNotUrban[-testFolds$Fold01, ], stuntingNotUrban[-testFolds$Fold02, ],
                        stuntingNotUrban[-testFolds$Fold03, ], stuntingNotUrban[-testFolds$Fold04, ],
                        stuntingNotUrban[-testFolds$Fold05, ], stuntingNotUrban[-testFolds$Fold06, ],
                        stuntingNotUrban[-testFolds$Fold07, ], stuntingNotUrban[-testFolds$Fold08, ],
                        stuntingNotUrban[-testFolds$Fold09, ], stuntingNotUrban[-testFolds$Fold10, ]
                        )
UBStuntingNotUrban = list()
for (i in 1:10){
  curSet = sansTestStuntingNotUrban[[i]]
  validationFolds = createFolds(sansTestStuntingNotUrban[[i]]$Stunting, k = 10)
  UBStuntingNotUrban[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStuntingNotUrban = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStuntingNotUrban[[i]] = list()
  for (j in 1:10){
    SMOTEStuntingNotUrban[[i]][[j]] = SMOTE(X = as.data.frame(UBStuntingNotUrban[[i]][[j]]),
                              target = UBStuntingNotUrban[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEStuntingNotUrban[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBStunting[[1]][[1]]$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEStunting[[1]][[1]]$data$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (SMOTE)')
# abline(h = 0)


# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestStunting = rbind(testSetsStunting[1:10])
# ForFS_UBStunting = rbind(sansTestStunting[1:10])
# ForFS_SMOTEStunting = rbind(SMOTEStunting[1:10])

# write.csv(ForFS_TestStunting, "ForFS_TestStunting.csv")
# write.csv(ForFS_UBStunting, "ForFS_UBStunting.csv")
# write.csv(ForFS_SMOTEStunting, "ForFS_SMOTEStunting.csv")
```



```{r Thinness Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
set.seed(13)
testFolds = createFolds(thinness$Thinness, k = 10) #Create k splits
testSetsThinness = list(thinness[testFolds$Fold01, ], thinness[testFolds$Fold02, ],
                        thinness[testFolds$Fold03, ], thinness[testFolds$Fold04, ],
                        thinness[testFolds$Fold05, ], thinness[testFolds$Fold06, ],
                        thinness[testFolds$Fold07, ], thinness[testFolds$Fold08, ],
                        thinness[testFolds$Fold09, ], thinness[testFolds$Fold10, ]
                        )
sansTestThinness = list(thinness[-testFolds$Fold01, ], thinness[-testFolds$Fold02, ],
                        thinness[-testFolds$Fold03, ], thinness[-testFolds$Fold04, ],
                        thinness[-testFolds$Fold05, ], thinness[-testFolds$Fold06, ],
                        thinness[-testFolds$Fold07, ], thinness[-testFolds$Fold08, ],
                        thinness[-testFolds$Fold09, ], thinness[-testFolds$Fold10, ]
                        )
UBThinness = list()
for (i in 1:10){
  curSet = sansTestThinness[[i]]
  validationFolds = createFolds(sansTestThinness[[i]]$Thinness, k = 10)
  UBThinness[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinness = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinness[[i]] = list()
  for (j in 1:10){
    SMOTEThinness[[i]][[j]] = SMOTE(X = as.data.frame(UBThinness[[i]][[j]]),
                              target = UBThinness[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEThinness[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(thinnessMale$Thinness, k = 10) #Create k splits
testSetsThinnessMale = list(thinnessMale[testFolds$Fold01, ], thinnessMale[testFolds$Fold02, ],
                        thinnessMale[testFolds$Fold03, ], thinnessMale[testFolds$Fold04, ],
                        thinnessMale[testFolds$Fold05, ], thinnessMale[testFolds$Fold06, ],
                        thinnessMale[testFolds$Fold07, ], thinnessMale[testFolds$Fold08, ],
                        thinnessMale[testFolds$Fold09, ], thinnessMale[testFolds$Fold10, ]
                        )
sansTestThinnessMale = list(thinnessMale[-testFolds$Fold01, ], thinnessMale[-testFolds$Fold02, ],
                        thinnessMale[-testFolds$Fold03, ], thinnessMale[-testFolds$Fold04, ],
                        thinnessMale[-testFolds$Fold05, ], thinnessMale[-testFolds$Fold06, ],
                        thinnessMale[-testFolds$Fold07, ], thinnessMale[-testFolds$Fold08, ],
                        thinnessMale[-testFolds$Fold09, ], thinnessMale[-testFolds$Fold10, ]
                        )
UBThinnessMale = list()
for (i in 1:10){
  curSet = sansTestThinnessMale[[i]]
  validationFolds = createFolds(sansTestThinnessMale[[i]]$Thinness, k = 10)
  UBThinnessMale[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinnessMale = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinnessMale[[i]] = list()
  for (j in 1:10){
    SMOTEThinnessMale[[i]][[j]] = SMOTE(X = as.data.frame(UBThinnessMale[[i]][[j]]),
                              target = UBThinnessMale[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEThinnessMale[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(thinnessFemale$Thinness, k = 10) #Create k splits
testSetsThinnessFemale = list(thinnessFemale[testFolds$Fold01, ], thinnessFemale[testFolds$Fold02, ],
                        thinnessFemale[testFolds$Fold03, ], thinnessFemale[testFolds$Fold04, ],
                        thinnessFemale[testFolds$Fold05, ], thinnessFemale[testFolds$Fold06, ],
                        thinnessFemale[testFolds$Fold07, ], thinnessFemale[testFolds$Fold08, ],
                        thinnessFemale[testFolds$Fold09, ], thinnessFemale[testFolds$Fold10, ]
                        )
sansTestThinnessFemale = list(thinnessFemale[-testFolds$Fold01, ], thinnessFemale[-testFolds$Fold02, ],
                        thinnessFemale[-testFolds$Fold03, ], thinnessFemale[-testFolds$Fold04, ],
                        thinnessFemale[-testFolds$Fold05, ], thinnessFemale[-testFolds$Fold06, ],
                        thinnessFemale[-testFolds$Fold07, ], thinnessFemale[-testFolds$Fold08, ],
                        thinnessFemale[-testFolds$Fold09, ], thinnessFemale[-testFolds$Fold10, ]
                        )
UBThinnessFemale = list()
for (i in 1:10){
  curSet = sansTestThinnessFemale[[i]]
  validationFolds = createFolds(sansTestThinnessFemale[[i]]$Thinness, k = 10)
  UBThinnessFemale[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinnessFemale = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinnessFemale[[i]] = list()
  for (j in 1:10){
    SMOTEThinnessFemale[[i]][[j]] = SMOTE(X = as.data.frame(UBThinnessFemale[[i]][[j]]),
                              target = UBThinnessFemale[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEThinnessFemale[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(thinnessUrban$Thinness, k = 10) #Create k splits
testSetsThinnessUrban = list(thinnessUrban[testFolds$Fold01, ], thinnessUrban[testFolds$Fold02, ],
                        thinnessUrban[testFolds$Fold03, ], thinnessUrban[testFolds$Fold04, ],
                        thinnessUrban[testFolds$Fold05, ], thinnessUrban[testFolds$Fold06, ],
                        thinnessUrban[testFolds$Fold07, ], thinnessUrban[testFolds$Fold08, ],
                        thinnessUrban[testFolds$Fold09, ], thinnessUrban[testFolds$Fold10, ]
                        )
sansTestThinnessUrban = list(thinnessUrban[-testFolds$Fold01, ], thinnessUrban[-testFolds$Fold02, ],
                        thinnessUrban[-testFolds$Fold03, ], thinnessUrban[-testFolds$Fold04, ],
                        thinnessUrban[-testFolds$Fold05, ], thinnessUrban[-testFolds$Fold06, ],
                        thinnessUrban[-testFolds$Fold07, ], thinnessUrban[-testFolds$Fold08, ],
                        thinnessUrban[-testFolds$Fold09, ], thinnessUrban[-testFolds$Fold10, ]
                        )
UBThinnessUrban = list()
for (i in 1:10){
  curSet = sansTestThinnessUrban[[i]]
  validationFolds = createFolds(sansTestThinnessUrban[[i]]$Thinness, k = 10)
  UBThinnessUrban[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinnessUrban = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinnessUrban[[i]] = list()
  for (j in 1:10){
    SMOTEThinnessUrban[[i]][[j]] = SMOTE(X = as.data.frame(UBThinnessUrban[[i]][[j]]),
                              target = UBThinnessUrban[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEThinnessUrban[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(thinnessNotUrban$Thinness, k = 10) #Create k splits
testSetsThinnessNotUrban = list(thinnessNotUrban[testFolds$Fold01, ], thinnessNotUrban[testFolds$Fold02, ],
                        thinnessNotUrban[testFolds$Fold03, ], thinnessNotUrban[testFolds$Fold04, ],
                        thinnessNotUrban[testFolds$Fold05, ], thinnessNotUrban[testFolds$Fold06, ],
                        thinnessNotUrban[testFolds$Fold07, ], thinnessNotUrban[testFolds$Fold08, ],
                        thinnessNotUrban[testFolds$Fold09, ], thinnessNotUrban[testFolds$Fold10, ]
                        )
sansTestThinnessNotUrban = list(thinnessNotUrban[-testFolds$Fold01, ], thinnessNotUrban[-testFolds$Fold02, ],
                        thinnessNotUrban[-testFolds$Fold03, ], thinnessNotUrban[-testFolds$Fold04, ],
                        thinnessNotUrban[-testFolds$Fold05, ], thinnessNotUrban[-testFolds$Fold06, ],
                        thinnessNotUrban[-testFolds$Fold07, ], thinnessNotUrban[-testFolds$Fold08, ],
                        thinnessNotUrban[-testFolds$Fold09, ], thinnessNotUrban[-testFolds$Fold10, ]
                        )
UBThinnessNotUrban = list()
for (i in 1:10){
  curSet = sansTestThinnessNotUrban[[i]]
  validationFolds = createFolds(sansTestThinnessNotUrban[[i]]$Thinness, k = 10)
  UBThinnessNotUrban[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinnessNotUrban = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinnessNotUrban[[i]] = list()
  for (j in 1:10){
    SMOTEThinnessNotUrban[[i]][[j]] = SMOTE(X = as.data.frame(UBThinnessNotUrban[[i]][[j]]),
                              target = UBThinnessNotUrban[[i]][[j]]$Thinness,
                              K = 4, #K had to be reduced; there weren't k = 5 minority class neighbors to extrapolate from, only 4
                              dup_size = 0
                              )
     SMOTEThinnessNotUrban[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBThinness[[1]][[1]]$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEThinness[[1]][[1]]$data$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (SMOTE)')
# abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestThinness = rbind(testSetsThinness[1:10])
# ForFS_UBThinness = rbind(trainSetsUBThinness[1:10])
# ForFS_SMOTEThinness = rbind(trainSetsSMOTEThinness[1:10])

# write.csv(ForFS_TestThinness, "ForFS_TestThinness.csv")
# write.csv(ForFS_UBThinness, "ForFS_UBThinness.csv")
# write.csv(ForFS_SMOTEThinness, "ForFS_SMOTEThinness.csv")
```



```{r Wasting Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
set.seed(13)
testFolds = createFolds(wasting$Wasting, k = 10) #Create k splits
testSetsWasting = list(wasting[testFolds$Fold01, ], wasting[testFolds$Fold02, ],
                        wasting[testFolds$Fold03, ], wasting[testFolds$Fold04, ],
                        wasting[testFolds$Fold05, ], wasting[testFolds$Fold06, ],
                        wasting[testFolds$Fold07, ], wasting[testFolds$Fold08, ],
                        wasting[testFolds$Fold09, ], wasting[testFolds$Fold10, ]
                        )
sansTestWasting = list(wasting[-testFolds$Fold01, ], wasting[-testFolds$Fold02, ],
                        wasting[-testFolds$Fold03, ], wasting[-testFolds$Fold04, ],
                        wasting[-testFolds$Fold05, ], wasting[-testFolds$Fold06, ],
                        wasting[-testFolds$Fold07, ], wasting[-testFolds$Fold08, ],
                        wasting[-testFolds$Fold09, ], wasting[-testFolds$Fold10, ]
                        )
UBWasting = list()
for (i in 1:10){
  curSet = sansTestWasting[[i]]
  validationFolds = createFolds(sansTestWasting[[i]]$Wasting, k = 10)
  UBWasting[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEWasting = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEWasting[[i]] = list()
  for (j in 1:10){
    SMOTEWasting[[i]][[j]] = SMOTE(X = as.data.frame(UBWasting[[i]][[j]]),
                              target = UBWasting[[i]][[j]]$Wasting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEWasting[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(wastingMale$Wasting, k = 10) #Create k splits
testSetsWastingMale = list(wastingMale[testFolds$Fold01, ], wastingMale[testFolds$Fold02, ],
                        wastingMale[testFolds$Fold03, ], wastingMale[testFolds$Fold04, ],
                        wastingMale[testFolds$Fold05, ], wastingMale[testFolds$Fold06, ],
                        wastingMale[testFolds$Fold07, ], wastingMale[testFolds$Fold08, ],
                        wastingMale[testFolds$Fold09, ], wastingMale[testFolds$Fold10, ]
                        )
sansTestWastingMale = list(wastingMale[-testFolds$Fold01, ], wastingMale[-testFolds$Fold02, ],
                        wastingMale[-testFolds$Fold03, ], wastingMale[-testFolds$Fold04, ],
                        wastingMale[-testFolds$Fold05, ], wastingMale[-testFolds$Fold06, ],
                        wastingMale[-testFolds$Fold07, ], wastingMale[-testFolds$Fold08, ],
                        wastingMale[-testFolds$Fold09, ], wastingMale[-testFolds$Fold10, ]
                        )
UBWastingMale = list()
for (i in 1:10){
  curSet = sansTestWastingMale[[i]]
  validationFolds = createFolds(sansTestWastingMale[[i]]$Wasting, k = 10)
  UBWastingMale[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEWastingMale = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEWastingMale[[i]] = list()
  for (j in 1:10){
    SMOTEWastingMale[[i]][[j]] = SMOTE(X = as.data.frame(UBWastingMale[[i]][[j]]),
                              target = UBWastingMale[[i]][[j]]$Wasting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEWastingMale[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(wastingFemale$Wasting, k = 10) #Create k splits
testSetsWastingFemale = list(wastingFemale[testFolds$Fold01, ], wastingFemale[testFolds$Fold02, ],
                        wastingFemale[testFolds$Fold03, ], wastingFemale[testFolds$Fold04, ],
                        wastingFemale[testFolds$Fold05, ], wastingFemale[testFolds$Fold06, ],
                        wastingFemale[testFolds$Fold07, ], wastingFemale[testFolds$Fold08, ],
                        wastingFemale[testFolds$Fold09, ], wastingFemale[testFolds$Fold10, ]
                        )
sansTestWastingFemale = list(wastingFemale[-testFolds$Fold01, ], wastingFemale[-testFolds$Fold02, ],
                        wastingFemale[-testFolds$Fold03, ], wastingFemale[-testFolds$Fold04, ],
                        wastingFemale[-testFolds$Fold05, ], wastingFemale[-testFolds$Fold06, ],
                        wastingFemale[-testFolds$Fold07, ], wastingFemale[-testFolds$Fold08, ],
                        wastingFemale[-testFolds$Fold09, ], wastingFemale[-testFolds$Fold10, ]
                        )
UBWastingFemale = list()
for (i in 1:10){
  curSet = sansTestWastingFemale[[i]]
  validationFolds = createFolds(sansTestWastingFemale[[i]]$Wasting, k = 10)
  UBWastingFemale[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEWastingFemale = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEWastingFemale[[i]] = list()
  for (j in 1:10){
    SMOTEWastingFemale[[i]][[j]] = SMOTE(X = as.data.frame(UBWastingFemale[[i]][[j]]),
                              target = UBWastingFemale[[i]][[j]]$Wasting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEWastingFemale[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(wastingUrban$Wasting, k = 10) #Create k splits
testSetsWastingUrban = list(wastingUrban[testFolds$Fold01, ], wastingUrban[testFolds$Fold02, ],
                        wastingUrban[testFolds$Fold03, ], wastingUrban[testFolds$Fold04, ],
                        wastingUrban[testFolds$Fold05, ], wastingUrban[testFolds$Fold06, ],
                        wastingUrban[testFolds$Fold07, ], wastingUrban[testFolds$Fold08, ],
                        wastingUrban[testFolds$Fold09, ], wastingUrban[testFolds$Fold10, ]
                        )
sansTestWastingUrban = list(wastingUrban[-testFolds$Fold01, ], wastingUrban[-testFolds$Fold02, ],
                        wastingUrban[-testFolds$Fold03, ], wastingUrban[-testFolds$Fold04, ],
                        wastingUrban[-testFolds$Fold05, ], wastingUrban[-testFolds$Fold06, ],
                        wastingUrban[-testFolds$Fold07, ], wastingUrban[-testFolds$Fold08, ],
                        wastingUrban[-testFolds$Fold09, ], wastingUrban[-testFolds$Fold10, ]
                        )
UBWastingUrban = list()
for (i in 1:10){
  curSet = sansTestWastingUrban[[i]]
  validationFolds = createFolds(sansTestWastingUrban[[i]]$Wasting, k = 10)
  UBWastingUrban[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEWastingUrban = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEWastingUrban[[i]] = list()
  for (j in 1:10){
    SMOTEWastingUrban[[i]][[j]] = SMOTE(X = as.data.frame(UBWastingUrban[[i]][[j]]),
                              target = UBWastingUrban[[i]][[j]]$Wasting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEWastingUrban[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################
testFolds = createFolds(wastingNotUrban$Wasting, k = 10) #Create k splits
testSetsWastingNotUrban = list(wastingNotUrban[testFolds$Fold01, ], wastingNotUrban[testFolds$Fold02, ],
                        wastingNotUrban[testFolds$Fold03, ], wastingNotUrban[testFolds$Fold04, ],
                        wastingNotUrban[testFolds$Fold05, ], wastingNotUrban[testFolds$Fold06, ],
                        wastingNotUrban[testFolds$Fold07, ], wastingNotUrban[testFolds$Fold08, ],
                        wastingNotUrban[testFolds$Fold09, ], wastingNotUrban[testFolds$Fold10, ]
                        )
sansTestWastingNotUrban = list(wastingNotUrban[-testFolds$Fold01, ], wastingNotUrban[-testFolds$Fold02, ],
                        wastingNotUrban[-testFolds$Fold03, ], wastingNotUrban[-testFolds$Fold04, ],
                        wastingNotUrban[-testFolds$Fold05, ], wastingNotUrban[-testFolds$Fold06, ],
                        wastingNotUrban[-testFolds$Fold07, ], wastingNotUrban[-testFolds$Fold08, ],
                        wastingNotUrban[-testFolds$Fold09, ], wastingNotUrban[-testFolds$Fold10, ]
                        )
UBWastingNotUrban = list()
for (i in 1:10){
  curSet = sansTestWastingNotUrban[[i]]
  validationFolds = createFolds(sansTestWastingNotUrban[[i]]$Wasting, k = 10)
  UBWastingNotUrban[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEWastingNotUrban = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEWastingNotUrban[[i]] = list()
  for (j in 1:10){
    SMOTEWastingNotUrban[[i]][[j]] = SMOTE(X = as.data.frame(UBWastingNotUrban[[i]][[j]]),
                              target = UBWastingNotUrban[[i]][[j]]$Wasting,
                              K = 5,
                              dup_size = 0
                              )
     SMOTEWastingNotUrban[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}
################################################################################

# Barplots for visualization of class imbalance
#####
barplot(prop.table(table(UBWasting[[1]][[1]]$Wasting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'Wasting Training Set Class Distribution (Unbalanced)')
abline(h = 0)

barplot(prop.table(table(SMOTEWasting[[1]][[1]]$data$Wasting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'Wasting Training Set Class Distribution (SMOTE)')
abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestWastingYoung = rbind(testSetsWastingYoung[1:10])
# ForFS_UBWastingYoung = rbind(trainSetsUBWastingYoung[1:10])
# ForFS_SMOTEWastingYoung = rbind(trainSetsSMOTEWastingYoung[1:10])

# write.csv(ForFS_TestWastingYoung, "ForFS_TestWastingYoung.csv")
# write.csv(ForFS_UBWastingYoung, "ForFS_UBWastingYoung.csv")
# write.csv(ForFS_SMOTEWastingYoung, "ForFS_SMOTEWastingYoung.csv")
```


#####
# FEATURE SELECTION: Selects important model features to reduce noise and help 
# prevent overfitting. All feature selection methods should be run on SMOTE-
# balanced datasets.
#####
```{r ReliefF Computations}
#This one takes ~90 minutes to run. Have fun waiting.
library(FSelectorRcpp)

################################################################################
FS_RLF_Stunting = list()
for (i in 1:10){
  FS_RLF_Stunting[[i]] = list()
  for (j in 1:10){
    relF = relief(Stunting~.,
                as.data.frame(SMOTEStunting[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Stunting[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_Thinness = list()
for (i in 1:10){
  FS_RLF_Thinness[[i]] = list()
  for (j in 1:10){
    relF = relief(Thinness~.,
                as.data.frame(SMOTEThinness[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Thinness[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_StuntingYoung = list()
for (i in 1:10){
  FS_RLF_StuntingYoung[[i]] = list()
  for (j in 1:10){
    relF = relief(Stunting~.,
                as.data.frame(SMOTEStuntingYoung[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_StuntingYoung[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_ThinnessYoung = list()
for (i in 1:10){
  FS_RLF_ThinnessYoung[[i]] = list()
  for (j in 1:10){
    relF = relief(Thinness~.,
                as.data.frame(SMOTEThinnessYoung[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_ThinnessYoung[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_WastingYoung = list()
for (i in 1:10){
  FS_RLF_WastingYoung[[i]] = list()
  for (j in 1:10){
    relF = relief(Wasting~.,
                as.data.frame(SMOTEWastingYoung[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_WastingYoung[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
```



```{r ReliefF Results}
#Separated from computations so I don't have to wait an hour and a half to see if my new code can parse the results correctly
################################################################################
FS_RLF = data.frame(Stunting = double(),
                       Thinness = double(),
                       StuntingYoung = double(),
                       ThinnessYoung = double(),
                       WastingYoung = double()
                       )
for (i in 1:54){
 FS_RLF[i,] = 0
}
row.names(FS_RLF) = names(stunting)[1:54]
################################################################################
COPY = FS_RLF_Stunting
for (factor in names(stunting)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 1]] = sum/100
}
################################################################################
COPY = FS_RLF_Thinness
for (factor in names(thinness)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 2]] = sum/100
}
################################################################################
COPY = FS_RLF_StuntingYoung
for (factor in names(StuntingYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 3]] = sum/100
}
################################################################################
COPY = FS_RLF_ThinnessYoung
for (factor in names(thinnessYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 4]] = sum/100
}
################################################################################
COPY = FS_RLF_WastingYoung
for (factor in names(WastingYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 5]] = sum/100
}
write.csv(AVG_FS_RLF, "FS_RLF_Avg.csv")
################################################################################
```



```{r InfoGain}
#This one is pretty quick.
library(FSelectorRcpp)

FS_IG = data.frame(Stunting = double(),
                       Thinness = double(),
                       StuntingYoung = double(),
                       ThinnessYoung = double(),
                       WastingYoung = double()
                       )
for (i in 1:54){
 FS_IG[i,] = 0
}
row.names(FS_IG) = names(stunting)[1:54]
################################################################################
for (i in 1:10){
  for (j in 1:10){
    infogain = information_gain(formula = Stunting~.,
                              data = as.data.frame(SMOTEStunting[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    infogain$importance = sort(infogain$importance, decreasing = TRUE)
    for (k in 1:20){
      factor = infogain[[k, 1]]
      if (infogain[[k, 2]] > 0){
        FS_IG[[factor, 1]] = FS_IG[[factor, 1]] + 1
      }
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    infogain = information_gain(formula = Thinness~.,
                              data = as.data.frame(SMOTEThinness[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    infogain$importance = sort(infogain$importance, decreasing = TRUE)
    for (k in 1:20){
      factor = infogain[[k, 1]]
      if (infogain[[k, 2]] > 0){
        FS_IG[[factor, 2]] = FS_IG[[factor, 2]] + 1
      }
    }
  }
}#########################################################################
for (i in 1:10){
  for (j in 1:10){
    infogain = information_gain(formula = Stunting~.,
                              data = as.data.frame(SMOTEStuntingYoung[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    infogain$importance = sort(infogain$importance, decreasing = TRUE)
    for (k in 1:20){
      factor = infogain[[k, 1]]
      if (infogain[[k, 2]] > 0){
        FS_IG[[factor, 3]] = FS_IG[[factor, 3]] + 1
      }
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    infogain = information_gain(formula = Thinness~.,
                              data = as.data.frame(SMOTEThinnessYoung[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    infogain$importance = sort(infogain$importance, decreasing = TRUE)
    for (k in 1:20){
      factor = infogain[[k, 1]]
      if (infogain[[k, 2]] > 0){
        FS_IG[[factor, 4]] = FS_IG[[factor, 4]] + 1
      }
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    infogain = information_gain(formula = Wasting~.,
                              data = as.data.frame(SMOTEWastingYoung[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    infogain$importance = sort(infogain$importance, decreasing = TRUE)
    for (k in 1:20){
      factor = infogain[[k, 1]]
      if (infogain[[k, 2]] > 0){
        FS_IG[[factor, 5]] = FS_IG[[factor, 5]] + 1
      }
    }
  }
}
################################################################################
write.csv(FS_IG, "FS_IG.csv")
```



```{r mRMR}
library(mRMRe)
#This one is pretty quick.

FS_MRMR = data.frame(Stunting = double(),
                       Thinness = double(),
                       StuntingYoung = double(),
                       ThinnessYoung = double(),
                       WastingYoung = double()
                       )
for (i in 1:54){
 FS_MRMR[i,] = 0
}
row.names(FS_MRMR) = names(stunting)[1:54]
################################################################################
for (i in 1:10){
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEStunting[[i]][[j]]$data)
    results = mRMR.classic("mRMRe.Filter",
                           data = mrmr,
                           target_indices = 55,
                           feature_count = 20,
                           method = "exhaustive"
                           )
    # results = mRMR.ensemble(#"mRMRe.Filter",
    #                        data = mrmr,#SMOTEStunting[[i]][[j]]$data,
    #                        target_indices = 55,
    #                        solution_count = 20,
    #                        feature_count  = 20,
    #                        method = "exhaustive"
    #                        )
    #featureNames(results)
    for (k in 1:20){
      factorInd = results@filters[["55"]][[k,1]]
      factor = names(stunting)[[factorInd]]
      FS_MRMR[[factor, 1]] = FS_MRMR[[factor, 1]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEThinness[[i]][[j]]$data)
    results = mRMR.classic("mRMRe.Filter",
                           data = mrmr,
                           target_indices = 55,
                           feature_count = 20,
                           method = "exhaustive"
                           )
    for (k in 1:20){
      factorInd = results@filters[["55"]][[k]]
      factor = names(stunting)[[factorInd]]
      FS_MRMR[[factor, 2]] = FS_MRMR[[factor, 2]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEStuntingYoung[[i]][[j]]$data)
    results = mRMR.classic("mRMRe.Filter",
                           data = mrmr,
                           target_indices = 55,
                           feature_count = 20,
                           method = "exhaustive"
                           )
    for (k in 1:20){
      factorInd = results@filters[["55"]][[k]]
      factor = names(stunting)[[factorInd]]
      FS_MRMR[[factor, 3]] = FS_MRMR[[factor, 3]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEThinnessYoung[[i]][[j]]$data)
    results = mRMR.classic("mRMRe.Filter",
                           data = mrmr,
                           target_indices = 55,
                           feature_count = 20,
                           method = "exhaustive"
                           )
    for (k in 1:20){
      factorInd = results@filters[["55"]][[k]]
      factor = names(stunting)[[factorInd]]
      FS_MRMR[[factor, 4]] = FS_MRMR[[factor, 4]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEWastingYoung[[i]][[j]]$data)
    results = mRMR.classic("mRMRe.Filter",
                           data = mrmr,
                           target_indices = 55,
                           feature_count = 20,
                           method = "exhaustive"
                           )
    for (k in 1:20){
      factorInd = results@filters[["55"]][[k]]
      factor = names(stunting)[[factorInd]]
      FS_MRMR[[factor, 5]] = FS_MRMR[[factor, 5]] + 1
    }
  }
}
################################################################################
write.csv(FS_MRMR, "FS_MRMR.csv")
```



```{r JMI}
#This one is pretty quick.
library(praznik)

FS_JMI = data.frame(Stunting = double(),
                       Thinness = double(),
                       StuntingYoung = double(),
                       ThinnessYoung = double(),
                       WastingYoung = double()
                       )
for (i in 1:54){
 FS_JMI[i,] = 0
}
row.names(FS_JMI) = names(stunting)[1:54]
################################################################################
for (i in 1:10){
  for (j in 1:10){
    FS_JMI_Stunting = JMI(SMOTEStunting[[i]][[j]]$data[1:54], 
                                    SMOTEStunting[[i]][[j]]$data$Stunting,
                                    k = 20
                                    )
    for (k in 1:20){
      factor = names(stunting)[FS_JMI_Stunting$selection[[k]]]
      FS_JMI[[factor, 1]] = FS_JMI[[factor, 1]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    FS_JMI_Thinness = JMI(SMOTEThinness[[i]][[j]]$data[1:54],
                                    SMOTEThinness[[i]][[j]]$data$Thinness,
                                    k = 20
                                    )
    for (k in 1:20){
      factor = names(stunting)[FS_JMI_Thinness$selection[[k]]]
      FS_JMI[[factor, 2]] = FS_JMI[[factor, 2]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    FS_JMI_StuntingYoung = JMI(SMOTEStuntingYoung[[i]][[j]]$data[1:54], 
                                    SMOTEStuntingYoung[[i]][[j]]$data$Stunting,
                                    k = 20
                                    )
    for (k in 1:20){
      factor = names(stunting)[FS_JMI_StuntingYoung$selection[[k]]]
      FS_JMI[[factor, 3]] = FS_JMI[[factor, 3]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    FS_JMI_ThinnessYoung = JMI(SMOTEThinnessYoung[[i]][[j]]$data[1:54], 
                                    SMOTEThinnessYoung[[i]][[j]]$data$Thinness,
                                    k = 20
                                    )
    for (k in 1:20){
      factor = names(stunting)[FS_JMI_ThinnessYoung$selection[[k]]]
      FS_JMI[[factor, 4]] = FS_JMI[[factor, 4]] + 1
    }
  }
}
################################################################################
for (i in 1:10){
  for (j in 1:10){
    FS_JMI_WastingYoung = JMI(SMOTEWastingYoung[[i]][[j]]$data[1:54], 
                                    SMOTEWastingYoung[[i]][[j]]$data$Wasting,
                                    k = 20
                                    )
    for (k in 1:20){
      factor = names(stunting)[FS_JMI_WastingYoung$selection[[k]]]
      FS_JMI[[factor, 5]] = FS_JMI[[factor, 5]] + 1
    }
  }
}
################################################################################
write.csv(FS_JMI, "FS_JMI.csv")
```



```{r Repeated Univariate Logistic Regression (FS), warning = FALSE}
#This one takes ~5 minutes.
#Repeated Univariate Logistic Regression for FS in training models

library(broom)
library(car)
library(performance)
library(MASS)

results = data.frame(StuntingRejects = double(), StuntingOR = double(),
                     ThinnessRejects = double(), ThinnessOR = double(),
                     StuntingYoungRejects = double(), StuntingYoungOR = double(),
                     ThinnessYoungRejects = double(), ThinnessYoungOR = double(),
                     WastingYoungRejects = double(), WastingYoungOR = double()
                     )
################################################################################
#Stunting PV-based feature selection & crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 1]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Stunting","~",factor),
                    data = SMOTEStunting[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 1]] = results[[factor, 1]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 2]] = CORSum/100
}
################################################################################
#Thinness PV-based feature selection & crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 3]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Thinness","~",factor),
                    data = SMOTEThinness[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 3]] = results[[factor, 3]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 4]] = CORSum/100
}
################################################################################
#StuntingYoung PV-based feature selection & crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 5]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Stunting","~",factor),
                    data = SMOTEStuntingYoung[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 5]] = results[[factor, 5]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 6]] = CORSum/100
}
################################################################################
#ThinnessYoung PV-based feature selection & crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 7]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Thinness","~",factor),
                    data = SMOTEThinnessYoung[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 7]] = results[[factor, 7]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 8]] = CORSum/100
}
################################################################################
#WastingYoung PV-based feature selection & crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 9]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Wasting","~",factor),
                    data = SMOTEWastingYoung[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 9]] = results[[factor, 9]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 10]] = CORSum/100
}
################################################################################
write.csv(results, "FS_R-UVLoR.csv")
```



```{r Repeated Multivariate Logistic Regression (FS), warning = FALSE}
#Takes ~2 minutes.
results = data.frame(StuntingRejects = double(), StuntingAOR = double(),
                     ThinnessRejects = double(), ThinnessAOR = double(),
                     StuntingYoungRejects = double(), StuntingYoungAOR = double(),
                     ThinnessYoungRejects = double(), ThinnessYoungAOR = double(),
                     WastingYoungRejects = double(), WastingYoungAOR = double()
                     )
################################################################################
#Stunting PV-based feature selection & approximate odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 1]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 2]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Stunting~.,
                data = SMOTEStunting[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 1]] = results[[factor, 1]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){ #Explained below
        results[[factor, 2]] = results[[factor, 2]] + regSummary[[index, 2]]
      }
    }
  }
}
#Dividing AOR by # of times we reject H0; had issue before where a factor couldn't be rejected but had a very high p-value and thus generated an absurd OR.
#Now we're only incrementing a factor's OR when it is significant, and dividing it by the # of times it is significant.
#We lose data every time a factor isn't significant, but in exchange get reliable AOR for use in describing the effect of significant factors
for (factor in names(stunting)[1:54]){
  results[[factor, 2]] = results[[factor, 2]]/results[[factor, 1]] 
}
################################################################################
#Thinness PV-based feature selection & approximate odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 3]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 4]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Thinness~.,
                data = SMOTEThinness[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 3]] = results[[factor, 3]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 4]] = results[[factor, 4]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 4]] = results[[factor, 4]]/results[[factor, 3]] 
}
################################################################################
#StuntingYoung PV-based feature selection & approximate odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 5]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 6]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Stunting~.,
                data = SMOTEStuntingYoung[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 5]] = results[[factor, 5]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 6]] = results[[factor, 6]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 6]] = results[[factor, 6]]/results[[factor, 5]] 
}
################################################################################
#ThinnessYoung PV-based feature selection & approximate odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 7]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 8]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Thinness~.,
                data = SMOTEThinnessYoung[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 7]] = results[[factor, 7]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 8]] = results[[factor, 8]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 8]] = results[[factor, 8]]/results[[factor, 7]] 
}
################################################################################
#WastingYoung PV-based feature selection & approximate odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 9]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 10]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Wasting~.,
                data = SMOTEWastingYoung[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 9]] = results[[factor, 9]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 10]] = results[[factor, 10]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 10]] = results[[factor, 10]]/results[[factor, 9]] 
}
################################################################################
write.csv(results, "FS_R-MVLoR.csv")
```



```{r Literature-Based}
#This one is pretty quick (it's almost like there's no computations...)
FS_LB = c(1:3, 4, 6:15, 17:19, 20:25, 27:29, 32:36, 38, 47:54)
FS_LB = sort(FS_LB)
for (i in FS_LB){
  cat(names(stunting)[i],"\n")
}
```



#####
# CLASSIFIER MODELLING:
#####
```{r Model Data Preparation}
#Creating train/test sets of selected features from original datasets for use in classifier modelling.

```



```{r Random Forest}
library(randomForest)
library(datasets)
library(caret)

```



```{r Support Vector Machine}

```



```{r Extreme Gradient Boosting}
library(xgboost)
library(caret)
library(tidyverse)


```



```{r Multivariate Logistic Regression (OR), warning = FALSE}
#This MVLoR is for Odds Ratios, uses entire dataset.
#NOT used for feature selection for classifier models; do not mistake this data for the training/test set-derived repeated multivariate logistic regression above.
#library(devtools)
library(broom)
library(car)
library(performance)
library(MASS)

OR_Stunting = glm(Stunting~., data = stunting, family = "binomial")
write.csv(cbind(
          tidy(OR_Stunting, exponentiate = TRUE, conf.level = 0.95),
          p.adjust(summary(OR_Stunting)$coefficients[1:41,4],method = "BH"),
          exp(confint(OR_Stunting))),
          "OR_Stunting.csv"
          )

StepStunting = stepAIC(OR_Stunting, direction = "both", trace = FALSE)   # Stepwise regression model
tidy(StepStunting, exponentiate = TRUE, conf.level = 0.95)
################################################################################
OR_Thinness = glm(Thinness~., data = thinness, family = "binomial")
write.csv(cbind(
          tidy(OR_Thinness, exponentiate = TRUE, conf.level = 0.95),
          p.adjust(summary(OR_Thinness)$coefficients[1:41,4],method = "BH"),
          exp(confint(OR_Thinness))),
          "OR_Thinness.csv"
          )
StepThinness = stepAIC(OR_Thinness, direction = "both", trace = FALSE)   # Stepwise regression model
tidy(StepThinness, exponentiate = TRUE, conf.level = 0.95)
################################################################################
OR_Wasting = glm(Wasting~., data = wasting, family = "binomial")
write.csv(cbind(
          tidy(OR_Wasting, exponentiate = TRUE, conf.level = 0.95),
          p.adjust(summary(OR_Wasting)$coefficients[1:41,4],method = "BH"),
          exp(confint(OR_Wasting))),
          "OR_Wasting.csv"
          )
StepWasting = stepAIC(OR_Wasting, direction = "both", trace = FALSE)   # Stepwise regression model
tidy(StepWasting, exponentiate = TRUE, conf.level = 0.95)

# ################################################################################
# OR_Stunting = glm(Stunting~., data = stunting, family = "binomial")
# #summary(OR_Stunting)
# OddsRatio = exp(cbind(OR = coef(OR_Stunting), confint(na.omit(OR_Stunting))))
# #BH = p.adjust(summary(OR_Stunting)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_Stunting$coefficients[,4])), "OR_Stunting.csv")
# 
# ################################################################################
# OR_Thinness = glm(Thinness~., data = thinness, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_Thinness), confint(na.omit(OR_Thinness))))
# #BH = p.adjust(summary(OR_Thinness)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_Thinness$coefficients[,4])), "OR_Thinness.csv")
# 
# ################################################################################
# OR_StuntingYoung = glm(Stunting~., data = stuntingYoung, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_StuntingYoung), confint(na.omit(OR_StuntingYoung))))
# #BH = p.adjust(summary(OR_StuntingYoung)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_StuntingYoung$coefficients[,4])), "OR_StuntingYoung.csv")
# 
# ################################################################################
# OR_ThinnessYoung = glm(Thinness~., data = thinnessYoung, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_ThinnessYoung), confint(na.omit(OR_ThinnessYoung))))
# #BH = p.adjust(summary(OR_ThinnessYoung)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_ThinnessYoung$coefficients[,4])), "OR_ThinnessYoung.csv")
# 
# ################################################################################
# OR_WastingYoung = glm(Wasting~., data = wastingYoung, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_WastingYoung), confint(na.omit(OR_WastingYoung))))
# #BH = p.adjust(summary(OR_WastingYoung)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_WastingYoung$coefficients[,4])), "OR_WastingYoung.csv")

```

#####
# ASSOCIATION RULE LEARNING:
#####
```{r ARL Data Preparation, warning = FALSE}
#Preparing SMOTE-balancing of entire datasets for use in association rule learning. Datasets previously balanced with SMOTE were only training data for use in Feature Selection, and thus were subsets of ~81% of the original data. Ours will use the remaining 19% for ARL.
#I.e. The SMOTE balancing we did before didn't include everything, but we want everything for ARL. So now we have to make new balanced datasets with everything in them.

ARL_Stunting_Older = SMOTE(X = stunting,
                     target = stunting$Stunting,
                     K = 5,
                     dup_size = 2
                     )
ARL_Stunting_Older = ARL_Stunting_Older$data
barplot(prop.table(table(ARL_Stunting_Older$Stunting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Stunting",
        names = c("No", "Yes"),
        main = 'ARL_Stunting_Older SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Thinness_Older = SMOTE(X = thinness,
                     target = thinness$Thinness,
                     K = 5,
                     dup_size = 22
                     )
ARL_Thinness_Older = ARL_Thinness_Older$data
barplot(prop.table(table(ARL_Thinness_Older$Thinness)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Stunting",
        names = c("No", "Yes"),
        main = 'ARL_Thinness_Older SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Stunting_Young = SMOTE(X = StuntingYoung,
                     target = StuntingYoung$Stunting,
                     K = 5,
                     dup_size = 3
                     )
ARL_Stunting_Young = ARL_Stunting_Young$data
barplot(prop.table(table(ARL_Stunting_Young$Stunting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Stunting",
        names = c("No", "Yes"),
        main = 'ARL_Stunting_Young SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Thinness_Young = SMOTE(X = thinnessYoung,
                     target = thinnessYoung$Thinness,
                     K = 5,
                     dup_size = 30
                     )
ARL_Thinness_Young = ARL_Thinness_Young$data
barplot(prop.table(table(ARL_Thinness_Young$Thinness)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Stunting",
        names = c("No", "Yes"),
        main = 'ARL_Thinness_Young SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Wasting_Young = SMOTE(X = WastingYoung,
                     target = WastingYoung$Wasting,
                     K = 5,
                     dup_size = 9
                     )
ARL_Wasting_Young = ARL_Wasting_Young$data
barplot(prop.table(table(ARL_Wasting_Young$Wasting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Stunting",
        names = c("No", "Yes"),
        main = 'ARL_Wasting_Young SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Stunting_Male = subset(StuntingFull, Sex == 0)
ARL_Stunting_Male = SMOTE(X = ARL_Stunting_Male,
                     target = ARL_Stunting_Male$Stunting,
                     K = 5,
                     dup_size = 2
                     )
ARL_Stunting_Male = ARL_Stunting_Male$data
barplot(prop.table(table(ARL_Stunting_Male$Stunting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Stunting",
        names = c("No", "Yes"),
        main = 'ARL_Stunting_Male SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Stunting_Female = subset(StuntingFull, Sex == 1)
ARL_Stunting_Female = SMOTE(X = ARL_Stunting_Female,
                     target = ARL_Stunting_Female$Stunting,
                     K = 5,
                     dup_size = 3
                     )
ARL_Stunting_Female = ARL_Stunting_Female$data
barplot(prop.table(table(ARL_Stunting_Female$Stunting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Stunting",
        names = c("No", "Yes"),
        main = 'ARL_Stunting_Female SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Thinness_Male = subset(ThinnessFull, Sex == 0)
ARL_Thinness_Male = SMOTE(X = ARL_Thinness_Male,
                     target = ARL_Thinness_Male$Thinness,
                     K = 5,
                     dup_size = 27
                     )
ARL_Thinness_Male = ARL_Thinness_Male$data
barplot(prop.table(table(ARL_Thinness_Male$Thinness)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Thinness",
        names = c("No", "Yes"),
        main = 'ARL_Thinness_Male SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Thinness_Female = subset(ThinnessFull, Sex == 1)
ARL_Thinness_Female = SMOTE(X = ARL_Thinness_Female,
                     target = ARL_Thinness_Female$Thinness,
                     K = 5,
                     dup_size = 25
                     )
ARL_Thinness_Female = ARL_Thinness_Female$data
barplot(prop.table(table(ARL_Thinness_Female$Thinness)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Thinness",
        names = c("No", "Yes"),
        main = 'ARL_Thinness_Female SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Wasting_Male = subset(WastingFull, Sex == 0)
ARL_Wasting_Male = SMOTE(X = ARL_Wasting_Male,
                     target = ARL_Wasting_Male$Wasting,
                     K = 5,
                     dup_size = 7
                     )
ARL_Wasting_Male = ARL_Wasting_Male$data
barplot(prop.table(table(ARL_Wasting_Male$Wasting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Wasting_Male SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Wasting_Female = subset(WastingFull, Sex == 1)
ARL_Wasting_Female = SMOTE(X = ARL_Wasting_Female,
                     target = ARL_Wasting_Female$Wasting,
                     K = 5,
                     dup_size = 11
                     )
ARL_Wasting_Female = ARL_Wasting_Female$data
barplot(prop.table(table(ARL_Wasting_Female$Wasting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Wasting_Female SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Stunting_NotUrban = subset(StuntingFull, Suburban == 1 | Rural == 1)
ARL_Stunting_NotUrban = SMOTE(X = ARL_Stunting_NotUrban,
                     target = ARL_Stunting_NotUrban$Stunting,
                     K = 5,
                     dup_size = 2
                     )
ARL_Stunting_NotUrban = ARL_Stunting_NotUrban$data
barplot(prop.table(table(ARL_Stunting_NotUrban$Stunting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Stunting_NotUrban SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Stunting_Urban = subset(StuntingFull, Rural == 0 & Suburban == 0)
ARL_Stunting_Urban = SMOTE(X = ARL_Stunting_Urban,
                     target = ARL_Stunting_Urban$Stunting,
                     K = 5,
                     dup_size = 2
                     )
ARL_Stunting_Urban = ARL_Stunting_Urban$data
barplot(prop.table(table(ARL_Stunting_Urban$Stunting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Stunting_Urban SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Thinness_NotUrban = subset(ThinnessFull, Suburban == 1 | Rural == 1)
ARL_Thinness_NotUrban = SMOTE(X = ARL_Thinness_NotUrban,
                     target = ARL_Thinness_NotUrban$Thinness,
                     K = 5,
                     dup_size = 33
                     )
ARL_Thinness_NotUrban = ARL_Thinness_NotUrban$data
barplot(prop.table(table(ARL_Thinness_NotUrban$Thinness)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Thinness_NotUrban SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Thinness_Urban = subset(ThinnessFull, Rural == 0 & Suburban == 0)
ARL_Thinness_Urban = SMOTE(X = ARL_Thinness_Urban,
                     target = ARL_Thinness_Urban$Thinness,
                     K = 5,
                     dup_size = 25
                     )
ARL_Thinness_Urban = ARL_Thinness_Urban$data
barplot(prop.table(table(ARL_Thinness_Urban$Thinness)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Thinness_Urban SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Wasting_NotUrban = subset(WastingFull, Suburban == 1 | Rural == 1)
ARL_Wasting_NotUrban = SMOTE(X = ARL_Wasting_NotUrban,
                     target = ARL_Wasting_NotUrban$Wasting,
                     K = 5,
                     dup_size = 9
                     )
ARL_Wasting_NotUrban = ARL_Wasting_NotUrban$data
barplot(prop.table(table(ARL_Wasting_NotUrban$Wasting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Wasting_NotUrban SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
ARL_Wasting_Urban = subset(WastingFull, Rural == 0 & Suburban == 0)
ARL_Wasting_Urban = SMOTE(X = ARL_Wasting_Urban,
                     target = ARL_Wasting_Urban$Wasting,
                     K = 5,
                     dup_size = 9
                     )
ARL_Wasting_Urban = ARL_Wasting_Urban$data
barplot(prop.table(table(ARL_Wasting_Urban$Wasting)),
        col = c("white", "darkgrey"),
        ylim = c(0, 1),
        xlim = c(0, 3),
        ylab = "Prevalence (%)",
        xlab = "Wasting",
        names = c("No", "Yes"),
        main = 'ARL_Wasting_Urban SMOTE-Balanced Dataset Class Distribution'
        )
abline(h = 0)
################################################################################
```



```{r Association Rule Learning: Stunting & StuntingYoung}
#Pretty quick.
#I played around with support and confidence values until I got ~10 rules per iteration of the Apriori algorithm. 

#https://www.geeksforgeeks.org/apriori-algorithm-in-r-programming/

library(arules)
library(arulesViz)

ruleSet = data.frame(rules = character(),
                     support = numeric(),
                     confidence = numeric(),
                     coverage = numeric(),
                     lift = numeric(),
                     count = numeric())
################################################################################
#Male vs. Female associations on Stunting
ruleSet[nrow(ruleSet)+1, 1] = "STUNTING:"
data = data.frame(lapply(stunting, as.logical))


ruleSet[nrow(ruleSet)+1, 1] = "FEMALE:"
female = subset(data, Sex == TRUE)
female = female[,-2]
rules = apriori(female, 
                parameter = list(support = 0.05,
                                 confidence = 0.38,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#append(ruleSet, as(rules, "data.frame"))
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "MALE:"
male = subset(data, Sex == FALSE)
male = male[,-2]
rules = apriori(male, 
                parameter = list(support = 0.05,
                                 confidence = 0.71,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))
################################################################################
#Rural vs. suburban vs. urban rules on Stunting
ruleSet[nrow(ruleSet)+1, 1] = "RURAL:"
# rural = subset(data, Rural == TRUE)
# rural = rural[,-c(14,15)]
# rules = apriori(rural,
#                 parameter = list(support = 0.17,
#                                  confidence = 0.35,
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Stunting")
#                                   )
#                 )
# inspect(rules[1:10])

# Rural has too few samples to narrow selection of rules down to ~10 (smallest possible is 23,260); every rule has count = 1
ruleSet[nrow(ruleSet)+1, 1] = "SUBURBAN:"
suburban = subset(data, Suburban == TRUE)
suburban = suburban[,-c(14,15)]
rules = apriori(suburban, 
                parameter = list(support = 0.10,
                                 confidence = 0.5,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "URBAN:"
urban = subset(data, (Rural == FALSE && Suburban == FALSE))
urban = urban[,-c(14,15)]
rules = apriori(urban, 
                parameter = list(support = 0.05,
                                 confidence = 0.45,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:8])
ruleSet = rbind(ruleSet, as(rules[1:8], "data.frame"))
################################################################################
#Male vs. Female associations on StuntingYoung
data = data.frame(lapply(StuntingYoung, as.logical))
ruleSet[nrow(ruleSet)+1, 1] = "STUNTINGYOUNG:"

ruleSet[nrow(ruleSet)+1, 1] = "FEMALE:"
female = subset(data, Sex == TRUE)
female = female[,-2]
rules = apriori(female, 
                parameter = list(support = 0.05,
                                 confidence = 0.31,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "MALE:"
male = subset(data, Sex == FALSE)
male = male[,-2]
rules = apriori(male, 
                parameter = list(support = 0.05,
                                 confidence = 0.42,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))
################################################################################
#Rural vs. suburban vs. urban rules on StuntingYoung
ruleSet[nrow(ruleSet)+1, 1] = "RURAL:"
# rural = subset(data, Rural == TRUE)
# rural = rural[,-c(14,15)]
# rules = apriori(rural, 
#                 parameter = list(support = 0.14, #Anything above 0.14 returns nothing
#                                  confidence = 1, #This data is very strange
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Stunting")
#                                   )
#                 )
# inspect(rules[1:10])

#Much like Stunting's Rural data, I think there's too few samples for accurate analysis. 250k rules is absurd, especially considering the other Rural data had nothing.

ruleSet[nrow(ruleSet)+1, 1] = "SUBURBAN:"
suburban = subset(data, Suburban == TRUE)
suburban = suburban[,-c(14,15)]
rules = apriori(suburban, 
                parameter = list(support = 0.10,
                                 confidence = 0.42,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "URBAN:"
urban = subset(data, Rural == FALSE)
urban = subset(data, Suburban == FALSE) #I don't know why this has to be two separate lines of subsetting. Coding is magic, but I am not a wizard.
urban = urban[,-c(14,15)]
rules = apriori(urban, 
                parameter = list(support = 0.05,
                                 confidence = 0.304,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:8])
ruleSet = rbind(ruleSet, as(rules[1:8], "data.frame"))
################################################################################
#Young (<11) vs adolescent (11+) rules on Stunting & StuntingYouth
ruleSet[nrow(ruleSet)+1, 1] = "STUNTING v. STUNTINGYOUNG:"

ruleSet[nrow(ruleSet)+1, 1] = "STUNTING OVERALL:"
rules = apriori(data.frame(lapply(stunting, as.logical)), 
                parameter = list(support = 0.07,
                                 confidence = 0.355,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "STUNTINGYOUNG OVERALL:"
rules = apriori(data.frame(lapply(StuntingYoung, as.logical)), 
                parameter = list(support = 0.09,
                                 confidence = 0.258,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Stunting")
                                  )
                )
#inspect(rules[1:8])
ruleSet = rbind(ruleSet, as(rules[1:8], "data.frame"))
```



```{r Association Rule Learning: Thinness & ThinnessYoung}
################################################################################
#Thinness female vs. male
ruleSet[nrow(ruleSet)+1, 1] = "THINNESS:"
data = data.frame(lapply(thinness, as.logical))

ruleSet[nrow(ruleSet)+1, 1] = "FEMALE:"
# female = subset(data, Sex == TRUE)
# female = female[,-2]
# rules = apriori(female, 
#                 parameter = list(support = 0.01,
#                                  confidence = 0.01,
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Thinness")
#                                   )
#                 )
#append(ruleSet, as(rules, "data.frame"))
#inspect(rules[1:10])
# ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "MALE:"
male = subset(data, Sex == FALSE)
male = male[,-2]
rules = apriori(male, 
                parameter = list(support = 0.03,
                                 confidence = 0.20,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Thinness")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))
################################################################################
#Rural vs. suburban vs. urban rules on Thinness
ruleSet[nrow(ruleSet)+1, 1] = "RURAL:"
# rural = subset(data, Rural == TRUE)
# rural = rural[,-c(14,15)]
# rules = apriori(rural,
#                 parameter = list(support = 0.07,
#                                  confidence = 0.90,
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Thinness")
#                                   )
#                 )
# inspect(rules[1:10])
# Rural has too few samples to narrow selection of rules down to ~10 (smallest possible is 23,260); every rule has count = 1

ruleSet[nrow(ruleSet)+1, 1] = "SUBURBAN:"
suburban = subset(data, Suburban == TRUE)
suburban = suburban[,-c(14,15)]
rules = apriori(suburban, 
                parameter = list(support = 0.02,
                                 confidence = 0.50,
                                 maxlen = 7,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Thinness")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "URBAN:"
# urban = subset(data, Rural == FALSE) 
# urban = subset(urban, Suburban == FALSE)
# urban = urban[,-c(14,15)]
# rules = apriori(urban, 
#                 parameter = list(support = 0.02,
#                                  confidence = 0.10,
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Thinness")
#                                   )
#                 )
# #inspect(rules[1:8])
# ruleSet = rbind(ruleSet, as(rules[1:8], "data.frame"))
################################################################################
#Male vs. Female associations on ThinnessYoung
data = data.frame(lapply(thinnessYoung, as.logical))
ruleSet[nrow(ruleSet)+1, 1] = "THINNESSYOUNG:"

ruleSet[nrow(ruleSet)+1, 1] = "FEMALE:"
female = subset(data, Sex == TRUE)
female = female[,-2]
rules = apriori(female, 
                parameter = list(support = 0.02,
                                 confidence = 0.109,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Thinness")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "MALE:"
# male = subset(data, Sex == FALSE)
# male = male[,-2]
# rules = apriori(male, 
#                 parameter = list(support = 0.02,
#                                  confidence = 0.05,
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Thinness")
#                                   )
#                 )
# inspect(rules[1:10])
# ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))
################################################################################
#Rural vs. suburban vs. urban rules on ThinnessYoung
ruleSet[nrow(ruleSet)+1, 1] = "RURAL:"
# rural = subset(data, Rural == TRUE)
# rural = rural[,-c(14,15)]
# rules = apriori(rural,
#                 parameter = list(support = 0.14, #Anything above 0.14 returns nothing
#                                  confidence = 1, #This data is very strange
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Thinness")
#                                   )
#                 )
# inspect(rules[1:10])
ruleSet[nrow(ruleSet)+1, 1] = "NO RURAL ARE THIN"
#All rural samples are not thin.

ruleSet[nrow(ruleSet)+1, 1] = "SUBURBAN:"
# suburban = subset(data, Suburban == TRUE)
# suburban = suburban[,-c(14,15)]
# rules = apriori(suburban, 
#                 parameter = list(support = 0.02,
#                                  confidence = 0.1,
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Thinness")
#                                   )
#                 )
# inspect(rules[1:10])
# ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "URBAN:"
# urban = subset(data, Rural == FALSE)
# urban = subset(data, Suburban == FALSE) #I don't know why this has to be two separate lines of subsetting. Coding is magic, but I am not a wizard.
# urban = urban[,-c(14,15)]
# rules = apriori(urban, 
#                 parameter = list(support = 0.02,
#                                  confidence = 0.10,
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Thinness")
#                                   )
#                 )
# inspect(rules[1:10])
# ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))
################################################################################
#Young (<11) vs adolescent (11+) rules on Thinness & ThinnessYouth
ruleSet[nrow(ruleSet)+1, 1] = "THINNESS v. THINNESSYOUNG:"

ruleSet[nrow(ruleSet)+1, 1] = "THINNESS OVERALL:"
rules = apriori(data.frame(lapply(thinness, as.logical)), 
                parameter = list(support = 0.01,
                                 confidence = 0.35,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Thinness")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "THINNESSYOUNG OVERALL:"
rules = apriori(data.frame(lapply(thinnessYoung, as.logical)), 
                parameter = list(support = 0.02,
                                 confidence = 0.15,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Thinness")
                                  )
                )
#inspect(rules[1:8])
ruleSet = rbind(ruleSet, as(rules[1:8], "data.frame"))
```



```{r Association Rule Learning: WastingYoung}
#Pretty quick.
#Again, same as above two blocks except only conducted on WastingYoung data.
################################################################################
#Male vs. Female associations on WastingYoung
data = data.frame(lapply(WastingYoung, as.logical))
ruleSet[nrow(ruleSet)+1, 1] = "WASTINGYOUNG:"

ruleSet[nrow(ruleSet)+1, 1] = "FEMALE:"
female = subset(data, Sex == TRUE)
female = female[,-2]
rules = apriori(female, 
                parameter = list(support = 0.02,
                                 confidence = 0.33,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Wasting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "MALE:"
male = subset(data, Sex == FALSE)
male = male[,-2]
rules = apriori(male,
                parameter = list(support = 0.02,
                                 confidence = 0.36,
                                 maxlen = 7,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Wasting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))
################################################################################
#Rural vs. suburban vs. urban rules on WastingYoung
ruleSet[nrow(ruleSet)+1, 1] = "RURAL:"
# rural = subset(data, Rural == TRUE)
# rural = rural[,-c(14,15)]
# rules = apriori(rural,
#                 parameter = list(support = 0.14, #Anything above 0.14 returns nothing
#                                  confidence = 1, #This data is very strange
#                                  maxlen = 10,
#                                  target = "rules"),
#                 appearance = list(default = "lhs", rhs = c("Wasting")
#                                   )
#                 )
# inspect(rules[1:10])
ruleSet[nrow(ruleSet)+1, 1] = "NO RURAL ARE WASTED"
#All rural samples are not thin.

ruleSet[nrow(ruleSet)+1, 1] = "SUBURBAN:"
suburban = subset(data, Suburban == TRUE)
suburban = suburban[,-c(14,15)]
rules = apriori(suburban,
                parameter = list(support = 0.03,
                                 confidence = 0.50,
                                 maxlen = 7,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Wasting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))

ruleSet[nrow(ruleSet)+1, 1] = "URBAN:"
urban = subset(data, Rural == FALSE)
urban = subset(data, Suburban == FALSE) #I don't know why this has to be two separate lines of subsetting. Coding is magic, but I am not a wizard.
urban = urban[,-c(14,15)]
rules = apriori(urban,
                parameter = list(support = 0.02,
                                 confidence = 0.26,
                                 maxlen = 10,
                                 target = "rules"),
                appearance = list(default = "lhs", rhs = c("Wasting")
                                  )
                )
#inspect(rules[1:10])
ruleSet = rbind(ruleSet, as(rules[1:10], "data.frame"))
################################################################################
write.csv(ruleSet, "Association_Rules.csv", row.names = FALSE)
```