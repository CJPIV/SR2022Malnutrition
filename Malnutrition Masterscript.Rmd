---
title: "Malnutrition Masterscript"
author: "Jim Perry"
date: "5/30/2022"
output: pdf_document
---
# PREPROCESSING: Preparing data from original survey for use in statistical/ML analysis.
#####
```{r Import Data}
#Reading and cleaning the original spreadsheet from the survey.
library(readxl)
library(dplyr)

SURVEY = read_excel("SURVEY.xlsx")
SURVEY = SURVEY[-1,] #First row is a misread full of NAs - drop it
SURVEY = SURVEY[,-c(35, 54, 115, 134)] #Empty columns

#Now the fun part - manually rename any weird variable names
names(SURVEY)[13] = "Class" #Which class in the grade is the child in?
names(SURVEY)[17] = "FamilySize"
names(SURVEY)[18] = "NumOldBros"
names(SURVEY)[19] = "Bros<12?"
names(SURVEY)[20] = "Vaccine"
names(SURVEY)[21] = "BCGScar"
names(SURVEY)[22] = "FeverLast2Weeks"
names(SURVEY)[23] = "FeverQuantity"
names(SURVEY)[24] = "DiarrheaLast2Weeks"
names(SURVEY)[25] = "DiarrheaQuantity"
names(SURVEY)[26] = "CoughLast2Weeks"
names(SURVEY)[27] = "CoughQuantity"
names(SURVEY)[28] = "NailsTrimmed"
names(SURVEY)[29] = "NailsDirty"
names(SURVEY)[30] = "NailTrimFrequency"
names(SURVEY)[31] = "SchoolLat"
names(SURVEY)[32] = "SchoolLatDoors"
names(SURVEY)[33] = "SchoolLatFlies"
names(SURVEY)[34] = "SchoolLatVisibleStool"
names(SURVEY)[35] = "HeardOfAL"
names(SURVEY)[36] = "HeardOfTT"
names(SURVEY)[37] = "HeardOfHW"
names(SURVEY)[38] = "HeardOfHIV"
names(SURVEY)[39] = "HeardOfWorms"
names(SURVEY)[40] = "HeardOfMalaria"
names(SURVEY)[41] = "HeardOfTB"
names(SURVEY)[42] = "HeardOfSCh"
names(SURVEY)[43] = "ToldByFam"
names(SURVEY)[44] = "ToldByHP"
names(SURVEY)[45] = "ToldByTeacher"
names(SURVEY)[46] = "ToldByMedia"
names(SURVEY)[47] = "KnowWormsSpread"
names(SURVEY)[48] = "HowKnowWormsSpread"
names(SURVEY)[49] = "KnowWormsBad"
names(SURVEY)[50] = "HowKnowWormsBad"
names(SURVEY)[51] = "KnowAvoidWorms"
names(SURVEY)[52] = "HowKnowAvoidWorms"
names(SURVEY)[53] = "WhereLive"
names(SURVEY)[54] = "Address"
names(SURVEY)[55] = "Occupation"
names(SURVEY)[56] = "MomEduc"
names(SURVEY)[57] = "HouseFloorMats"
names(SURVEY)[58] = "KitchenSeparate"
names(SURVEY)[59] = "SepKitchenMats"
names(SURVEY)[60] = "SepKitchenRoof"
names(SURVEY)[61] = "SepKitchenWall"
names(SURVEY)[62] = "SepKitchenNeither" #Should be able to get removed in 1HE
names(SURVEY)[63] = "CookWood"
names(SURVEY)[64] = "CookGas"
names(SURVEY)[65] = "CookCoal"
names(SURVEY)[66] = "CookKerosine"
names(SURVEY)[67] = "CookElectric"
names(SURVEY)[68] = "Electricity"
names(SURVEY)[69] = "Radio"
names(SURVEY)[70] = "TV"
names(SURVEY)[71] = "Phone"
names(SURVEY)[72] = "WhyPhone"
names(SURVEY)[73] = "Cattle"
names(SURVEY)[74] = "Sheep/Goat"
names(SURVEY)[75] = "Chicken"
names(SURVEY)[76] = "HousePet"
names(SURVEY)[77] = "NoAnimal"
names(SURVEY)[78] = "HouseHasWater"
names(SURVEY)[79] = "WhereGetWater"
names(SURVEY)[80] = "WaterTreated"
names(SURVEY)[81] = "HowTreat"
names(SURVEY)[82] = "FamLat"
names(SURVEY)[83] = "LatInside"
names(SURVEY)[84] = "LatDistanceHouse"
names(SURVEY)[85] = "LatDistanceKitchen"
names(SURVEY)[86] = "LatConnectedTo"
names(SURVEY)[87] = "RiverBathFrequency"
names(SURVEY)[88] = "RiverLaundryFrequency"
names(SURVEY)[89] = "DefecateField"
names(SURVEY)[90] = "UseSchoolLat"
names(SURVEY)[91] = "UseTP"
names(SURVEY)[92] = "WashHandsLat"
names(SURVEY)[93] = "WashHandsLatHow"
names(SURVEY)[94] = "WashHandsSoapLatFrequency"
names(SURVEY)[95] = "WashHandsEat"
names(SURVEY)[96] = "WashHandsEatHow"
names(SURVEY)[97] = "WashHandsSoapEatFrequency"
names(SURVEY)[98] = "EatSoil"
names(SURVEY)[99] = "FavFruit"
names(SURVEY)[100] = "WashFruit"
names(SURVEY)[101] = "EatRawVeg"
names(SURVEY)[102] = "WashRawVegFrequency"
names(SURVEY)[103] = "WalkBarefoot"
names(SURVEY)[104] = "HomeShoeOrSandal"
names(SURVEY)[105] = "ForWhatBarefoot"
names(SURVEY)[106] = "DewormPill"
names(SURVEY)[107] = "WhenDewormPill"
names(SURVEY)[108] = "Antibiotics"
names(SURVEY)[109] = "MostFreqFood"
names(SURVEY)[110] = "TakesMeds"
names(SURVEY)[111] = "Name/TypeOfMeds"
names(SURVEY)[112] = "AntiMalaria3Months"
names(SURVEY)[113] = "Wheezing"
names(SURVEY)[114] = "Wheezing2Yrs"
names(SURVEY)[115] = "Wheezing1Yr"
names(SURVEY)[116] = "Wheezing1YrQuantity"
names(SURVEY)[117] = "Asthma"
names(SURVEY)[118] = "Asthma2Yrs"
names(SURVEY)[119] = "Asthma1Yr"
names(SURVEY)[120] = "DocConfirmedAsthma"
names(SURVEY)[121] = "Rash"
names(SURVEY)[122] = "RashElbow"
names(SURVEY)[123] = "RashKnees"
names(SURVEY)[124] = "RashAnkles"
names(SURVEY)[125] = "RashButt"
names(SURVEY)[126] = "RashNeck"
names(SURVEY)[127] = "RashEyesEars"
names(SURVEY)[128] = "HayFever"
names(SURVEY)[129] = "HayFever2Yrs"
names(SURVEY)[130] = "HayFever1Yr"

SURVEY = SURVEY[,-c(1, 7, 8, 9)] #Don't need these columns; I forgot to remove them earlier, and doing so now would mean I'd have to re-index the >100 variable renames I just did above. No way.
```



```{r Handling NAs & Datatype Conversions, warning = FALSE}
reducedSurvey = SURVEY[,-c(44, 46, 48, 51, 64, 95, 101, 105, 107)] #Potential candidates for removal from dataset; we'll work with this, as I think these changes will end up in the final iteration.
################################################################################
#Convert "Where do you get your water from?" into "Tap water" b/c it's majority case
tapWater = c()
for (i in 1:1036){
  if (is.na(reducedSurvey[[i, 70]])){
    tapWater[[i]] = 0
    next
  }
  if (tolower(reducedSurvey[[i, 70]]) == "tap water"){
    tapWater[[i]] = 1
  }else{
    tapWater[[i]] = 0
  }
}
tapWater = as.numeric(as.character(tapWater))
reducedSurvey[[70]] = tapWater
names(reducedSurvey)[[70]] = "TapWater"
################################################################################
#1HE Rural/Suburban/Urban split
rural = c()
suburban = c()
for (i in 1:1036){
  if (is.na(reducedSurvey[[i, 46]])){
      next
  }
  neighborhood = reducedSurvey[[i, 46]]
  if(neighborhood == 0){
      rural[[i]] = 0
      suburban[[i]] = 0
  }else if(neighborhood == 1){
    rural[[i]] = 0
    suburban[[i]] = 1
  }else if (neighborhood == 2){
      rural[[i]] = 1
      suburban[[i]] = 0
    }
}
suburban = as.numeric(as.character(suburban))
reducedSurvey[[46]] = suburban
names(reducedSurvey)[[46]] = "Suburban"
rural = as.numeric(as.character(rural)) #Can't convert directly from double to numeric
reducedSurvey[[47]] = rural# = replace(reducedSurvey, reducedSurvey$Address, rural)
names(reducedSurvey)[[47]] = "Rural"
################################################################################
#Remove units from measurements so they can be converted to numerics
for (i in 1:1036){
  distance = reducedSurvey[[i, 75]]
  reducedSurvey[[i, 75]] = substr(distance, 1, length(distance))
  distance2 = reducedSurvey[[i, 76]]
  reducedSurvey[[i, 76]] = substr(distance2, 1, length(distance2))
  whenDeworm = reducedSurvey[[i, 96]]
  reducedSurvey[[i, 96]] = substr(whenDeworm, 1, length(whenDeworm))

}
reducedSurvey[[75]] = as.numeric(reducedSurvey[[75]]) #Can typecast directly to numeric
reducedSurvey[[76]] = as.numeric(reducedSurvey[[76]])
reducedSurvey[[96]] = as.numeric(reducedSurvey[[96]])
################################################################################
#These factors have number measurements but are of datatype "character"; simple conversion to datatype "numeric"
changeInds = c(19, 21, 23, 31:42, 50:59, 64:68, 72, 76, 77, 82:117)
for (factor in changeInds){
  reducedSurvey[[factor]] = as.numeric(reducedSurvey[[factor]])
}
################################################################################
#Drop "Class"? Doesn't seem specific enough to be useful.
reducedSurvey = reducedSurvey[, -9]
################################################################################
#Adjusting data values
#Convert "99"s in numeric factors to "Na"s for later imputation under kNN algorithm
for (sample in 1:1036){
  for (factor in 11:116) #Columns where 99s indicate "I don't know" as a possible answer
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if (reducedSurvey[[sample, factor]] == 99){
      reducedSurvey[[sample, factor]] = NA
    }
}

#A sample's height was misinput as 92.00 meters.
for (sample in 1:1036){
  if (reducedSurvey[[sample, 10]] == 92.00){ 
    reducedSurvey[[sample, 10]] = NA
    break
  }
}

#Impute conditional responses where "NA" implies 0
for (sample in 1:1036){
  for (factor in c(18, 20, 22, 31:41, 53, 55:58, 64:67, 71, 95, 109:113)){
    if (is.na(reducedSurvey[[sample, factor]])){
      reducedSurvey[[sample, factor]] = 0
    }
  }
}

#These columns were input as a different binary; as "NA/0" instead of "0/1"
for (sample in 1:1036){
  for (factor in c(30, 54, 63)){ 
    if (is.na(reducedSurvey[[sample, factor]])){
      reducedSurvey[[sample, factor]] = 0
    }else if (reducedSurvey[[sample, factor]] == 0){
      reducedSurvey[[sample, factor]] = 1
    }else{
      reducedSurvey[[sample, factor]] = 0
    }
  }
}

#Convert different numeric binaries 0/2 binaries into 0/1
for (sample in 1:1036){
  for (factor in c(49, 65)){
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if (reducedSurvey[[sample, factor]] == 2){
      reducedSurvey[[sample, factor]] = 1
    }else if (reducedSurvey[[sample, factor]] != 0){
      reducedSurvey[[sample, factor]] = NA
    }
  }
}
#CookElectric's binary is 0/4; converting into 0/1
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 58]])){
      next
    }else if (reducedSurvey[[sample, 58]] == 4){
      reducedSurvey[[sample, 58]] = 1
    }else if (reducedSurvey[[sample, 58]] != 0){
      reducedSurvey[[sample, 58]] = NA
    }
  }
}
#0/3 here
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 66]])){
      reducedSurvey[[sample, 66]] = 0
    }else if (reducedSurvey[[sample, 66]] == 3){
      reducedSurvey[[sample, 66]] = 1
    }else if (reducedSurvey[[sample, 6]] != 0){
      reducedSurvey[[sample, 66]] = NA
    }
  }
}
#I don't know why this code in the above loop doesn't work. I don't know why it does work when placed into its own loop. Code is mysterious; let's hope this desire for loop autonomy doesn't spread
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 66]])){
      reducedSurvey[[sample, 66]] = 0
    }
  }
}

#copySet = reducedSurvey
#reducedSurvey = copySet

#Setting weird values in binary vectors (anything that's not 0/1) to NA for imputation
for (sample in 1:1036){
  for (factor in c(15, 16, 21, 23, 24, 49, 51, 58, 65, 68, 72:73)){
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if ((reducedSurvey[[sample, factor]] != 0) & (reducedSurvey[[sample, factor]] != 1)){
      reducedSurvey[[sample, factor]] = NA
    }
  }
}



reducedSurvey$HouseFloorMats = tolower(reducedSurvey$HouseFloorMats)

#Getting rid of some more weird values
for (sample in 1:1036){
  for (factor in c(85:86))
  if (is.na(reducedSurvey[[sample, factor]])){
    next
  }
  else if (reducedSurvey[[sample, factor]] == 21 || reducedSurvey[[sample, factor]] == 10){
    reducedSurvey[[sample, factor]] = NA
  }
}

for (sample in 1:1036){
  if (is.na(reducedSurvey[[sample, 10]])){
    next
  }
  else if (reducedSurvey[[sample, 10]] == 1.79){
    reducedSurvey[[sample, 10]] = 1.29
  }
}

```



```{r kNN, One Hot Encoding}
library(VIM)
library(fastDummies)
################################################################################
#DEAL WITH CATEGORICALS
#impute ordinal categoricals
imputed = kNN(reducedSurvey, variable = colnames(reducedSurvey), k = 5, impNA = TRUE)
imputed = subset(imputed, select = AL:HayFever1Yr)

#1HE nominal categoricals
#I already encoded some of these by hand in the last code block, but the remainder can be handled by this imported function
oneHotEncoded = dummy_cols(.data = imputed,
                     ignore_na = TRUE,
                     remove_first_dummy =  TRUE,
                     split = ",")
oneHotEncoded = oneHotEncoded[,-c(48, 123)] #One category got broken up and the other was created to account for a sample answer that got misinput as both
oneHotEncoded[[843, 121]] = 1

```



```{r Final Layer of Data Handling}
#Handle variables we decided to remove or calculate differently after I wrote 300 lines of code. I'm not mad - I'm not just rewriting everything above to account for new index values.
processed = oneHotEncoded[, -c(1:5, 11, 13:14, 18, 20, 22, 25:26, 30:44, 49, 52:56, 61, 66, 70, 73:75, 83, 86, 92, 94, 99:104, 107:112, 114:119, 124)]

#Encode mother's education
for (i in 1:1036){
  education = processed[[i, 19]]
  processed$momFinishedPrimary[[i]] = 0
  processed$momFinishedSecondary[[i]] = 0
  processed$momFinishedTertiary[[i]] = 0
  if (education == 1){
    processed$momFinishedPrimary[[i]] = 1
  }else if (education == 2){
    processed$momFinishedSecondary[[i]] = 1
  }else if (education == 3){
    processed$momFinishedTertiary[[i]] = 1
  }
}

processed$momFinishedPrimary = as.numeric(as.character(processed$momFinishedPrimary))
processed$momFinishedSecondary = as.numeric(as.character(processed$momFinishedSecondary))
processed$momFinishedTertiary = as.numeric(as.character(processed$momFinishedTertiary))
processed = processed[, -19]

#Convert house floor materials into a binary for whether that floor is dust or not
for (i in 1:1036){
  if (sum(processed[i,57:61]) > 0){
    processed$houseDustFloor[[i]] = 0
  }else{
    processed$houseDustFloor[[i]] = 1
  }
}
processed$houseDustFloor = as.numeric(as.character(processed$houseDustFloor))
processed = processed[,-c(57:60)]

#Converting 1/2 binaries to 0/1
for (sample in 1:1036){
  for (factor in 53:56){
    if (processed[sample, factor] == 2){
      processed[sample, factor] = 0
    }
  }
}

#Every sample has the same answer for this variable - it tells us nothing
processed = processed[,-c(21)]
processed = processed[,-3]

#1HE washing hands after lat into "never", "with water", and "with soap" groups
for (i in 1:1036){
  if (processed[[i,38]] == 2){
    processed$washHandsLatWater[[i]] = 0
    processed$washHandsLatWaterSoap[[i]] = 0
  }else{
    if (processed[[i, 39]] == 0){
      processed$washHandsLatWater[[i]] = 1
      processed$washHandsLatWaterSoap[[i]] = 0
    }else if (processed[[i, 39]] == 1){
      processed$washHandsLatWater[[i]] = 0
      processed$washHandsLatWaterSoap[[i]] = 1
    }
  }
}
processed$washHandsLatWater = as.numeric(as.character(processed$washHandsLatWater))
processed$washHandsLatWaterSoap = as.numeric(as.character(processed$washHandsLatWaterSoap))
processed = processed[, -c(38:39)]

#1HE washing hands before eat into "never", "with water", "with soap" groups
for (i in 1:1036){
  if (processed[[i,38]] == 2){
    processed$washHandsWaterB4Eat[[i]] = 0
    processed$washHandsSoapB4Eat[[i]] = 0
  }else{
    if (processed[[i, 39]] == 0){
      processed$washHandsWaterB4Eat[[i]] = 1
      processed$washHandsSoapB4Eat[[i]] = 0
    }else if (processed[[i, 39]] == 1){
      processed$washHandsWaterB4Eat[[i]] = 0
      processed$washHandsSoapB4Eat[[i]] = 1
    }
  }
}
processed$washHandsWaterB4Eat = as.numeric(as.character(processed$washHandsWaterB4Eat))
processed$washHandsSoapB4Eat = as.numeric(as.character(processed$washHandsSoapB4Eat))
processed = processed[, -c(38:39)]

#Change ternary variables into binaries based on biological significance
for (sample in 1:1036){
  for (factor in c(33:37)){
    if (processed[[sample, factor]] == 2){
      processed[[sample, factor]] = 0
    }else{
      processed[[sample, factor]] = 1
    }
  }
}
names(processed)[[33]] = "RiverBathing"
names(processed)[[34]] = "RiverLaundry"

for (sample in 1:1036){
 if (processed[[sample, 38]] == 1){
   processed[[sample, 38]] = 0
 }else{
   processed[[sample, 38]] = 1
 }
}

for (sample in 1:1036){
  if (processed[[sample, 39]] != 0){
    processed[[sample, 39]] = 0
  }else if (processed[[sample, 39]] == 0){
    processed[[sample, 39]] = 1
  }
}
names(processed)[[39]] = "NeverWashFruit"

#More ternary variables, but split differently according to biological significance
for (sample in 1:1036){
  for (factor in c(40:42)){
    if (processed[[sample, factor]] != 0){
      processed[[sample, factor]] = 1
    }else{
      processed[[sample, factor]] = 0
    }
  }
}

#This loop below does two things: it re-formats data so 0 is false and 1 is true, OR turns already-correct but useless data into correctly-encoded data (i.e. "NoPhone" and "NoWaterInHouse")
for (sample in 1:1036){
  for (factor in c(23,28,30,31,36,37)){
    if (processed[[sample, factor]] == 1){
      processed[[sample, factor]] = 0
    }else{
      processed[[sample, factor]] = 1
    }
  }
}
names(processed)[[23]] = "NoPhone"
names(processed)[[28]] = "NoWaterInHouse"
names(processed)[[31]] = "NoFamLat"
names(processed)[[36]] = "DontUseSchoolLat"
names(processed)[[37]] = "DontUseTP"
```

```{r Dependent Variable Calculations}
#Derive WAZ/HAZ/BAZ for specific analysis
#WAZ: Weight-relative-to-age Z-score
#HAZ: Height-relative-to-age Z-score (WHO says unreliable for ages 11+, so ignore older kids)
#BAZ: BMI-relative-to-age Z-score

library(anthroplus)

depVars = anthroplus_zscores(sex = processed[["Sex"]]+1,
                             age_in_months = processed[["Age"]]*12,
                             weight_in_kg = processed[["weight"]],
                             height_in_cm = processed[["Height"]]*100
                             )

for (i in 1:1036){
  ZHeight = depVars$zhfa[[i]]
  processed$HAZ[[i]] = ZHeight
  processed$Stunting[[i]] = ifelse(ZHeight < -2, 1, 0 )
  
  ZBMI = depVars$zbfa[[i]]
  processed$BAZ[[i]] = ZBMI
  processed$Thinness[[i]] = ifelse (ZBMI < -2, 1, 0)
  
  if (processed[[i,"Age"]] < 11){ #scores not calculated for children 11 and up
    ZWeight = depVars$zwfa[[i]]
    processed$WAZ[[i]] = ZWeight
    processed$Wasting[[i]] = ifelse (ZWeight < -2, 1, 0)
  }else{
    processed$WAZ[[i]] = NA
    processed$Wasting[[i]] = NA
  }
}

processed$HAZ = as.numeric(as.character(processed$HAZ))
processed$Stunting = as.numeric(as.character(processed$Stunting))
processed$BAZ = as.numeric(as.character(processed$BAZ))
processed$Thinness = as.numeric(as.character(processed$Thinness))

#badAges = c()
for (i in 1:1036){
  if (is.na(processed$Stunting[i])){
    processed = processed[-i,]
  }
}
#as.numeric(as.character(badAges))
#processed = processed[[-badAges,]]
```



```{r Saving Preprocessing}
stunting = processed[,-c(3, 4, 19, 41, 59, 61:64)]
thinness = processed[,-c(3, 4, 19, 41, 59:61, 63:65)]
wasting = processed[,-c(3, 4, 19, 41, 59:63)]

################################################################################
wastingYoung = subset(wasting, Age < 11)
wastingYoung$Wasting = as.numeric(as.character(wastingYoung$Wasting))
#wastingYoung = wastingYoung[,-27] #All values for row 27 were the same

stuntingYoung = subset(stunting, Age < 11)
#stuntingYoung = stuntingYoung[,-c(19)]

thinnessYoung = subset(thinness, Age < 11)
#thinnessYoung = thinnessYoung[,-c(19)]

stunting = subset(stunting, Age > 10)

thinness = subset(thinness, Age > 10)
#thinness = thinness[,-27]

################################################################################
write.csv(stunting, "stunting.csv", row.names = FALSE)
write.csv(thinness, "thinness.csv", row.names = FALSE)
write.csv(wastingYoung, "wastingUnder11.csv", row.names = FALSE)
write.csv(stuntingYoung, "stuntingUnder11.csv", row.names = FALSE)
write.csv(thinnessYoung, "thinnessUnder11.csv", row.names = FALSE)
```

# TRAIN/TEST SPLITS: Prepares train/test splits for use in feature selection and
# classifier model training. Each dependent variable (Stunting and Thinness)
# will be cloned; one clone will remain as-is (unbalanced), and the other will
# be balanced via SMOTE for (hopefully) improved classifier results.
#####

```{r Read Data}
stunting = read.csv("stunting.csv")
thinness = read.csv("thinness.csv")
StuntingYoung = read.csv("stuntingUnder11.csv")
thinnessYoung = read.csv("thinnessUnder11.csv")
WastingYoung = read.csv("wastingUnder11.csv")
```

```{r Stunting Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
testFolds = createFolds(stunting$Stunting, k = 10) #Create k splits
testSetsStunting = list(stunting[testFolds$Fold01, ], stunting[testFolds$Fold02, ],
                        stunting[testFolds$Fold03, ], stunting[testFolds$Fold04, ],
                        stunting[testFolds$Fold05, ], stunting[testFolds$Fold06, ],
                        stunting[testFolds$Fold07, ], stunting[testFolds$Fold08, ],
                        stunting[testFolds$Fold09, ], stunting[testFolds$Fold10, ]
                        )
sansTestStunting = list(stunting[-testFolds$Fold01, ], stunting[-testFolds$Fold02, ],
                        stunting[-testFolds$Fold03, ], stunting[-testFolds$Fold04, ],
                        stunting[-testFolds$Fold05, ], stunting[-testFolds$Fold06, ],
                        stunting[-testFolds$Fold07, ], stunting[-testFolds$Fold08, ],
                        stunting[-testFolds$Fold09, ], stunting[-testFolds$Fold10, ]
                        )
UBStunting = list()
for (i in 1:10){
  curSet = sansTestStunting[[i]]
  validationFolds = createFolds(sansTestStunting[[i]]$Stunting, k = 10)
  UBStunting[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStunting = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStunting[[i]] = list()
  for (j in 1:10){
    SMOTEStunting[[i]][[j]] = SMOTE(X = as.data.frame(UBStunting[[i]][[j]]),
                              target = UBStunting[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 2
                              )
     SMOTEStunting[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBStunting[[1]][[1]]$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEStunting[[1]][[1]]$data$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (SMOTE)')
# abline(h = 0)


# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestStunting = rbind(testSetsStunting[1:10])
# ForFS_UBStunting = rbind(sansTestStunting[1:10])
# ForFS_SMOTEStunting = rbind(SMOTEStunting[1:10])

# write.csv(ForFS_TestStunting, "ForFS_TestStunting.csv")
# write.csv(ForFS_UBStunting, "ForFS_UBStunting.csv")
# write.csv(ForFS_SMOTEStunting, "ForFS_SMOTEStunting.csv")
```



```{r Thinness Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
testFolds = createFolds(thinness$Thinness, k = 10) #Create k splits
testSetsThinness = list(thinness[testFolds$Fold01, ], thinness[testFolds$Fold02, ],
                        thinness[testFolds$Fold03, ], thinness[testFolds$Fold04, ],
                        thinness[testFolds$Fold05, ], thinness[testFolds$Fold06, ],
                        thinness[testFolds$Fold07, ], thinness[testFolds$Fold08, ],
                        thinness[testFolds$Fold09, ], thinness[testFolds$Fold10, ]
                        )
sansTestThinness = list(thinness[-testFolds$Fold01, ], thinness[-testFolds$Fold02, ],
                        thinness[-testFolds$Fold03, ], thinness[-testFolds$Fold04, ],
                        thinness[-testFolds$Fold05, ], thinness[-testFolds$Fold06, ],
                        thinness[-testFolds$Fold07, ], thinness[-testFolds$Fold08, ],
                        thinness[-testFolds$Fold09, ], thinness[-testFolds$Fold10, ]
                        )
UBThinness = list()
for (i in 1:10){
  curSet = sansTestThinness[[i]]
  validationFolds = createFolds(sansTestThinness[[i]]$Thinness, k = 10)
  UBThinness[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinness = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinness[[i]] = list()
  for (j in 1:10){
    SMOTEThinness[[i]][[j]] = SMOTE(X = as.data.frame(UBThinness[[i]][[j]]),
                              target = UBThinness[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 20
                              )
    SMOTEThinness[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBThinness[[1]][[1]]$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEThinness[[1]][[1]]$data$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (SMOTE)')
# abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestThinness = rbind(testSetsThinness[1:10])
# ForFS_UBThinness = rbind(trainSetsUBThinness[1:10])
# ForFS_SMOTEThinness = rbind(trainSetsSMOTEThinness[1:10])

# write.csv(ForFS_TestThinness, "ForFS_TestThinness.csv")
# write.csv(ForFS_UBThinness, "ForFS_UBThinness.csv")
# write.csv(ForFS_SMOTEThinness, "ForFS_SMOTEThinness.csv")
```

```{r StuntingYoung Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
testFolds = createFolds(StuntingYoung$Stunting, k = 10) #Create k splits
testSetsStuntingYoung = list(StuntingYoung[testFolds$Fold01, ],
                             StuntingYoung[testFolds$Fold02, ],
                        StuntingYoung[testFolds$Fold03, ], StuntingYoung[testFolds$Fold04, ],
                        StuntingYoung[testFolds$Fold05, ], StuntingYoung[testFolds$Fold06, ],
                        StuntingYoung[testFolds$Fold07, ], StuntingYoung[testFolds$Fold08, ],
                        StuntingYoung[testFolds$Fold09, ], StuntingYoung[testFolds$Fold10, ]
                        )
sansTestStuntingYoung = list(StuntingYoung[-testFolds$Fold01, ], StuntingYoung[-testFolds$Fold02, ], StuntingYoung[-testFolds$Fold03, ], StuntingYoung[-testFolds$Fold04, ],StuntingYoung[-testFolds$Fold05, ], StuntingYoung[-testFolds$Fold06, ],StuntingYoung[-testFolds$Fold07, ], StuntingYoung[-testFolds$Fold08, ],StuntingYoung[-testFolds$Fold09, ], StuntingYoung[-testFolds$Fold10, ]
                        )
UBStuntingYoung = list()
for (i in 1:10){
  curSet = sansTestStuntingYoung[[i]]
  validationFolds = createFolds(sansTestStuntingYoung[[i]]$Stunting, k = 10)
  UBStuntingYoung[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStuntingYoung = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStuntingYoung[[i]] = list()
  for (j in 1:10){
    SMOTEStuntingYoung[[i]][[j]] = SMOTE(X = as.data.frame(UBStuntingYoung[[i]][[j]]),
                              target = UBStuntingYoung[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 3
                              )
    SMOTEStuntingYoung[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBStuntingYoung[[1]][[1]]$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'StuntingYoung Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEStuntingYoung[[1]][[1]]$data$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'StuntingYoung Training Set Class Distribution (SMOTE)')
# abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestStuntingYoung = rbind(testSetsStuntingYoung[1:10])
# ForFS_UBStuntingYoung = rbind(trainSetsUBStuntingYoung[1:10])
# ForFS_SMOTEStuntingYoung = rbind(trainSetsSMOTEStuntingYoung[1:10])

# write.csv(ForFS_TestStuntingYoung, "ForFS_TestStuntingYoung.csv")
# write.csv(ForFS_UBStuntingYoung, "ForFS_UBStuntingYoung.csv")
# write.csv(ForFS_SMOTEStuntingYoung, "ForFS_SMOTEStuntingYoung.csv")
```

```{r ThinnessYoung Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
testFolds = createFolds(thinnessYoung$Thinness, k = 10) #Create k splits
testSetsThinnessYoung = list(thinnessYoung[testFolds$Fold01, ],
                             thinnessYoung[testFolds$Fold02, ],
                        thinnessYoung[testFolds$Fold03, ], thinnessYoung[testFolds$Fold04, ],
                        thinnessYoung[testFolds$Fold05, ], thinnessYoung[testFolds$Fold06, ],
                        thinnessYoung[testFolds$Fold07, ], thinnessYoung[testFolds$Fold08, ],
                        thinnessYoung[testFolds$Fold09, ], thinnessYoung[testFolds$Fold10, ]
                        )
sansTestThinnessYoung = list(thinnessYoung[-testFolds$Fold01, ], thinnessYoung[-testFolds$Fold02, ], thinnessYoung[-testFolds$Fold03, ], thinnessYoung[-testFolds$Fold04, ],thinnessYoung[-testFolds$Fold05, ], thinnessYoung[-testFolds$Fold06, ],thinnessYoung[-testFolds$Fold07, ], thinnessYoung[-testFolds$Fold08, ],thinnessYoung[-testFolds$Fold09, ], thinnessYoung[-testFolds$Fold10, ]
                        )
UBThinnessYoung = list()
for (i in 1:10){
  curSet = sansTestThinnessYoung[[i]]
  validationFolds = createFolds(sansTestThinnessYoung[[i]]$Thinness, k = 10)
  UBThinnessYoung[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinnessYoung = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinnessYoung[[i]] = list()
  for (j in 1:10){
    SMOTEThinnessYoung[[i]][[j]] = SMOTE(X = as.data.frame(UBThinnessYoung[[i]][[j]]),
                              target = UBThinnessYoung[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 34
                              )
    SMOTEThinnessYoung[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBThinnessYoung[[1]][[1]]$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'ThinnessYoung Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEThinnessYoung[[1]][[1]]$data$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'ThinnessYoung Training Set Class Distribution (SMOTE)')
# abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestThinnessYoung = rbind(testSetsThinnessYoung[1:10])
# ForFS_UBThinnessYoung = rbind(trainSetsUBThinnessYoung[1:10])
# ForFS_SMOTEThinnessYoung = rbind(trainSetsSMOTEThinnessYoung[1:10])

# write.csv(ForFS_TestThinnessYoung, "ForFS_TestThinnessYoung.csv")
# write.csv(ForFS_UBThinnessYoung, "ForFS_UBThinnessYoung.csv")
# write.csv(ForFS_SMOTEThinnessYoung, "ForFS_SMOTEThinnessYoung.csv")
```

```{r WastingYoung Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

set.seed(13)
testFolds = createFolds(WastingYoung$Wasting, k = 10) #Create k splits
testSetsWastingYoung = list(WastingYoung[testFolds$Fold01, ],
                             WastingYoung[testFolds$Fold02, ],
                        WastingYoung[testFolds$Fold03, ], WastingYoung[testFolds$Fold04, ],
                        WastingYoung[testFolds$Fold05, ], WastingYoung[testFolds$Fold06, ],
                        WastingYoung[testFolds$Fold07, ], WastingYoung[testFolds$Fold08, ],
                        WastingYoung[testFolds$Fold09, ], WastingYoung[testFolds$Fold10, ]
                        )
sansTestWastingYoung = list(WastingYoung[-testFolds$Fold01, ], WastingYoung[-testFolds$Fold02, ], WastingYoung[-testFolds$Fold03, ], WastingYoung[-testFolds$Fold04, ],WastingYoung[-testFolds$Fold05, ], WastingYoung[-testFolds$Fold06, ],WastingYoung[-testFolds$Fold07, ], WastingYoung[-testFolds$Fold08, ],WastingYoung[-testFolds$Fold09, ], WastingYoung[-testFolds$Fold10, ]
                        )
UBWastingYoung = list()
for (i in 1:10){
  curSet = sansTestWastingYoung[[i]]
  validationFolds = createFolds(sansTestWastingYoung[[i]]$Wasting, k = 10)
  UBWastingYoung[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEWastingYoung = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEWastingYoung[[i]] = list()
  for (j in 1:10){
    SMOTEWastingYoung[[i]][[j]] = SMOTE(X = as.data.frame(UBWastingYoung[[i]][[j]]),
                              target = UBWastingYoung[[i]][[j]]$Wasting,
                              K = 5,
                              dup_size = 10
                              )
    SMOTEWastingYoung[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBWastingYoung[[1]][[1]]$Wasting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Wasting",
#         names = c("No", "Yes"),
#         main = 'WastingYoung Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEWastingYoung[[1]][[1]]$data$Wasting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Wasting",
#         names = c("No", "Yes"),
#         main = 'WastingYoung Training Set Class Distribution (SMOTE)')
# abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestWastingYoung = rbind(testSetsWastingYoung[1:10])
# ForFS_UBWastingYoung = rbind(trainSetsUBWastingYoung[1:10])
# ForFS_SMOTEWastingYoung = rbind(trainSetsSMOTEWastingYoung[1:10])

# write.csv(ForFS_TestWastingYoung, "ForFS_TestWastingYoung.csv")
# write.csv(ForFS_UBWastingYoung, "ForFS_UBWastingYoung.csv")
# write.csv(ForFS_SMOTEWastingYoung, "ForFS_SMOTEWastingYoung.csv")
```

#####
# FEATURE SELECTION: Selects important model features to reduce noise and help 
# prevent overfitting. All feature selection methods should be run on SMOTE-
# balanced datasets.
#####
```{r ReliefF}
#This one takes ~90 minutes to run. Have fun waiting.
library(FSelectorRcpp)
################################################################################
FS_RLF_Stunting = list()
for (i in 1:10){
  FS_RLF_Stunting[[i]] = list()
  for (j in 1:10){
    relF = relief(Stunting~.,
                as.data.frame(SMOTEStunting[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Stunting[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_Thinness = list()
for (i in 1:10){
  FS_RLF_Thinness[[i]] = list()
  for (j in 1:10){
    relF = relief(Thinness~.,
                as.data.frame(SMOTEThinness[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Thinness[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_StuntingYoung = list()
for (i in 1:10){
  FS_RLF_StuntingYoung[[i]] = list()
  for (j in 1:10){
    relF = relief(Stunting~.,
                as.data.frame(SMOTEStuntingYoung[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_StuntingYoung[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_ThinnessYoung = list()
for (i in 1:10){
  FS_RLF_ThinnessYoung[[i]] = list()
  for (j in 1:10){
    relF = relief(Thinness~.,
                as.data.frame(SMOTEThinnessYoung[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_ThinnessYoung[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_WastingYoung = list()
for (i in 1:10){
  FS_RLF_WastingYoung[[i]] = list()
  for (j in 1:10){
    relF = relief(Wasting~.,
                as.data.frame(SMOTEWastingYoung[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_WastingYoung[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
```



```{r ReliefF Averaging}
#Averaging
################################################################################
AVG_FS_RLF = data.frame(Stunting = double(),
                        Thinness = double(),
                        StuntingYoung = double(),
                        ThinnessYoung = double(),
                        WastingYoung = double())
COPY = FS_RLF_Stunting
for (factor in names(stunting)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 1]] = sum/100
}
################################################################################
COPY = FS_RLF_Thinness
for (factor in names(thinness)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 2]] = sum/100
}
################################################################################
COPY = FS_RLF_StuntingYoung
for (factor in names(StuntingYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 3]] = sum/100
}
################################################################################
COPY = FS_RLF_ThinnessYoung
for (factor in names(thinnessYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 4]] = sum/100
}
################################################################################
COPY = FS_RLF_WastingYoung
for (factor in names(WastingYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_RLF[[factor, 5]] = sum/100
}
write.csv(AVG_FS_RLF, "FS_RLF_Avg.csv")
################################################################################
```



```{r InfoGain}
#This one is pretty quick.
library(FSelectorRcpp)
################################################################################
FS_IG_Stunting = list()
for (i in 1:10){
  FS_IG_Stunting[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Stunting~.,
                              data = as.data.frame(SMOTEStunting[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_Stunting[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
################################################################################
FS_IG_Thinness = list()
for (i in 1:10){
  FS_IG_Thinness[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Thinness~.,
                              data = as.data.frame(SMOTEThinness[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_Thinness[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
################################################################################
FS_IG_StuntingYoung = list()
for (i in 1:10){
  FS_IG_StuntingYoung[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Stunting~.,
                              data = as.data.frame(SMOTEStuntingYoung[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_StuntingYoung[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
################################################################################
FS_IG_ThinnessYoung = list()
for (i in 1:10){
  FS_IG_ThinnessYoung[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Thinness~.,
                              data = as.data.frame(SMOTEThinnessYoung[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_ThinnessYoung[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
################################################################################
FS_IG_WastingYoung = list()
for (i in 1:10){
  FS_IG_WastingYoung[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Wasting~.,
                              data = as.data.frame(SMOTEWastingYoung[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_WastingYoung[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
################################################################################
```



```{r InfoGain Averaging}
################################################################################
AVG_FS_IG = data.frame(Stunting = double(),
                       Thinness = double(),
                       StuntingYoung = double(),
                       ThinnessYoung = double(),
                       WastingYoung = double()
                       )
COPY = FS_IG_Stunting
for (factor in names(stunting)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_IG[[factor, 1]] = sum/100
}
################################################################################
COPY = FS_IG_Thinness
for (factor in names(thinness)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_IG[[factor, 2]] = sum/100
}
################################################################################
COPY = FS_IG_StuntingYoung
for (factor in names(StuntingYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_IG[[factor, 3]] = sum/100
}
################################################################################
COPY = FS_IG_ThinnessYoung
for (factor in names(thinnessYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_IG[[factor, 4]] = sum/100
}
################################################################################
COPY = FS_IG_WastingYoung
for (factor in names(WastingYoung)[1:54]){
  sum = 0
  for (i in 1:10){
    for (j in 1:10){
      index = which(COPY[[i]][[j]]$attributes == factor)
      if (is.na(COPY[[i]][[j]]$importance[[index]])){
        next
      }else{
        sum = sum + COPY[[i]][[j]]$importance[[index]]
      }
    }
  }
  AVG_FS_IG[[factor, 5]] = sum/100
}
write.csv(AVG_FS_IG, "FS_IG_Avg.csv")
################################################################################
```



```{r mRMR}
library(mRMRe)
#In progress; mRMR finishes selecting, but results should be rewritten for legibility.
#This one is pretty quick.
################################################################################
FS_MRMR_Stunting = list()
for (i in 1:10){
  FS_MRMR_Stunting[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEStunting[[i]][[j]]$data)
    FS_MRMR_Stunting[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 55,
                                              feature_count = 15
                                              )
  }
}
################################################################################
FS_MRMR_Thinness = list()
for (i in 1:10){
  FS_MRMR_Thinness[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEThinness[[i]][[j]]$data)
    FS_MRMR_Thinness[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 55,
                                              feature_count = 15
                                              )
  }
}
################################################################################
FS_MRMR_StuntingYoung = list()
for (i in 1:10){
  FS_MRMR_StuntingYoung[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEStuntingYoung[[i]][[j]]$data)
    FS_MRMR_StuntingYoung[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 55,
                                              feature_count = 15
                                              )
  }
}
################################################################################
FS_MRMR_ThinnessYoung = list()
for (i in 1:10){
  FS_MRMR_ThinnessYoung[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEThinnessYoung[[i]][[j]]$data)
    FS_MRMR_ThinnessYoung[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 55,
                                              feature_count = 15
                                              )
  }
}
################################################################################
FS_MRMR_WastingYoung = list()
for (i in 1:10){
  FS_MRMR_WastingYoung[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEWastingYoung[[i]][[j]]$data)
    FS_MRMR_WastingYoung[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 55,
                                              feature_count = 15
                                              )
  }
}
################################################################################

```



```{r JMI}
#This one is pretty quick.
library(praznik)
################################################################################
FS_JMI_Stunting = list()
for (i in 1:10){
  FS_JMI_Stunting[[i]] = list()
  for (j in 1:10){
    FS_JMI_Stunting[[i]][[j]] = JMI(SMOTEStunting[[i]][[j]]$data, 
                                    SMOTEStunting[[i]][[j]]$data$Stunting,
                                    k = 13
                                    )
  }
}
################################################################################
FS_JMI_Thinness = list()
for (i in 1:10){
  FS_JMI_Thinness[[i]] = list()
  for (j in 1:10){
    FS_JMI_Thinness[[i]][[j]] = JMI(SMOTEThinness[[i]][[j]]$data,
                                    SMOTEThinness[[i]][[j]]$data$Thinness,
                                    k = 13
                                    )
  }
}
################################################################################
FS_JMI_StuntingYoung = list()
for (i in 1:10){
  FS_JMI_StuntingYoung[[i]] = list()
  for (j in 1:10){
    FS_JMI_StuntingYoung[[i]][[j]] = JMI(SMOTEStuntingYoung[[i]][[j]]$data, 
                                    SMOTEStuntingYoung[[i]][[j]]$data$Stunting,
                                    k = 13
                                    )
  }
}
################################################################################
FS_JMI_ThinnessYoung = list()
for (i in 1:10){
  FS_JMI_ThinnessYoung[[i]] = list()
  for (j in 1:10){
    FS_JMI_ThinnessYoung[[i]][[j]] = JMI(SMOTEThinnessYoung[[i]][[j]]$data, 
                                    SMOTEThinnessYoung[[i]][[j]]$data$Thinness,
                                    k = 13
                                    )
  }
}
################################################################################
FS_JMI_WastingYoung = list()
for (i in 1:10){
  FS_JMI_WastingYoung[[i]] = list()
  for (j in 1:10){
    FS_JMI_WastingYoung[[i]][[j]] = JMI(SMOTEWastingYoung[[i]][[j]]$data[1:55], 
                                    SMOTEWastingYoung[[i]][[j]]$data$Wasting,
                                    k = 13
                                    )
  }
}
################################################################################
```



```{r Repeated Univariate Logistic Regression (FS)}
#This one takes ~5 minutes.
#Repeated Univariate Logistic Regression for FS in training models

library(broom)
library(car)
library(performance)
library(MASS)

results = data.frame(StuntingRejects = double(), StuntingOR = double(),
                     ThinnessRejects = double(), ThinnessOR = double(),
                     StuntingYoungRejects = double(), StuntingYoungOR = double(),
                     ThinnessYoungRejects = double(), ThinnessYoungOR = double(),
                     WastingYoungRejects = double(), WastingYoungOR = double()
                     )
################################################################################
#Stunting PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 1]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Stunting","~",factor),
                    data = SMOTEStunting[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 1]] = results[[factor, 1]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 2]] = CORSum/100
}
################################################################################
#Thinness PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 3]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Thinness","~",factor),
                    data = SMOTEThinness[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 3]] = results[[factor, 3]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 4]] = CORSum/100
}
################################################################################
#StuntingYoung PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 5]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Stunting","~",factor),
                    data = SMOTEStuntingYoung[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 5]] = results[[factor, 5]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 6]] = CORSum/100
}
################################################################################
#ThinnessYoung PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 7]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Thinness","~",factor),
                    data = SMOTEThinnessYoung[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 7]] = results[[factor, 7]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 8]] = CORSum/100
}
################################################################################
#WastingYoung PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 9]] = 0
  CORSum = 0
  for (i in 1:10){
    for (j in 1:10){
        UVLoR = glm(paste0("Wasting","~",factor),
                    data = SMOTEWastingYoung[[i]][[j]][["data"]],
                    family = "binomial"
                    )
        regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
        if (!is.na(regSummary[[2, 5]]) && regSummary[[2, 5]] < 0.05){
          results[[factor, 9]] = results[[factor, 9]] + 1
        }
        CORSum = CORSum + regSummary[[2, 2]] #COR
    }
  }
  results[[factor, 10]] = CORSum/100
}
################################################################################
write.csv(results, "FS_R-UVLoR.csv")
```



```{r Repeated Multivariate Logistic Regression (FS), warning = FALSE}
results = data.frame(StuntingRejects = double(), StuntingAOR = double(),
                     ThinnessRejects = double(), ThinnessAOR = double(),
                     StuntingYoungRejects = double(), StuntingYoungAOR = double(),
                     ThinnessYoungRejects = double(), ThinnessYoungAOR = double(),
                     WastingYoungRejects = double(), WastingYoungAOR = double()
                     )
################################################################################
#Stunting PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 1]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 2]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Stunting~.,
                data = SMOTEStunting[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 1]] = results[[factor, 1]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){ #Explained below
        results[[factor, 2]] = results[[factor, 2]] + regSummary[[index, 2]]
      }
    }
  }
}
#Dividing AOR by # of times we reject H0; had issue before where a factor couldn't be rejected but had a very high p-value and thus generated an absurd OR.
#Now we're only incrementing a factor's OR when it is significant, and dividing it by the # of times it is significant.
#We lose data every time a factor isn't significant, but in exchange get reliable AOR for use in describing the effect of significant factors
for (factor in names(stunting)[1:54]){
  results[[factor, 2]] = results[[factor, 2]]/results[[factor, 1]] 
}
################################################################################
#Thinness PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 3]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 4]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Thinness~.,
                data = SMOTEThinness[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 3]] = results[[factor, 3]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 4]] = results[[factor, 4]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 4]] = results[[factor, 4]]/results[[factor, 3]] 
}
################################################################################
#StuntingYoung PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 5]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 6]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Stunting~.,
                data = SMOTEStuntingYoung[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 5]] = results[[factor, 5]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 6]] = results[[factor, 6]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 6]] = results[[factor, 6]]/results[[factor, 5]] 
}
################################################################################
#ThinnessYoung PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 7]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 8]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Thinness~.,
                data = SMOTEThinnessYoung[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 7]] = results[[factor, 7]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 8]] = results[[factor, 8]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 8]] = results[[factor, 8]]/results[[factor, 7]] 
}
################################################################################
#WastingYoung PV-based feature selection & average crude odds ratios
for (factor in names(stunting)[1:54]){
  results[[factor, 9]] = 0 #Must instantiate columns or next lines can't access them
      results[[factor, 10]] = 0 # ^
}

for (i in 1:10){
  for (j in 1:10){
    index = 0
    UVLoR = glm(Wasting~.,
                data = SMOTEWastingYoung[[i]][[j]][["data"]],
                family = "binomial"
                )
    regSummary = tidy(UVLoR, exponentiate = TRUE, conf.level = 0.95)
    for (factor in names(stunting)[1:54]){
      index = index + 1
      if (!is.na(regSummary[[index, 5]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 9]] = results[[factor, 9]] + 1
      }
      if (!is.na(regSummary[[index, 2]]) && regSummary[[index, 5]] < 0.05){
        results[[factor, 10]] = results[[factor, 10]] + regSummary[[index, 2]]
      }
    }
  }
}
for (factor in names(stunting)[1:54]){
  results[[factor, 10]] = results[[factor, 10]]/results[[factor, 9]] 
}
################################################################################
write.csv(results, "FS_R-MVLoR.csv")
```



```{r Literature-Based}
#This one is pretty quick (it's almost like there's no computations...)
FS_LB = list(c(1, 2, 14, 4, 6:18, 19, 21:25, 28, 29, 33:36, 38, 47:54))
```

#####
# CLASSIFIER MODELS:
#####
```{r Random Forest}
RF_UB_Stunting
RF_SMOTE_Stunting
RF_UB_Thinness
RF_SMOTE_Thinness
```



```{r Support Vector Machine}
SVM_UB_Stunting
SVM_SMOTE_Stunting
SVM_UB_Thinness
SVM_SMOTE_Thinness
```



```{r Extreme Gradient Boosting}
XGB_UB_Stunting
XGB_SMOTE_Stunting
XGB_UB_Thinness
XGB_SMOTE_Thinness
```



```{r Multivariate Logistic Regression (OR), warning = FALSE}
#This MVLoR is for Odds Ratios, uses entire dataset.
#NOT used for feature selection for classifier models; do not mistake this data for the training/test set-derived repeated multivariate logistic regression above.
#library(devtools)
library(broom)
library(car)
library(performance)
library(MASS)

OR_Stunting = glm(Stunting~., data = stunting, family = "binomial")
write.csv(cbind(
          tidy(OR_Stunting, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_Stunting))),
          "OR_Stunting.csv"
          )
OR_Thinness = glm(Thinness~., data = thinness, family = "binomial")
write.csv(cbind(
          tidy(OR_Thinness, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_Thinness))),
          "OR_Thinness.csv"
          )
OR_StuntingYoung = glm(Stunting~., data = stuntingYoung, family = "binomial")
write.csv(cbind(
          tidy(OR_StuntingYoung, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_StuntingYoung))),
          "OR_StuntingYoung.csv"
          )
OR_ThinnessYoung = glm(Thinness~., data = thinnessYoung, family = "binomial")
write.csv(cbind(
          tidy(OR_ThinnessYoung, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_ThinnessYoung))),
          "OR_ThinnessYoung.csv"
          )
OR_WastingYoung = glm(Wasting~., data = wastingYoung, family = "binomial")
write.csv(cbind(
          tidy(OR_WastingYoung, exponentiate = TRUE, conf.level = 0.95)),
          #exp(confint(OR_WastingYoung))),
          "OR_WastingYoung.csv"
          )



# ################################################################################
# OR_Stunting = glm(Stunting~., data = stunting, family = "binomial")
# #summary(OR_Stunting)
# OddsRatio = exp(cbind(OR = coef(OR_Stunting), confint(na.omit(OR_Stunting))))
# #BH = p.adjust(summary(OR_Stunting)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_Stunting$coefficients[,4])), "OR_Stunting.csv")
# 
# ################################################################################
# OR_Thinness = glm(Thinness~., data = thinness, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_Thinness), confint(na.omit(OR_Thinness))))
# #BH = p.adjust(summary(OR_Thinness)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_Thinness$coefficients[,4])), "OR_Thinness.csv")
# 
# ################################################################################
# OR_StuntingYoung = glm(Stunting~., data = stuntingYoung, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_StuntingYoung), confint(na.omit(OR_StuntingYoung))))
# #BH = p.adjust(summary(OR_StuntingYoung)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_StuntingYoung$coefficients[,4])), "OR_StuntingYoung.csv")
# 
# ################################################################################
# OR_ThinnessYoung = glm(Thinness~., data = thinnessYoung, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_ThinnessYoung), confint(na.omit(OR_ThinnessYoung))))
# #BH = p.adjust(summary(OR_ThinnessYoung)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_ThinnessYoung$coefficients[,4])), "OR_ThinnessYoung.csv")
# 
# ################################################################################
# OR_WastingYoung = glm(Wasting~., data = wastingYoung, family = "binomial")
# #summary(OR_Thinness)
# OddsRatio = exp(cbind(OR = coef(OR_WastingYoung), confint(na.omit(OR_WastingYoung))))
# #BH = p.adjust(summary(OR_WastingYoung)$coefficients[,4], method = "BH")
# write.csv(cbind(OddsRatio, summary(OR_WastingYoung$coefficients[,4])), "OR_WastingYoung.csv")

```

#####
# ASSOCIATION RULE LEARNING:
#####
```{r Association Rule Learning}
#Takes a bit of time - couple minutes, maybe.
#DO NOT manually exit if program is slow or frozen; the Apriori algorithm is written in C, and has no qualms with messing things up in your computer if you interrupt it.
#Also, warning: do not increase maxlen above 3 unless you have a lot of free time and want your computer to be unresponsive.

#https://www.geeksforgeeks.org/apriori-algorithm-in-r-programming/
#Should I be doing this on SMOTEd data and averaging??
library(arules)
library(arulesViz)
ARL_Stunting = apriori(stunting,
                        parameter = list(support = 0.01, confidence = 0.1),
                        maxlen = 3
                        )
summary(ARL_Stunting)
inspect(ARL_Stunting)[1:5]
ARL_Thinness = apriori(thinness,
                        parameter = list(support = 0.01, confidence = 0.33),
                        maxlen = 3,
                        minlen = 2
                        )
ARL_StuntingYoung = apriori(StuntingYoung,
                        parameter = list(support = 0.01, confidence = 0.33),
                        maxlen = 3,
                        minlen = 2
                        )
ARL_ThinnessYoung = apriori(thinnessYoung,
                        parameter = list(support = 0.01, confidence = 0.33),
                        maxlen = 3,
                        minlen = 2
                        )
ARL_WastingYoung = apriori(WastingYoung,
                        parameter = list(support = 0.01, confidence = 0.33),
                        maxlen = 3,
                        minlen = 2
                        )
```