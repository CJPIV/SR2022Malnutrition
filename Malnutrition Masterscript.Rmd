---
title: "Malnutrition Masterscript"
author: "Jim Perry"
date: "5/30/2022"
output: pdf_document
---
# PREPROCESSING: Preparing data from original survey for use in statistical/ML analysis.
#####
```{r Import Data}
#Reading and cleaning the original spreadsheet from the survey.
library(readxl)
library(dplyr)

SURVEY = read_excel("SURVEY.xlsx")
SURVEY = SURVEY[-1,] #First row is a misread full of NAs - drop it
SURVEY = SURVEY[,-c(35, 54, 115, 134)] #Empty columns

#Now the fun part - manually rename any weird variable names
names(SURVEY)[13] = "Class" #Which class in the grade is the child in?
names(SURVEY)[17] = "FamilySize"
names(SURVEY)[18] = "NumOldBros"
names(SURVEY)[19] = "Bros<12?"
names(SURVEY)[20] = "Vaccine"
names(SURVEY)[21] = "BCGScar"
names(SURVEY)[22] = "FeverLast2Weeks"
names(SURVEY)[23] = "FeverQuantity"
names(SURVEY)[24] = "DiarrheaLast2Weeks"
names(SURVEY)[25] = "DiarrheaQuantity"
names(SURVEY)[26] = "CoughLast2Weeks"
names(SURVEY)[27] = "CoughQuantity"
names(SURVEY)[28] = "NailsTrimmed"
names(SURVEY)[29] = "NailsDirty"
names(SURVEY)[30] = "NailTrimFrequency"
names(SURVEY)[31] = "SchoolLat"
names(SURVEY)[32] = "SchoolLatDoors"
names(SURVEY)[33] = "SchoolLatFlies"
names(SURVEY)[34] = "SchoolLatVisibleStool"
names(SURVEY)[35] = "HeardOfAL"
names(SURVEY)[36] = "HeardOfTT"
names(SURVEY)[37] = "HeardOfHW"
names(SURVEY)[38] = "HeardOfHIV"
names(SURVEY)[39] = "HeardOfWorms"
names(SURVEY)[40] = "HeardOfMalaria"
names(SURVEY)[41] = "HeardOfTB"
names(SURVEY)[42] = "HeardOfSCh"
names(SURVEY)[43] = "ToldByFam"
names(SURVEY)[44] = "ToldByHP"
names(SURVEY)[45] = "ToldByTeacher"
names(SURVEY)[46] = "ToldByMedia"
names(SURVEY)[47] = "KnowWormsSpread"
names(SURVEY)[48] = "HowKnowWormsSpread"
names(SURVEY)[49] = "KnowWormsBad"
names(SURVEY)[50] = "HowKnowWormsBad"
names(SURVEY)[51] = "KnowAvoidWorms"
names(SURVEY)[52] = "HowKnowAvoidWorms"
names(SURVEY)[53] = "WhereLive"
names(SURVEY)[54] = "Address"
names(SURVEY)[55] = "Occupation"
names(SURVEY)[56] = "MomEduc"
names(SURVEY)[57] = "HouseFloorMats"
names(SURVEY)[58] = "KitchenSeparate"
names(SURVEY)[59] = "SepKitchenMats"
names(SURVEY)[60] = "SepKitchenRoof"
names(SURVEY)[61] = "SepKitchenWall"
names(SURVEY)[62] = "SepKitchenNone"
names(SURVEY)[63] = "CookWood"
names(SURVEY)[64] = "CookGas"
names(SURVEY)[65] = "CookCoal"
names(SURVEY)[66] = "CookKerosine"
names(SURVEY)[67] = "CookElectric"
names(SURVEY)[68] = "Electricity"
names(SURVEY)[69] = "Radio"
names(SURVEY)[70] = "TV"
names(SURVEY)[71] = "Phone"
names(SURVEY)[72] = "WhyPhone"
names(SURVEY)[73] = "Cattle"
names(SURVEY)[74] = "Sheep/Goat"
names(SURVEY)[75] = "Chicken"
names(SURVEY)[76] = "HousePet"
names(SURVEY)[77] = "None"
names(SURVEY)[78] = "HouseHasWater"
names(SURVEY)[79] = "WhereGetWater"
names(SURVEY)[80] = "WaterTreated"
names(SURVEY)[81] = "HowTreat"
names(SURVEY)[82] = "FamLat"
names(SURVEY)[83] = "LatOutside"
names(SURVEY)[84] = "LatDistanceHouse"
names(SURVEY)[85] = "LatDistanceKitchen"
names(SURVEY)[86] = "LatConnectedTo"
names(SURVEY)[87] = "RiverBathFrequency"
names(SURVEY)[88] = "RiverLaundryFrequency"
names(SURVEY)[89] = "DeficateField"
names(SURVEY)[90] = "UseSchoolLat"
names(SURVEY)[91] = "UseTP"
names(SURVEY)[92] = "WashHandsLat"
names(SURVEY)[93] = "WashHandsLatHow"
names(SURVEY)[94] = "WashHandsSoapLatFrequency"
names(SURVEY)[95] = "WashHandsEat"
names(SURVEY)[96] = "WashHandsEatHow"
names(SURVEY)[97] = "WashHandsSoapEatFrequency"
names(SURVEY)[98] = "EatSoil"
names(SURVEY)[99] = "FavFruit"
names(SURVEY)[100] = "WashFruit"
names(SURVEY)[101] = "EatRawVeg"
names(SURVEY)[102] = "WashRawVegFrequency"
names(SURVEY)[103] = "WalkBarefoot"
names(SURVEY)[104] = "HomeShoeOrSandal"
names(SURVEY)[105] = "ForWhatBarefoot"
names(SURVEY)[106] = "DewormPill"
names(SURVEY)[107] = "WhenDewormPill"
names(SURVEY)[108] = "Antibiotics"
names(SURVEY)[109] = "MostFreqFood"
names(SURVEY)[110] = "TakesMeds"
names(SURVEY)[111] = "Name/TypeOfMeds"
names(SURVEY)[112] = "AntiMalaria3Months"
names(SURVEY)[113] = "Wheezing"
names(SURVEY)[114] = "Wheezing2Yrs"
names(SURVEY)[115] = "Wheezing1Yr"
names(SURVEY)[116] = "Wheezing1YrQuantity"
names(SURVEY)[117] = "Asthma"
names(SURVEY)[118] = "Asthma2Yrs"
names(SURVEY)[119] = "Asthma1Yr"
names(SURVEY)[120] = "DocConfirmedAsthma"
names(SURVEY)[121] = "Rash"
names(SURVEY)[122] = "RashElbow"
names(SURVEY)[123] = "RashKnees"
names(SURVEY)[124] = "RashAnkles"
names(SURVEY)[125] = "RashButt"
names(SURVEY)[126] = "RashNeck"
names(SURVEY)[127] = "RashEyesEars"
names(SURVEY)[128] = "HayFever"
names(SURVEY)[129] = "HayFever2Yrs"
names(SURVEY)[130] = "HayFever1Yr"

SURVEY = SURVEY[,-c(1, 7, 8, 9)] #Don't need these columns; I forgot to remove them earlier, and doing so now would mean I'd have to re-index the >100 variable renames I just did above. No way.
```

```{r Missing Value Imputation}
#Fill in missing values in dataset
library(VIM)
library(fastDummies)

SURVEY = dummy_cols(.data = SURVEY, ignore_na = TRUE)

#Convert categorical columns into factors
catIndices = c()
SURVEY[,catIndices] = lapply(SURVEY[,catIndices], factor)



kNNSurvey = kNN(SURVEY, variable = colnames(SURVEY), k = 5, select = AL:HayFever1Yr)
```

```{r Dependent Variable Calculations}
#Derive WAZ/HAZ/BAZ for specific analysis
#WAZ: Weight-relative-to-age Z-score
#HAZ: Height-relative-to-age Z-score (WHO says unreliable for ages 11+, so ignore older kids)
#BAZ: BMI-relative-to-age Z-score

library(anthroplus)

with(
  SURVEY,
  anthroplus_zscores(sex = SURVEY["Sex"]+1,
                              age_in_months = SURVEY["Age"]*12,
                              weight_in_kg = Survey["weight"],
                              height_in_cm = SURVEY["Height"]
                              )
)
# names(SURVEY)[] = "WAZ"
# names(SURVEY)[] = "HAZ"
# names(SURVEY)[] = "BAZ"
# names(SURVEY)[] = ""
# names(SURVEY)[] = ""
```

# TRAIN/TEST SPLITS: Prepares train/test splits for use in feature selection and
# classifier model training. Each dependent variable (Stunting and Thinness)
# will be cloned; one clone will remain as-is (unbalanced), and the other will
# be balanced via SMOTE for (hopefully) improved classifier results.
#####
```{r Stunting Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

# stunting = read.csv("C:/Users/Carmen Perry/Desktop/Colgate Stuff/Summer Research 2022/MalnutritionProject-20220523T125732Z-001/MalnutritionProject/Code/SR2022Malnutrition/Data/stunting.csv")
stunting = read.csv("stunting.csv")

set.seed(13)
testFolds = createFolds(stunting$Stunting, k = 10) #Create k splits
testSetsStunting = list(stunting[testFolds$Fold01, ], stunting[testFolds$Fold02, ],
                        stunting[testFolds$Fold03, ], stunting[testFolds$Fold04, ],
                        stunting[testFolds$Fold05, ], stunting[testFolds$Fold06, ],
                        stunting[testFolds$Fold07, ], stunting[testFolds$Fold08, ],
                        stunting[testFolds$Fold09, ], stunting[testFolds$Fold10, ]
                        )
sansTestStunting = list(stunting[-testFolds$Fold01, ], stunting[-testFolds$Fold02, ],
                        stunting[-testFolds$Fold03, ], stunting[-testFolds$Fold04, ],
                        stunting[-testFolds$Fold05, ], stunting[-testFolds$Fold06, ],
                        stunting[-testFolds$Fold07, ], stunting[-testFolds$Fold08, ],
                        stunting[-testFolds$Fold09, ], stunting[-testFolds$Fold10, ]
                        )
UBStunting = list()
for (i in 1:10){
  curSet = sansTestStunting[[i]]
  validationFolds = createFolds(sansTestStunting[[i]]$Stunting, k = 10)
  UBStunting[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStunting = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStunting[[i]] = list()
  for (j in 1:10){
    SMOTEStunting[[i]][[j]] = SMOTE(X = as.data.frame(UBStunting[[i]][[j]]),
                              target = UBStunting[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 3
                              )
     SMOTEStunting[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBStunting[[1]][[1]]$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEStunting[[1]][[1]]$data$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (SMOTE)')
# abline(h = 0)


# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestStunting = rbind(testSetsStunting[1:10])
# ForFS_UBStunting = rbind(sansTestStunting[1:10])
# ForFS_SMOTEStunting = rbind(SMOTEStunting[1:10])

# write.csv(ForFS_TestStunting, "ForFS_TestStunting.csv")
# write.csv(ForFS_UBStunting, "ForFS_UBStunting.csv")
# write.csv(ForFS_SMOTEStunting, "ForFS_SMOTEStunting.csv")
```

```{r Thinness Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

# thinness = read.csv("C:/Users/Carmen Perry/Desktop/Colgate Stuff/Summer Research 2022/MalnutritionProject-20220523T125732Z-001/MalnutritionProject/Code/SR2022Malnutrition/Data/thinness.csv")
thinness = read.csv("thinness.csv")

set.seed(13)
testFolds = createFolds(thinness$Thinness, k = 10) #Create k splits
testSetsThinness = list(thinness[testFolds$Fold01, ], thinness[testFolds$Fold02, ],
                        thinness[testFolds$Fold03, ], thinness[testFolds$Fold04, ],
                        thinness[testFolds$Fold05, ], thinness[testFolds$Fold06, ],
                        thinness[testFolds$Fold07, ], thinness[testFolds$Fold08, ],
                        thinness[testFolds$Fold09, ], thinness[testFolds$Fold10, ]
                        )
sansTestThinness = list(thinness[-testFolds$Fold01, ], thinness[-testFolds$Fold02, ],
                        thinness[-testFolds$Fold03, ], thinness[-testFolds$Fold04, ],
                        thinness[-testFolds$Fold05, ], thinness[-testFolds$Fold06, ],
                        thinness[-testFolds$Fold07, ], thinness[-testFolds$Fold08, ],
                        thinness[-testFolds$Fold09, ], thinness[-testFolds$Fold10, ]
                        )
UBThinness = list()
for (i in 1:10){
  curSet = sansTestThinness[[i]]
  validationFolds = createFolds(sansTestThinness[[i]]$Thinness, k = 10)
  UBThinness[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinness = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinness[[i]] = list()
  for (j in 1:10){
    SMOTEThinness[[i]][[j]] = SMOTE(X = as.data.frame(UBThinness[[i]][[j]]),
                              target = UBThinness[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 24
                              )
    SMOTEThinness[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBThinness[[1]][[1]]$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEThinness[[1]][[1]]$data$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (SMOTE)')
# abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestThinness = rbind(testSetsThinness[1:10])
# ForFS_UBThinness = rbind(trainSetsUBThinness[1:10])
# ForFS_SMOTEThinness = rbind(trainSetsSMOTEThinness[1:10])

# write.csv(ForFS_TestThinness, "ForFS_TestThinness.csv")
# write.csv(ForFS_UBThinness, "ForFS_UBThinness.csv")
# write.csv(ForFS_SMOTEThinness, "ForFS_SMOTEThinness.csv")
```

#####
# FEATURE SELECTION: Selects important model features to reduce noise and help 
# prevent overfitting. All feature selection methods should be run on SMOTE-
# balanced datasets.
#####
```{r ReliefF}
#This one takes ~90 minutes to run. Have fun waiting.
library(FSelectorRcpp)
################################################################################
FS_RLF_Stunting = list()
for (i in 1:10){
  FS_RLF_Stunting[[i]] = list()
  for (j in 1:10){
    relF = relief(Stunting~.,
                as.data.frame(SMOTEStunting[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Stunting[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_Thinness = list()
for (i in 1:10){
  FS_RLF_Thinness[[i]] = list()
  for (j in 1:10){
    relF = relief(Thinness~.,
                as.data.frame(SMOTEThinness[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Thinness[[i]][[j]] = relF[order(relF$importance), ]
  }
}
```

```{r InfoGain}
#This one is pretty quick.
library(FSelectorRcpp)
################################################################################
FS_IG_Stunting = list()
for (i in 1:10){
  FS_IG_Stunting[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Stunting~.,
                              data = as.data.frame(SMOTEStunting[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_Stunting[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
################################################################################
FS_IG_Thinness = list()
for (i in 1:10){
  FS_IG_Thinness[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Thinness~.,
                              data = as.data.frame(SMOTEThinness[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_Thinness[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
```

```{r mRMR}
library(mRMRe)
#In progress; mRMR finishes selecting, but results should be rewritten for legibility.
#This one is pretty quick.
FS_MRMR_Stunting = list()
for (i in 1:10){
  FS_MRMR_Stunting[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEStunting[[i]][[j]]$data)
    FS_MRMR_Stunting[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 98,
                                              feature_count = 13
                                              )
  }
}
################################################################################
FS_MRMR_Thinness = list()
for (i in 1:10){
  FS_MRMR_Thinness[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEThinness[[i]][[j]]$data)
    FS_MRMR_Thinness[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 98,
                                              feature_count = 13
                                              )
  }
}
```

```{r JMI}
#This one is pretty quick.
library(praznik)
################################################################################
FS_JMI_Stunting = list()
for (i in 1:10){
  FS_JMI_Stunting[[i]] = list()
  for (j in 1:10){
    FS_JMI_Stunting[[i]][[j]] = JMI(SMOTEStunting[[i]][[j]]$data[1:97], 
                                    SMOTEStunting[[i]][[j]]$data$Stunting,
                                    k = 13
                                    )
  }
}
################################################################################
FS_JMI_Thinness = list()
for (i in 1:10){
  FS_JMI_Thinness[[i]] = list()
  for (j in 1:10){
    FS_JMI_Thinness[[i]][[j]] = JMI(SMOTEThinness[[i]][[j]]$data[1:97],
                                    SMOTEThinness[[i]][[j]]$data$Thinness,
                                    k = 13
                                    )
  }
}
```

```{r Literature-Based}
#This one is pretty quick (it's almost like there's no computations...)
FS_LB_Stunting = list(c('Age'), c('HouseholdNumb'), c('NumbOldSib'), c('Young12'),  c('DiarrheaPWeek'), c('Sex'), c('No.Vaccine'),c('NoTrimNails'), c('DirtyNails'), c('TrimOnce2Week'), c('TrimOnceMonth'), c('NoLatDoors'), c('Flies'), c('StoolFloor'), c('Suburban'), c('Rural'), c('NoLitMom'), c('PrimMom'), c('HSMom'), c('Radio'), c('TV'), c('Cattle'), c('SheepGoat'), c('Chicken'), c('Pet'), c('PotableWater'), c('DrinkDirect'), c('OwnLatrine'), c('Outside.Latrine'), c('Sewage'), c('Ditch'), c('River'), c('AlwaysSchoolLat'), c('SometimesSchoolLat'), c('SometimesTP'), c('NeverTP'), c('SometimesWashHands'), c('NeverWashHands'), c('WaterWashHands'), c('AlwaysSoil'), c('SometimesSoil'), c('NeverWashFruits'), c('SometimesWashFruits'), c('SometimesBarefoot'), c('AlwaysBarefoot'), c('NoShoes'), c('Sandals'), c('Deworm'), c('Antibiotic'), c('Drugs'), c('Rash'), c('Sneeze'), c('HayFever'), c('Colds')) 

FS_LB_Thinness = FS_LB_Stunting
```

#####
# CLASSIFIER MODELS:
#####
```{r Random Forest}
RF_UB_Stunting
RF_SMOTE_Stunting
RF_UB_Thinness
RF_SMOTE_Thinness
```

```{r Support Vector Machine}
SVM_UB_Stunting
SVM_SMOTE_Stunting
SVM_UB_Thinness
SVM_SMOTE_Thinness
```

```{r Extreme Gradient Boosting}
XGB_UB_Stunting
XGB_SMOTE_Stunting
XGB_UB_Thinness
XGB_SMOTE_Thinness
```

```{r Univariate Logistic Regression}
#Classifier or FS?

```

```{r Multivariate Logistic Regression, warning = FALSE}
#First MVLoR is for Odds Ratios, uses entire dataset.
#Second is for FS. Uses traditional train/test splits and 95% prevalence cutoff thresholds for choosing features.
#library(devtools)
library(broom)
OR_Stunting = glm(Stunting~., data = stunting, family = "binomial")
OR_Thinness = glm(Thinness~., data = thinness, family = "binomial")
write.csv(cbind( 
          tidy(OR_Stunting, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_Stunting))),
          "OR_Stunting.csv"
          )
write.csv(cbind( 
          tidy(OR_Thinness, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_Thinness))),
          "OR_Thinness.csv"
          )
#write.csv(tidy(OR_Thinness, exponentiate = TRUE, conf.level = 0.95), "OR_Thinness.csv")

```

#####
# ASSOCIATION RULE LEARNING:
#####
```{r Association Rule Learning}
library(arules)
library(arulesViz)

stuntingRules = apriori(stunting,
                        parameter = list(support = 0.1, confidence = 0.8),
                        maxlen = 3
                        )
thinnessRules = apriori(thinness,
                        parameter = list(support = 0.1, confidence = 0.8),
                        maxlen = 3
                        )
```