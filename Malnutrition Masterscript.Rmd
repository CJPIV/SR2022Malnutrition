---
title: "Malnutrition Masterscript"
author: "Jim Perry"
date: "5/30/2022"
output: pdf_document
---
# PREPROCESSING: Preparing data from original survey for use in statistical/ML analysis.
#####
```{r Import Data}
#Reading and cleaning the original spreadsheet from the survey.
library(readxl)
library(dplyr)

SURVEY = read_excel("SURVEY.xlsx")
SURVEY = SURVEY[-1,] #First row is a misread full of NAs - drop it
SURVEY = SURVEY[,-c(35, 54, 115, 134)] #Empty columns

#Now the fun part - manually rename any weird variable names
names(SURVEY)[13] = "Class" #Which class in the grade is the child in?
names(SURVEY)[17] = "FamilySize"
names(SURVEY)[18] = "NumOldBros"
names(SURVEY)[19] = "Bros<12?"
names(SURVEY)[20] = "Vaccine"
names(SURVEY)[21] = "BCGScar"
names(SURVEY)[22] = "FeverLast2Weeks"
names(SURVEY)[23] = "FeverQuantity"
names(SURVEY)[24] = "DiarrheaLast2Weeks"
names(SURVEY)[25] = "DiarrheaQuantity"
names(SURVEY)[26] = "CoughLast2Weeks"
names(SURVEY)[27] = "CoughQuantity"
names(SURVEY)[28] = "NailsTrimmed"
names(SURVEY)[29] = "NailsDirty"
names(SURVEY)[30] = "NailTrimFrequency"
names(SURVEY)[31] = "SchoolLat"
names(SURVEY)[32] = "SchoolLatDoors"
names(SURVEY)[33] = "SchoolLatFlies"
names(SURVEY)[34] = "SchoolLatVisibleStool"
names(SURVEY)[35] = "HeardOfAL"
names(SURVEY)[36] = "HeardOfTT"
names(SURVEY)[37] = "HeardOfHW"
names(SURVEY)[38] = "HeardOfHIV"
names(SURVEY)[39] = "HeardOfWorms"
names(SURVEY)[40] = "HeardOfMalaria"
names(SURVEY)[41] = "HeardOfTB"
names(SURVEY)[42] = "HeardOfSCh"
names(SURVEY)[43] = "ToldByFam"
names(SURVEY)[44] = "ToldByHP"
names(SURVEY)[45] = "ToldByTeacher"
names(SURVEY)[46] = "ToldByMedia"
names(SURVEY)[47] = "KnowWormsSpread"
names(SURVEY)[48] = "HowKnowWormsSpread"
names(SURVEY)[49] = "KnowWormsBad"
names(SURVEY)[50] = "HowKnowWormsBad"
names(SURVEY)[51] = "KnowAvoidWorms"
names(SURVEY)[52] = "HowKnowAvoidWorms"
names(SURVEY)[53] = "WhereLive"
names(SURVEY)[54] = "Address"
names(SURVEY)[55] = "Occupation"
names(SURVEY)[56] = "MomEduc"
names(SURVEY)[57] = "HouseFloorMats"
names(SURVEY)[58] = "KitchenSeparate"
names(SURVEY)[59] = "SepKitchenMats"
names(SURVEY)[60] = "SepKitchenRoof"
names(SURVEY)[61] = "SepKitchenWall"
names(SURVEY)[62] = "SepKitchenNeither" #Should be able to get removed in 1HE
names(SURVEY)[63] = "CookWood"
names(SURVEY)[64] = "CookGas"
names(SURVEY)[65] = "CookCoal"
names(SURVEY)[66] = "CookKerosine"
names(SURVEY)[67] = "CookElectric"
names(SURVEY)[68] = "Electricity"
names(SURVEY)[69] = "Radio"
names(SURVEY)[70] = "TV"
names(SURVEY)[71] = "Phone"
names(SURVEY)[72] = "WhyPhone"
names(SURVEY)[73] = "Cattle"
names(SURVEY)[74] = "Sheep/Goat"
names(SURVEY)[75] = "Chicken"
names(SURVEY)[76] = "HousePet"
names(SURVEY)[77] = "NoAnimal"
names(SURVEY)[78] = "HouseHasWater"
names(SURVEY)[79] = "WhereGetWater"
names(SURVEY)[80] = "WaterTreated"
names(SURVEY)[81] = "HowTreat"
names(SURVEY)[82] = "FamLat"
names(SURVEY)[83] = "LatInside"
names(SURVEY)[84] = "LatDistanceHouse"
names(SURVEY)[85] = "LatDistanceKitchen"
names(SURVEY)[86] = "LatConnectedTo"
names(SURVEY)[87] = "RiverBathFrequency"
names(SURVEY)[88] = "RiverLaundryFrequency"
names(SURVEY)[89] = "DefecateField"
names(SURVEY)[90] = "UseSchoolLat"
names(SURVEY)[91] = "UseTP"
names(SURVEY)[92] = "WashHandsLat"
names(SURVEY)[93] = "WashHandsLatHow"
names(SURVEY)[94] = "WashHandsSoapLatFrequency"
names(SURVEY)[95] = "WashHandsEat"
names(SURVEY)[96] = "WashHandsEatHow"
names(SURVEY)[97] = "WashHandsSoapEatFrequency"
names(SURVEY)[98] = "EatSoil"
names(SURVEY)[99] = "FavFruit"
names(SURVEY)[100] = "WashFruit"
names(SURVEY)[101] = "EatRawVeg"
names(SURVEY)[102] = "WashRawVegFrequency"
names(SURVEY)[103] = "WalkBarefoot"
names(SURVEY)[104] = "HomeShoeOrSandal"
names(SURVEY)[105] = "ForWhatBarefoot"
names(SURVEY)[106] = "DewormPill"
names(SURVEY)[107] = "WhenDewormPill"
names(SURVEY)[108] = "Antibiotics"
names(SURVEY)[109] = "MostFreqFood"
names(SURVEY)[110] = "TakesMeds"
names(SURVEY)[111] = "Name/TypeOfMeds"
names(SURVEY)[112] = "AntiMalaria3Months"
names(SURVEY)[113] = "Wheezing"
names(SURVEY)[114] = "Wheezing2Yrs"
names(SURVEY)[115] = "Wheezing1Yr"
names(SURVEY)[116] = "Wheezing1YrQuantity"
names(SURVEY)[117] = "Asthma"
names(SURVEY)[118] = "Asthma2Yrs"
names(SURVEY)[119] = "Asthma1Yr"
names(SURVEY)[120] = "DocConfirmedAsthma"
names(SURVEY)[121] = "Rash"
names(SURVEY)[122] = "RashElbow"
names(SURVEY)[123] = "RashKnees"
names(SURVEY)[124] = "RashAnkles"
names(SURVEY)[125] = "RashButt"
names(SURVEY)[126] = "RashNeck"
names(SURVEY)[127] = "RashEyesEars"
names(SURVEY)[128] = "HayFever"
names(SURVEY)[129] = "HayFever2Yrs"
names(SURVEY)[130] = "HayFever1Yr"

SURVEY = SURVEY[,-c(1, 7, 8, 9)] #Don't need these columns; I forgot to remove them earlier, and doing so now would mean I'd have to re-index the >100 variable renames I just did above. No way.
```



```{r Handling NAs & Datatype Conversions, warning = FALSE}
reducedSurvey = SURVEY[,-c(44, 46, 48, 51, 64, 95, 101, 105, 107)] #Potential candidates for removal from dataset; we'll work with this, as I think these changes will end up in the final iteration.
################################################################################
#Convert "Where do you get your water from?" into "Tap water" b/c it's majority case
tapWater = c()
for (i in 1:1036){
  if (is.na(reducedSurvey[[i, 70]])){
    tapWater[[i]] = 0
    next
  }
  if (tolower(reducedSurvey[[i, 70]]) == "tap water"){
    tapWater[[i]] = 1
  }else{
    tapWater[[i]] = 0
  }
}
tapWater = as.numeric(as.character(tapWater))
reducedSurvey[[70]] = tapWater
names(reducedSurvey)[[70]] = "TapWater"
################################################################################
#1HE Rural/Suburban/Urban split
rural = c()
suburban = c()
for (i in 1:1036){
  if (is.na(reducedSurvey[[i, 46]])){
      next
  }
  neighborhood = reducedSurvey[[i, 46]]
  if(neighborhood == 0){
      rural[[i]] = 0
      suburban[[i]] = 0
  }else if(neighborhood == 1){
    rural[[i]] = 0
    suburban[[i]] = 1
  }else if (neighborhood == 2){
      rural[[i]] = 1
      suburban[[i]] = 0
    }
}
suburban = as.numeric(as.character(suburban))
reducedSurvey[[46]] = suburban
names(reducedSurvey)[[46]] = "Suburban"
rural = as.numeric(as.character(rural)) #Can't convert directly from double to numeric
reducedSurvey[[47]] = rural# = replace(reducedSurvey, reducedSurvey$Address, rural)
names(reducedSurvey)[[47]] = "Rural"
################################################################################
#Remove units from measurements so they can be converted to numerics
for (i in 1:1036){
  distance = reducedSurvey[[i, 75]]
  reducedSurvey[[i, 75]] = substr(distance, 1, length(distance))
  distance2 = reducedSurvey[[i, 76]]
  reducedSurvey[[i, 76]] = substr(distance2, 1, length(distance2))
  whenDeworm = reducedSurvey[[i, 96]]
  reducedSurvey[[i, 96]] = substr(whenDeworm, 1, length(whenDeworm))

}
reducedSurvey[[75]] = as.numeric(reducedSurvey[[75]]) #Can typecast directly to numeric
reducedSurvey[[76]] = as.numeric(reducedSurvey[[76]])
reducedSurvey[[96]] = as.numeric(reducedSurvey[[96]])
################################################################################
#Convert "Others" disease presence factor from datatype "character" recording names of other present diseases to a binary recording presence of other diseases
# otherDisease = c()
# for (i in 1:1036){
#   if (is.na(reducedSurvey[[i, 5]])){
#       otherDisease[[i]] = 0
#       next
#   }
#   if (reducedSurvey[[i,5]] == "0"){
#     otherDisease[[i]] = 0
#   }else{
#     otherDisease[[i]] = 1
#   }
# }
# reducedSurvey[[5]] = as.numeric(as.character(otherDisease)) #Can't convert directly from double to numeric
# names(reducedSurvey)[[5]] = "OtherDisease"
################################################################################
#These factors have number measurements but are of datatype "character"; simple conversion to datatype "numeric"
changeInds = c(19, 21, 23, 31:42, 50:59, 64:68, 72, 76, 77, 82:117)
for (factor in changeInds){
  reducedSurvey[[factor]] = as.numeric(reducedSurvey[[factor]])
}
################################################################################
#Drop "Class"? Doesn't seem specific enough to be useful.
reducedSurvey = reducedSurvey[, -9]
################################################################################
#Adjusting data values
#Convert "99"s in numeric factors to "Na"s for later imputation under kNN algorithm
for (sample in 1:1036){
  for (factor in 11:116) #Columns where 99s indicate "I don't know" as a possible answer
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if (reducedSurvey[[sample, factor]] == 99){
      reducedSurvey[[sample, factor]] = NA
    }
}

#A sample's height was misinput as 92.00 meters.
for (sample in 1:1036){
  if (reducedSurvey[[sample, 10]] == 92.00){ 
    reducedSurvey[[sample, 10]] = NA
    break
  }
}

#Impute conditional responses where "NA" implies 0
for (sample in 1:1036){
  for (factor in c(18, 20, 22, 31:41, 53, 55:58, 64:67, 71, 95, 109:113)){
    if (is.na(reducedSurvey[[sample, factor]])){
      reducedSurvey[[sample, factor]] = 0
    }
  }
}

#These columns were input as a different binary; as "NA/0" instead of "0/1"
for (sample in 1:1036){
  for (factor in c(30, 54, 63)){ 
    if (is.na(reducedSurvey[[sample, factor]])){
      reducedSurvey[[sample, factor]] = 0
    }else if (reducedSurvey[[sample, factor]] == 0){
      reducedSurvey[[sample, factor]] = 1
    }else{
      reducedSurvey[[sample, factor]] = 0
    }
  }
}

#Convert different numeric binaries 0/2 binaries into 0/1
for (sample in 1:1036){
  for (factor in c(49, 65)){
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if (reducedSurvey[[sample, factor]] == 2){
      reducedSurvey[[sample, factor]] = 1
    }else if (reducedSurvey[[sample, factor]] != 0){
      reducedSurvey[[sample, factor]] = NA
    }
  }
}
#CookElectric's binary is 0/4; converting into 0/1
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 58]])){
      next
    }else if (reducedSurvey[[sample, 58]] == 4){
      reducedSurvey[[sample, 58]] = 1
    }else if (reducedSurvey[[sample, 58]] != 0){
      reducedSurvey[[sample, 58]] = NA
    }
  }
}
#0/3 here
for (sample in 1:1036){{
    if (is.na(reducedSurvey[[sample, 66]])){
      next
    }else if (reducedSurvey[[sample, 66]] == 3){
      reducedSurvey[[sample, 66]] = 1
    }else if (reducedSurvey[[sample, 6]] != 0){
      reducedSurvey[[sample, 66]] = NA
    }
  }
}

#copySet = reducedSurvey
#reducedSurvey = copySet

#Setting weird values in binary vectors (anything that's not 0/1) to NA for imputation
for (sample in 1:1036){
  for (factor in c(15, 16, 21, 23, 24, 49, 51, 58, 65, 68, 72:73)){
    if (is.na(reducedSurvey[[sample, factor]])){
      next
    }else if ((reducedSurvey[[sample, factor]] != 0) & (reducedSurvey[[sample, factor]] != 1)){
      reducedSurvey[[sample, factor]] = NA
    }
  }
}

reducedSurvey$HouseFloorMats = tolower(reducedSurvey$HouseFloorMats)

#Getting rid of some more weird values
for (sample in 1:1036){
  for (factor in c(85:86))
  if (is.na(reducedSurvey[[sample, factor]])){
    next
  }
  else if (reducedSurvey[[sample, factor]] == 21 || reducedSurvey[[sample, factor]] == 10){
    reducedSurvey[[sample, factor]] = NA
  }
}

for (sample in 1:1036){
  if (is.na(reducedSurvey[[sample, 10]])){
    next
  }
  else if (reducedSurvey[[sample, 10]] == 1.79){
    reducedSurvey[[sample, 10]] = 1.29
  }
}
```



```{r kNN, One Hot Encoding}
library(VIM)
library(fastDummies)
################################################################################
#DEAL WITH CATEGORICALS
#impute ordinal categoricals
imputed = kNN(reducedSurvey, variable = colnames(reducedSurvey), k = 5, impNA = TRUE)
imputed = subset(imputed, select = AL:HayFever1Yr)

#1HE nominal categoricals
#I already encoded some of these by hand in the last code block, but the remainder can be handled by this imported function
oneHotEncoded = dummy_cols(.data = imputed,
                     ignore_na = TRUE,
                     remove_first_dummy =  TRUE,
                     split = ",")
oneHotEncoded = oneHotEncoded[,-c(48, 123)] #One category got broken up and the other was created to account for a sample answer that got misinput as both
oneHotEncoded[[843, 121]] = 1

```



```{r Final Layer of Data Handling}
#Handle variables we decided to remove or calculate differently after I wrote 300 lines of code. I'm not mad - I'm not just rewriting everything above to account for new index values.
processed = oneHotEncoded[, -c(1:5, 11, 13:14, 18, 20, 22, 25:26, 30:44, 49, 52:56, 61, 66, 70, 73:75, 83, 86, 92, 94, 99:104, 107:112, 114:119, 124)]

#Encode mother's education
for (i in 1:1036){
  education = processed[[i, 19]]
  processed$momFinishedPrimary[[i]] = 0
  processed$momFinishedSecondary[[i]] = 0
  processed$momFinishedTertiary[[i]] = 0
  if (education == 1){
    processed$momFinishedPrimary[[i]] = 1
  }else if (education == 2){
    processed$momFinishedSecondary[[i]] = 1
  }else if (education == 3){
    processed$momFinishedTertiary[[i]] = 1
  }
}

processed$momFinishedPrimary = as.numeric(as.character(processed$momFinishedPrimary))
processed$momFinishedSecondary = as.numeric(as.character(processed$momFinishedSecondary))
processed$momFinishedTertiary = as.numeric(as.character(processed$momFinishedTertiary))
processed = processed[, -19]

#Convert house floor materials into a binary for whether that floor is dust or not
for (i in 1:1036){
  if (sum(processed[i,57:61]) > 0){
    processed$houseDustFloor[[i]] = 0
  }else{
    processed$houseDustFloor[[i]] = 1
  }
}
processed$houseDustFloor = as.numeric(as.character(processed$houseDustFloor))
processed = processed[,-c(57:60)]

#Converting 1/2 binaries to 0/1
for (sample in 1:1036){
  for (factor in 53:56){
    if (processed[sample, factor] == 2){
      processed[sample, factor] = 0
    }
  }
}

#Every sample has the same answer for this variable - it tells us nothing
processed = processed[,-c(21)]
processed = processed[,-3]

#1HE washing hands after lat into "never", "with water", and "with soap" groups
for (i in 1:1036){
  if (processed[[i,38]] == 2){
    processed$washHandsLatWater[[i]] = 0
    processed$washHandsLatWaterSoap[[i]] = 0
  }else{
    if (processed[[i, 39]] == 0){
      processed$washHandsLatWater[[i]] = 1
      processed$washHandsLatWaterSoap[[i]] = 0
    }else if (processed[[i, 39]] == 1){
      processed$washHandsLatWater[[i]] = 0
      processed$washHandsLatWaterSoap[[i]] = 1
    }
  }
}
processed$washHandsLatWater = as.numeric(as.character(processed$washHandsLatWater))
processed$washHandsLatWaterSoap = as.numeric(as.character(processed$washHandsLatWaterSoap))
processed = processed[, -c(38:39)]

#1HE washing hands before eat into "never", "with water", "with soap" groups
for (i in 1:1036){
  if (processed[[i,38]] == 2){
    processed$washHandsWaterB4Eat[[i]] = 0
    processed$washHandsSoapB4Eat[[i]] = 0
  }else{
    if (processed[[i, 39]] == 0){
      processed$washHandsWaterB4Eat[[i]] = 1
      processed$washHandsSoapB4Eat[[i]] = 0
    }else if (processed[[i, 39]] == 1){
      processed$washHandsWaterB4Eat[[i]] = 0
      processed$washHandsSoapB4Eat[[i]] = 1
    }
  }
}
processed$washHandsWaterB4Eat = as.numeric(as.character(processed$washHandsWaterB4Eat))
processed$washHandsSoapB4Eat = as.numeric(as.character(processed$washHandsSoapB4Eat))
processed = processed[, -c(38:39)]

#Change ternary variables into binaries based on biological significance
for (sample in 1:1036){
  for (factor in c(33:39)){
    if (processed[[sample, factor]] == 2){
      processed[[sample, factor]] = 1
    }else{
      processed[[sample, factor]] = 0
    }
  }
}
names(processed)[[33]] = "RiverBathing"
names(processed)[[34]] = "RiverLaundry"

#More ternary variables, but split differently according to biological significance
for (sample in 1:1036){
  for (factor in c(40:42)){
    if (processed[[sample, factor]] != 0){
      processed[[sample, factor]] = 1
    }else{
      processed[[sample, factor]] = 0
    }
  }
}
```

```{r Dependent Variable Calculations}
#Derive WAZ/HAZ/BAZ for specific analysis
#WAZ: Weight-relative-to-age Z-score
#HAZ: Height-relative-to-age Z-score (WHO says unreliable for ages 11+, so ignore older kids)
#BAZ: BMI-relative-to-age Z-score

library(anthroplus)

depVars = anthroplus_zscores(sex = processed[["Sex"]]+1,
                             age_in_months = processed[["Age"]]*12,
                             weight_in_kg = processed[["weight"]],
                             height_in_cm = processed[["Height"]]*100
                             )

for (i in 1:1036){
  ZHeight = depVars$zhfa[[i]]
  processed$HAZ[[i]] = ZHeight
  processed$Stunting[[i]] = ifelse(ZHeight < -2, 1, 0 )
  
  ZBMI = depVars$zbfa[[i]]
  processed$BAZ[[i]] = ZBMI
  processed$Thinness[[i]] = ifelse (ZBMI < -2, 1, 0)
  
  if (processed[[i,"Age"]] < 11){ #scores not calculated for children 11 and up
    ZWeight = depVars$zwfa[[i]]
    processed$WAZ[[i]] = ZWeight
    processed$Wasting[[i]] = ifelse (ZWeight < -2, 1, 0)
  }else{
    processed$WAZ[[i]] = NA
    processed$Wasting[[i]] = NA
  }
}

processed$HAZ = as.numeric(as.character(processed$HAZ))
processed$Stunting = as.numeric(as.character(processed$Stunting))
processed$BAZ = as.numeric(as.character(processed$BAZ))
processed$Thinness = as.numeric(as.character(processed$Thinness))
processed$WAZ = as.vector(as.character(processed$WAZ))
processed$Wasting = as.vector(as.character(processed$Wasting))

#badAges = c()
for (i in 1:1036){
  if (is.na(processed$Stunting[i])){
    processed = processed[-i,]
  }
}
#as.numeric(as.character(badAges))
#processed = processed[[-badAges,]]
```



```{r Saving Preprocessing}
stunting = processed[,-c(59, 61:64)]
thinness = processed[,-c(59:61, 63:65)]
wasting = processed[,-c(59:63)]
wasting = subset(wasting, Age < 11)

write.csv(stunting, "stunting.csv", row.names = FALSE)
write.csv(thinness, "thinness.csv", row.names = FALSE)
write.csv(wasting, "wasting.csv", row.names = FALSE)
```

# TRAIN/TEST SPLITS: Prepares train/test splits for use in feature selection and
# classifier model training. Each dependent variable (Stunting and Thinness)
# will be cloned; one clone will remain as-is (unbalanced), and the other will
# be balanced via SMOTE for (hopefully) improved classifier results.
#####
```{r Stunting Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

# stunting = read.csv("C:/Users/Carmen Perry/Desktop/Colgate Stuff/Summer Research 2022/MalnutritionProject-20220523T125732Z-001/MalnutritionProject/Code/SR2022Malnutrition/Data/stunting.csv")
stunting = read.csv("stunting.csv", row.names = FALSE)

set.seed(13)
testFolds = createFolds(stunting$Stunting, k = 10) #Create k splits
testSetsStunting = list(stunting[testFolds$Fold01, ], stunting[testFolds$Fold02, ],
                        stunting[testFolds$Fold03, ], stunting[testFolds$Fold04, ],
                        stunting[testFolds$Fold05, ], stunting[testFolds$Fold06, ],
                        stunting[testFolds$Fold07, ], stunting[testFolds$Fold08, ],
                        stunting[testFolds$Fold09, ], stunting[testFolds$Fold10, ]
                        )
sansTestStunting = list(stunting[-testFolds$Fold01, ], stunting[-testFolds$Fold02, ],
                        stunting[-testFolds$Fold03, ], stunting[-testFolds$Fold04, ],
                        stunting[-testFolds$Fold05, ], stunting[-testFolds$Fold06, ],
                        stunting[-testFolds$Fold07, ], stunting[-testFolds$Fold08, ],
                        stunting[-testFolds$Fold09, ], stunting[-testFolds$Fold10, ]
                        )
UBStunting = list()
for (i in 1:10){
  curSet = sansTestStunting[[i]]
  validationFolds = createFolds(sansTestStunting[[i]]$Stunting, k = 10)
  UBStunting[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEStunting = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEStunting[[i]] = list()
  for (j in 1:10){
    SMOTEStunting[[i]][[j]] = SMOTE(X = as.data.frame(UBStunting[[i]][[j]]),
                              target = UBStunting[[i]][[j]]$Stunting,
                              K = 5,
                              dup_size = 3
                              )
     SMOTEStunting[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBStunting[[1]][[1]]$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEStunting[[1]][[1]]$data$Stunting)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Stunting",
#         names = c("No", "Yes"),
#         main = 'Stunting Training Set Class Distribution (SMOTE)')
# abline(h = 0)


# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestStunting = rbind(testSetsStunting[1:10])
# ForFS_UBStunting = rbind(sansTestStunting[1:10])
# ForFS_SMOTEStunting = rbind(SMOTEStunting[1:10])

# write.csv(ForFS_TestStunting, "ForFS_TestStunting.csv")
# write.csv(ForFS_UBStunting, "ForFS_UBStunting.csv")
# write.csv(ForFS_SMOTEStunting, "ForFS_SMOTEStunting.csv")
```



```{r Thinness Train/Test Splits}
library(caret)
library(tidyverse)
library(smotefamily)

# thinness = read.csv("C:/Users/Carmen Perry/Desktop/Colgate Stuff/Summer Research 2022/MalnutritionProject-20220523T125732Z-001/MalnutritionProject/Code/SR2022Malnutrition/Data/thinness.csv")
thinness = read.csv("thinness.csv", row.names = FALSE)

set.seed(13)
testFolds = createFolds(thinness$Thinness, k = 10) #Create k splits
testSetsThinness = list(thinness[testFolds$Fold01, ], thinness[testFolds$Fold02, ],
                        thinness[testFolds$Fold03, ], thinness[testFolds$Fold04, ],
                        thinness[testFolds$Fold05, ], thinness[testFolds$Fold06, ],
                        thinness[testFolds$Fold07, ], thinness[testFolds$Fold08, ],
                        thinness[testFolds$Fold09, ], thinness[testFolds$Fold10, ]
                        )
sansTestThinness = list(thinness[-testFolds$Fold01, ], thinness[-testFolds$Fold02, ],
                        thinness[-testFolds$Fold03, ], thinness[-testFolds$Fold04, ],
                        thinness[-testFolds$Fold05, ], thinness[-testFolds$Fold06, ],
                        thinness[-testFolds$Fold07, ], thinness[-testFolds$Fold08, ],
                        thinness[-testFolds$Fold09, ], thinness[-testFolds$Fold10, ]
                        )
UBThinness = list()
for (i in 1:10){
  curSet = sansTestThinness[[i]]
  validationFolds = createFolds(sansTestThinness[[i]]$Thinness, k = 10)
  UBThinness[[i]] = list(curSet[-validationFolds$Fold01, ], #Remove subsets from current set
                         curSet[-validationFolds$Fold02, ],
                         curSet[-validationFolds$Fold03, ],
                         curSet[-validationFolds$Fold04, ],
                         curSet[-validationFolds$Fold05, ],
                         curSet[-validationFolds$Fold06, ],
                         curSet[-validationFolds$Fold07, ],
                         curSet[-validationFolds$Fold08, ],
                         curSet[-validationFolds$Fold09, ],
                         curSet[-validationFolds$Fold10, ]
                         )
}
SMOTEThinness = list()
for (i in 1:10){ #Use SMOTE to create balanced versions of each of the training sets
  SMOTEThinness[[i]] = list()
  for (j in 1:10){
    SMOTEThinness[[i]][[j]] = SMOTE(X = as.data.frame(UBThinness[[i]][[j]]),
                              target = UBThinness[[i]][[j]]$Thinness,
                              K = 5,
                              dup_size = 24
                              )
    SMOTEThinness[[i]][[j]]$data$class = NULL #Extra random variable, breaks modelling
  }
}

# Barplots for visualization of class imbalance
#####
# barplot(prop.table(table(UBThinness[[1]][[1]]$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (Unbalanced)')
# abline(h = 0)
# 
# barplot(prop.table(table(SMOTEThinness[[1]][[1]]$data$Thinness)),
#         col = c("white", "darkgrey"),
#         ylim = c(0, 1),
#         xlim = c(0, 3),
#         ylab = "Prevalence (%)",
#         xlab = "Thinness",
#         names = c("No", "Yes"),
#         main = 'Thinness Training Set Class Distribution (SMOTE)')
# abline(h = 0)

# Following code writes each training set to its own .csv files if needed in other scripts.
#####
# ForFS_TestThinness = rbind(testSetsThinness[1:10])
# ForFS_UBThinness = rbind(trainSetsUBThinness[1:10])
# ForFS_SMOTEThinness = rbind(trainSetsSMOTEThinness[1:10])

# write.csv(ForFS_TestThinness, "ForFS_TestThinness.csv")
# write.csv(ForFS_UBThinness, "ForFS_UBThinness.csv")
# write.csv(ForFS_SMOTEThinness, "ForFS_SMOTEThinness.csv")
```

#####
# FEATURE SELECTION: Selects important model features to reduce noise and help 
# prevent overfitting. All feature selection methods should be run on SMOTE-
# balanced datasets.
#####
```{r ReliefF}
#This one takes ~90 minutes to run. Have fun waiting.
library(FSelectorRcpp)
################################################################################
FS_RLF_Stunting = list()
for (i in 1:10){
  FS_RLF_Stunting[[i]] = list()
  for (j in 1:10){
    relF = relief(Stunting~.,
                as.data.frame(SMOTEStunting[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Stunting[[i]][[j]] = relF[order(relF$importance), ]
  }
}
################################################################################
FS_RLF_Thinness = list()
for (i in 1:10){
  FS_RLF_Thinness[[i]] = list()
  for (j in 1:10){
    relF = relief(Thinness~.,
                as.data.frame(SMOTEThinness[[i]][[j]]$data),
                neighboursCount = 5
                )
    FS_RLF_Thinness[[i]][[j]] = relF[order(relF$importance), ]
  }
}
```



```{r InfoGain}
#This one is pretty quick.
library(FSelectorRcpp)
################################################################################
FS_IG_Stunting = list()
for (i in 1:10){
  FS_IG_Stunting[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Stunting~.,
                              data = as.data.frame(SMOTEStunting[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_Stunting[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
################################################################################
FS_IG_Thinness = list()
for (i in 1:10){
  FS_IG_Thinness[[i]] = list()
  for (j in 1:10){
    infogain = information_gain(formula = Thinness~.,
                              data = as.data.frame(SMOTEThinness[[i]][[j]]$data),
                              type = 'infogain',
                              equal = TRUE
                              )
    FS_IG_Thinness[[i]][[j]] = infogain[order(infogain$importance), ]
  }
}
```



```{r mRMR}
library(mRMRe)
#In progress; mRMR finishes selecting, but results should be rewritten for legibility.
#This one is pretty quick.
FS_MRMR_Stunting = list()
for (i in 1:10){
  FS_MRMR_Stunting[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEStunting[[i]][[j]]$data)
    FS_MRMR_Stunting[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 98,
                                              feature_count = 13
                                              )
  }
}
################################################################################
FS_MRMR_Thinness = list()
for (i in 1:10){
  FS_MRMR_Thinness[[i]] = list()
  for (j in 1:10){
    mrmr = mRMR.data(data = SMOTEThinness[[i]][[j]]$data)
    FS_MRMR_Thinness[[i]][[j]] = mRMR.classic("mRMRe.Filter",
                                              data = mrmr,
                                              target_indices = 98,
                                              feature_count = 13
                                              )
  }
}
```



```{r JMI}
#This one is pretty quick.
library(praznik)
################################################################################
FS_JMI_Stunting = list()
for (i in 1:10){
  FS_JMI_Stunting[[i]] = list()
  for (j in 1:10){
    FS_JMI_Stunting[[i]][[j]] = JMI(SMOTEStunting[[i]][[j]]$data[1:97], 
                                    SMOTEStunting[[i]][[j]]$data$Stunting,
                                    k = 13
                                    )
  }
}
################################################################################
FS_JMI_Thinness = list()
for (i in 1:10){
  FS_JMI_Thinness[[i]] = list()
  for (j in 1:10){
    FS_JMI_Thinness[[i]][[j]] = JMI(SMOTEThinness[[i]][[j]]$data[1:97],
                                    SMOTEThinness[[i]][[j]]$data$Thinness,
                                    k = 13
                                    )
  }
}
```



```{r Literature-Based}
#This one is pretty quick (it's almost like there's no computations...)
FS_LB_Stunting = list(c('Age'), c('HouseholdNumb'), c('NumbOldSib'), c('Young12'),  c('DiarrheaPWeek'), c('Sex'), c('No.Vaccine'),c('NoTrimNails'), c('DirtyNails'), c('TrimOnce2Week'), c('TrimOnceMonth'), c('NoLatDoors'), c('Flies'), c('StoolFloor'), c('Suburban'), c('Rural'), c('NoLitMom'), c('PrimMom'), c('HSMom'), c('Radio'), c('TV'), c('Cattle'), c('SheepGoat'), c('Chicken'), c('Pet'), c('PotableWater'), c('DrinkDirect'), c('OwnLatrine'), c('Outside.Latrine'), c('Sewage'), c('Ditch'), c('River'), c('AlwaysSchoolLat'), c('SometimesSchoolLat'), c('SometimesTP'), c('NeverTP'), c('SometimesWashHands'), c('NeverWashHands'), c('WaterWashHands'), c('AlwaysSoil'), c('SometimesSoil'), c('NeverWashFruits'), c('SometimesWashFruits'), c('SometimesBarefoot'), c('AlwaysBarefoot'), c('NoShoes'), c('Sandals'), c('Deworm'), c('Antibiotic'), c('Drugs'), c('Rash'), c('Sneeze'), c('HayFever'), c('Colds')) 

FS_LB_Thinness = FS_LB_Stunting
```

#####
# CLASSIFIER MODELS:
#####
```{r Random Forest}
RF_UB_Stunting
RF_SMOTE_Stunting
RF_UB_Thinness
RF_SMOTE_Thinness
```



```{r Support Vector Machine}
SVM_UB_Stunting
SVM_SMOTE_Stunting
SVM_UB_Thinness
SVM_SMOTE_Thinness
```



```{r Extreme Gradient Boosting}
XGB_UB_Stunting
XGB_SMOTE_Stunting
XGB_UB_Thinness
XGB_SMOTE_Thinness
```



```{r Univariate Logistic Regression}
#Classifier or FS?

```



```{r Multivariate Logistic Regression, warning = FALSE}
#First MVLoR is for Odds Ratios, uses entire dataset.
#Second is for FS. Uses traditional train/test splits and 95% prevalence cutoff thresholds for choosing features.
#library(devtools)
library(broom)
OR_Stunting = glm(Stunting~., data = stunting, family = "binomial")
OR_Thinness = glm(Thinness~., data = thinness, family = "binomial")
write.csv(cbind( 
          tidy(OR_Stunting, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_Stunting))),
          "OR_Stunting.csv"
          )
write.csv(cbind( 
          tidy(OR_Thinness, exponentiate = TRUE, conf.level = 0.95),
          exp(confint(OR_Thinness))),
          "OR_Thinness.csv"
          )
#write.csv(tidy(OR_Thinness, exponentiate = TRUE, conf.level = 0.95), "OR_Thinness.csv")

```

#####
# ASSOCIATION RULE LEARNING:
#####
```{r Association Rule Learning}
#Takes a bit of time - couple minutes, maybe.
#DO NOT manually exit if program is slow or frozen; the Apriori algorithm is written in C, and has no qualms with messing things up in your computer if you interrupt it.
#Also, warning: do not increase maxlen above 3 unless you have a lot of free time and want your computer to be unresponsive.
library(arules)
library(arulesViz)

stuntingRules = apriori(stunting,
                        parameter = list(support = 0.1, confidence = 0.8),
                        maxlen = 3
                        )
thinnessRules = apriori(thinness,
                        parameter = list(support = 0.1, confidence = 0.8),
                        maxlen = 3
                        )
```